{"posts":[{"title":"T-SNE by Python","content":"This note records T-SNE by Python and the differences between PCA and T-SNE I am writing...... ","link":"https://cauliyang.github.io/post/t-sne-by-python/"},{"title":"Bin map ","content":"本文记录芯片数据如何筛选，以及使用bin map方法构建遗传图谱。 芯片数据的筛选大致分为以下步骤： 标记QC 1.1 计算标记的检出率，保留&gt;80%，删除缺失率大于20%的标记 可以适当调整阈值根据自己的数据量 1.2 计算标记的杂合率（如果杂合率很高的情况下，这个SNP可能还是有问题的），但是杂合率的评判标准依据不同群体有所不同，如F2或BC1群体，标记杂合率普遍在40-60%之间，但RIL群体或DH群体就会低很多，可以针对自己的群体，设置合适的阈值 1.3 删除双亲为杂合且是缺失的标记 1.4 计算双亲的无多态性，删除双亲没有多态性的标记 1.5 计算标记的MAF，保留MAF&gt;0.05 1.6 群体内家系基因型根据两亲本更改为 A B H ( 缺失不变） 1.7 删除多拷贝SNP，注释信息中chr hit number&gt;1的标记，这个一般针对那些非常差的标记，可以忽略此步骤 样本QC 2.1 确定标记后，用剩下的标记进行样本质控，计算每个材料的缺失率和杂合率，然后对材料进行筛选 卡方测验 3.1 进行卡方测验，计算标记的分离比，删除严重偏分离的标记，这一步也可以在JoinMap，rqtl,AsMap中操作。 ","link":"https://cauliyang.github.io/post/xin-pian-shu-ju-gou-jian-yi-chuan-lian-suo-tu-biao-ji-shai-xuan-liu-cheng/"},{"title":"Vcftools  Manual","content":"整理并记录处理VCf文件格式的工具vcftools的使用方法，主要用于自己使用。（侵权，立即删。😊） 1. 文件的读入和输出 参数 说明 --vcf 读入vcf类型文件，如不输入其他参数则统计位点，个体信息 --gzvcf 读入vcf.gz类型文件，如不输入其他参数则统计位点，个体信息 --bcf 读入bcf类型文件，如不输入其他参数则统计位点，个体信息 --out 输出文件，需要添加 --recode 参数(重构vcf结果输出INFO列信息） --stdout 输出到标准输出，可配合bgzip，gzip进行压缩 --recode 一般与--out一起使用，加入--recode-INFO-all则保留原文件INFO For instance: 提取一号染色体上的变异信息到filename文件 vcftools --vcf infile.vcf --chr 1 --recode --out filename 将一号染色体上的变异信息输出到屏幕 vcftools --vcf infile.vcf --chr 1 --recode --stdout 统计个体个数和突变位点总数 vcftools --vcf infile.vcf 提取染色体A01上的SNP，输出到A01.vcf。需要带--recode参数 vcftools --vcf infile.vcf --chr A01 --from-bp 1000000 --to-bp 2000000 --recode --recode-INFO-all --out A01.vcf 2.vcftools 对snp数据过滤 参数 说明 --chr，--not-chr 保留或者去掉某条染色体 --keep-only-indels，--remove-indels 保留indels，或者去掉indels保留snps --indv，--remove-indv 保留或者去掉某种样品 --keep，--remove 保留或者去掉某些样品 --max-maf 最大等位基因频率 --maf 最小等位基因频率 --max-missing 完整度(0-1之间）= 1 - 缺失度 --minDP 最小深度 --snps ，--exclude 根据SNP位点过滤 --min-alleles 最小等位基因数量 --max-alleles 最大等位基因数量 --remove-filtered-all 删除FILTER列不是PASS --SNPdensity 计算snp在bin内的密度，其后接bin大小 --extract-FORMAT-info 提取你想要的info，e.g. GT --get-INFO 多次提取info，e.g. --get-INFO NS --get-INFO DB -------------------; For instance: 深度设置为2，每个SNP位点完整度设置为0.7，最小等位基因频率设置为0.05 vcftools --vcf infile.vcf --minDP 2 --maf 0.05 --max-missing 0.7 --recode --out o utfile 从所有样品中提取S1和S2 vcftools --vcf infile.vcf --indv S1 --indv S2 --recode --out outfile 从所有样品中批量提取样品 vcftools --vcf infile.vcf --keep sample.list --recode --out outfile 提取两个等位基因的SNP位点 vcftools --vcf infile.vcf --min-alleles 2 --max-alleles 2 --recode --out outfile 3.Vcftools 在群体遗传学中的应用 计算遗传多样性参数 ： Fst，π，Tajima'sD 等 3.1 Fst计算 Fst是衡量群体间分化程度的重要参数，Fst越大，表明群体分化程度越高，受选择程度越高。基于Fst可以进行选择性消除分析。 For instance: 计算两个群体间fst值，S1.txt和S2.txt是包含了各群体的样品名 vcftools --vcf all.vcf --weir-fst-pop S1.txt --weir-fst-pop S2.txt ---fst-window-size 200000 --fst-window-step 100000 --out outfile 3.2 核苷酸多态性统计 核苷酸多样性π反映了群体的多态性。一般来说受选择程度越高的群体，遗传多样性越单一；野生群体遗传多样性较高。基于π可以进行选择性消除分析。 For instance: 计算群体核苷酸多态性 vcftools --vcf infile.vcf --window-pi 1000 --window-pi-step 1000 --out filename 计算所有单点或所选多点(--positions)的核苷酸多态性 vcftools --vcf infile.vcf --site-pi (--positions SNP_list.txt) --out filename 3.3 Tajima's D计算 Tajima's D衡量群体中性进化理论的指标，越偏离0，受选择程度越高。 For instance: 计算Tajima's D vcftools --vcf infile.vcf --TajimaD 100000 --out filename 3.4 LD等其他统计参数 参数 说明 --hap-r2，--geno-r2 LD统计 --TsTV SNP转换/颠换统计 --freq 每个SNP等位基因频率统计 --counts 每个SNP等位基因个数统计 --SNPdensity SNP频率 --missing-indv 样品等缺失率统计 --missing-site SNP缺失率统计 --depth 个体深度统计 --site-depth，--site-mean-depth 位点深度，平均深度统计 4.其他 4.1 比较两个vcf文件 比较2个群体的vcf文件，包含多个选项，常用的如 –diff-site –diff-indv vcftools --gzvcf input_file1.vcf.gz --gzdiff input_file2.vcf.gz --diff-site --out in1_v_in2 4.2 VCF转化plink vcftools还可以转化多种格式，常用的是转化成plink格式。 参数：--012，--IMPUTE，--ldhat-geno，--BEAGLE-GL，--BEAGLE-PL，--plink vcftools --vcf infile.vcf --plink --chr 1 --out output_in_plink 合并多个vcf文件(也可以使用GATK） bcftools merge A.vcf B.vcf &gt; AB.vcf 4.3 其他格式转换 --012 --IMPUTE --ldhat --ldhat-geno --BEAGLE-GL --BEAGLE-PL ---; 具体使用方法可以看 vcftools说明 5.总结 本文旨在学习和整理用到的大部分vcftools方法，如果有所纰漏欢迎指正！😊 ","link":"https://cauliyang.github.io/post/vcftools-1/"},{"title":"K-means  by Python","content":"本篇文章，详细记录如何使用Python进行K-means，分别用两种方法实现，并记录如何选取K值，并进行可视化评估结果。 img{ width: 80%; padding-left: 10%; } 1.K-means概念介绍 1.1 基础概念 K-means是一种常用的无监督学习技术，用于在无法知道正确答案下发现数据中隐藏的结构，聚类的目标是在数据中找到自然分组，确保相同集群中元素比不同的集群中元素更加相似。K-means方法非常擅长识别球形数据，其缺点是必须指定集群数K。如果选择K值不当会造成分群效果不好，后文将会介绍两种方法用来评估K值及分群效果。并且本文采用两种方式实现K-means 使用scikit-learn模块进行K-means聚类分析。 从头手写K-means方法。 1.2 算法原理 随机在样本中选取K质心作为起始聚类的中心。 将每个样本根据欧式距离分到最近的质心μ\\muμ所在的群中。 将所有样本分群后，重新计算以每个群的中心作为新的质心。 重复2，3 两步，知道质心不再改变，或者达到用户自定义的阈值或最大迭代数。 欧式距离的计算方法为： d(x,y)2=∑j=1m(xj−yj)2=∣∣x−y∣∣22d(x,y)^2 = \\sum^{m}_{j = 1}(x_j - y_j )^2 = ||x - y||^{2}_{2} d(x,y)2=j=1∑m​(xj​−yj​)2=∣∣x−y∣∣22​ 其中jjj代表数据的纬度。 基于欧式距离我们可以把分群的过程描述为一个优化的问题，是一种最小化群内误差平方和（SSE）的迭代方法也被称为群惯性。 SSE=∑i=1n∑j=1kw(i,j)∣∣x(i)−μ(i)∣∣22SSE = \\sum^{n}_{i =1 } \\sum^{k}_{j=1 } w^{(i,j)}||x^{(i)} - \\mu^{(i)}||^{2}_{2} SSE=i=1∑n​j=1∑k​w(i,j)∣∣x(i)−μ(i)∣∣22​ 其中 iii代表样本索引 jjj代表分群索引 2.使用scikit-learn实现K-means方法 2.1 创建测试数据并实现算法 首先导入所需要的模块： # import module import numpy as np from matplotlib import cm from sklearn.datasets import make_blobs from sklearn.cluster import KMeans import matplotlib.pyplot as plt from sklearn.metrics import silhouette_samples 因为二维数据可是简单的绘制在笛卡尔坐标系上，所以生成二维测试数据进行测试： # creat tested data X, y = make_blobs(n_samples=150, # volume of data n_features=2, # number of feature centers=3, # number of centroid cluster_std=0.5, # distribution of data shuffle=True, random_state=0) 绘图查看原始数据： # plot tested data plt.figure() plt.scatter(X[:, 0], X[:, 1], c='white', marker='o', edgecolor='black', s=50) plt.grid() plt.show() 从图中可以看出创建的测试数据有明显的分群情况,当然在真实的数据当中原始数据可能没有这么理想。我们先在没有推理的情况下确定K的值为3。 # k-means km = KMeans(n_clusters=3, # K value init='random', n_init=10, # number of repeatation max_iter=300, tol=1e-4, random_state=0) # predict labels y_km = km.fit_predict(X) 我们进行可视化分群结果： #creating function of ploting graph for reusing def plot_res(y_km, X, n_cluster): # init colors and markers colors = ['lightgreen', 'orange', 'lightblue'][:n_cluster] markers = ['s', 'o', 'v'][:n_cluster] # plot the cluster to comfirm the result of k-meams by sklearn for i, (c, m) in enumerate(zip(colors, markers)): plt.scatter(X[y_km == i, 0], X[y_km == i, 1], s=50, c=c, marker=m, edgecolor='black', label=f'cluster {i}') # plot centroipd of different clusters plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker='*', c='red', edgecolors='black', label='centroids') # plot lengend plt.legend(scatterpoints=1) # plot grid plt.grid() plt.show() 可以明显看到分群效果十分明显。不过其中还有许多问题： 如何确实K值 如何评估分群质量 2.2 如何选取K值 下面介绍如何使用肘解法选取合适的K值，肘解法目的是找出SSE变化幅度最大的K值。使用km.inertia_ 即可调出SSESSESSE的值。 distortions = [] # test different numbers of cluster to pick up the best K for i in range(1, 11): km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0) km.fit(X) distortions.append(km.inertia_) 测试1-11的K值选取，并进行可视化查看结果。 # plot the tested result for the best K plt.plot(range(1, 11), distortions, marker='o') plt.xlabel('Number of clusters') plt.ylabel('Distortion') plt.show() 从图中我们可以看出在K值为3的时候，SSESSESSE变化幅度最大，即可得知K为3是最优解。 2.3 如何评估分群的质量 评价聚类质量的一种方法是轮廓分析，他可以应用于其他聚类算法，度量其紧密程度。计算轮廓系数的步骤为： 计算集群内聚度a(i)a^{(i)}a(i)，即样本x(i)x^{(i)}x(i)于集群内所有其他点之间的平均距离。 计算集群与最近集群的分离度b(i)b^{(i)}b(i),即样本x(i)x^{(i)}x(i)与最近集群内所有样本的平均距离。 计算轮廓系数s(i)s^{(i)}s(i)，即集群内聚度与集群分离度之差，除以两者中较大的一个。 s(i)=b(i)−a(i)max⁡{b(i),a(i)}s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{\\max \\{{b^{(i)},a^{(i)}}\\}} s(i)=max{b(i),a(i)}b(i)−a(i)​ 轮廓系数的范围在-1到1之间，如果集群分离度和集群内聚度相等，即b(i)=a(i)b^{(i)}=a^{(i)}b(i)=a(i)。那么轮廓系数为0，如果b(i)&gt;&gt;a(i)b^{(i)} &gt;&gt; a^{(i)}b(i)&gt;&gt;a(i) 则接近理想的轮廓系数 1 。 可以使用scikit-learn中metric中的silhouette_samples计算样本的轮廓系数。也可以更方便的使用silhouette_scores直接计算所有样本的平均轮廓系数。下面显示K值基于3的分群结果。 # we can use the graph of silhouette to evaluate result km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=10, tol=1e-04, random_state=0) y_km = km.fit_predict(X) 绘制轮廓图进行可视化，直观的查看群内轮廓系数。 # difining fuction of plot-silhouette for reusing # plot the graph of silhouette def plot_sil(y_km, X): cluster_lables = np.unique(y_km) n_clusters = cluster_lables.shape[0] # using function of silhouette in sklearn to get silhouete scores silhouette_vals = silhouette_samples(X, y_km, metric='euclidean') # plot the graph y_ax_lower, y_ax_upper = 0, 0 yticks = [] for i, c in enumerate(cluster_lables): # get values of each cluster c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() # sort value for ploting y_ax_upper += len(c_silhouette_vals) color = cm.jet(float(i) / n_clusters) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color) yticks.append((y_ax_lower + y_ax_upper) / 2) y_ax_lower += len(c_silhouette_vals) silhouette_avg = np.mean(silhouette_vals) # get the label of yticks plt.axvline(silhouette_avg, color='red', linestyle='--') # plot the avaerage of silhouette plt.yticks(yticks, labels=cluster_lables) plt.ylabel('Cluster') plt.xlabel('Silhouette coefficient') plt.show() 从图中我们可以看出轮廓系数不接近于0，且接近于1表明我们的分群结果良好。且在图中轮廓系数的高度代表群内样本数量，如果样本数量相差太大，说明分群效果不是很好。图中虚线表示平均轮廓系数。 为更好的理解轮廓系数的使用，将K值变为2，进行聚类。 km = KMeans( n_clusters=2, # value of k has changed init='k-means++', n_init=10, max_iter=10, tol=1e-04, random_state=0) y_km = km.fit_predict(X) 使用上方作图函数，先观察分群效果。 从图中可以看出分群效果很差，可视化轮廓系数查看结果。 两个群的高度不一致表明分群效果不是很理想，且有的样本轮廓系数极低接近于0。表示分群有很大的问题，需要重新思考K值的选取。 3. K-means from scratch 我们根据算法原理使用Python一步步实现K-means，首先展示我们所用到的数据集，有关基因在不同条件下处理的表达数据，其中基因数量为样本数量，处理方式为纬度。并且设计为Terminal端使用。 终端使用方法为： Usage : python k_mean.py k data max_it (cetroids) 其中 k_mean.py 为程序脚本 k 为分群数量 data 为原始数据文件 max_it 为最大递归次数 centroids 为初始的质心，用户可以选择提供或者不提供 原始数据： gene_expression treat_1 treat_2 ... g_1 0.2 0.5 ... g_2 1.4 1.6 ... ... 4.2 2.1 ... 3.1 Get parameters from terminal 导入所需的模块 # import modules import sys import time import numpy as np from collections import Counter from operator import itemgetter 从终端获取用户传递参数： # defining function for getting parameters from terminal def get_argv(): ''' get the parameters entered by user and return the dictionary parameters ''' # get parameters argv_list = sys.argv # init parameters argv_name = ( 'data', 'init_cetroids', 'gene_num', # numbers of row 'ndim', 'max_it', # max numbers of iter 'k') # determine whether user provide init-centroids according numbers of parameters if len(argv_list) == 4: # if not provide init-centroid _, k, file, max_it = argv_list # get information of file argv_tuple = get_Cetroid(file, int(k)) + (int(max_it), int(k)) elif len(argv_list) == 5: # if provide init-centroid _, k, file, max_it, cetroid_file = argv_list # get information argv_tuple = get_Cetroid( file, int(k), cetroid_file=cetroid_file) + (int(max_it), int(k)) elif len(argv_list) &lt; 4: # if numbers of parameters is less than need parameters then print help print(''' ------------------------------------------------- Requirement : numpy Usage : python k_mean.py k data max_it (cetroids) Result_file : kmeans.out Contact : &lt;liyangyang&gt; &lt;yangyangli.vince@gmail.com&gt; ------------------------------------------------- ''') sys.exit(0) # return dictionary parameters return dict(zip(argv_name, argv_tuple)) 3.2 Creating function of report # difining function of reporting summary def summary(kw, tim, kmeanout='kmeans.out'): ''' Create a summary function, count recursive times, run time, etc.。 ''' # statistics for each Cluster data def print_cluster(kmean=kmeanout): # evaluate data counter = Counter(np.loadtxt(kmean, dtype=int)[:, 1]) # produce report for clu, num in counter.most_common(): print(f' Cluster_{clu} : {num}') # creat statistic header print('{:-^40}\\n'.format('Summary')) # print statstic report of each cluster print_cluster() # print overall information print(f''' Max_iter_number : {kw['max_it']} Cluster_number :{kw['k']} Time : {tim:.2f}s Date : {time.asctime()}''') # creat statistic tial print('{:-&lt;40}\\n'.format('-')) 3.3 Calculating Euclidean distance # defining function to calculate Euclidean distance def eucl_Distance(init_cetroids, piece_data): ''' Calculate the Euclidean distance between each data and the centroid ''' distance = np.sqrt(np.sum((init_cetroids - piece_data)**2, axis=1)) # return euclidean distance return distance 3.4 Getting centroid information and recursive function def get_Cetroid(file, k, cetroid_file=None): ''' This function is used to get raw data file information: raw data, centroid, data volume, feature dimension ''' # get content of file data = np.loadtxt(file) # get information: data volume, feature dimension gene_num, ndim = data.shape # Determine whether the user provides a centroid, and randomly if not provided if not (cetroid_file): # init centroid init_cetroids = np.zeros((k, ndim)) # provied centroid randomly for i in range(k): index = int(np.random.uniform(0, gene_num)) init_cetroids[i, :] = data[index, :] else: # if users provide centroid init_cetroids = np.loadtxt(cetroid_file) # return information return (data, init_cetroids, gene_num, ndim) def iter_Cetroid(**argv): ''' Iterative clustering results ''' # get neccessary parameters data, init_cetroids, gene_num, ndim, max_it, k = argv.values() # init results Result = np.zeros((gene_num, 2), dtype=int) # grouping data according to Euclidean distance for i in range(gene_num): # get Euclidean distance distance = eucl_Distance(init_cetroids, data[i, :]) # get the label of shortest distance cluster = distance.argmin() # grouping Result[i, :] = np.array([i, cluster]) # verify that the results of the iteration are stable and return a new centroid Handle, argv['init_cetroids'] = assert_Result(data, init_cetroids, Result, k) # return informattion return Result, Handle.all(), argv, max_it 3.5 Creating Body function and Main function def run(arg_dict, it_num=0): ''' the body of k-means ''' # perform an iteration and verify that the results are stable # then calculate the new centroid to be returned in dictionary form Result, handle, arg_dict, max_it = iter_Cetroid(**arg_dict) # determine whether the condition of end iteration is reached if not (handle) and (it_num &lt; max_it): # if not reach and the iteration continues it_num += 1 # print numbers of iteration print(f'...ing Iter Number :{it_num}') # recursive iteration run(arg_dict, it_num=it_num) # if reach condition else: # change lable,like change the lables from 0,1,2 to 1,2,3 Result = Result + 1 count_1 = Counter(Result[:, 1]) # save result file np.savetxt('kmeans.out', Result, fmt='%d') def main(): ''' the program main function, integrate workflow, and generate reports ''' # get start time TIC = time.time() # get parameter through terminal ARGV = get_argv() # running the body function of k-means run(ARGV) # get end time TOC = time.time() # generate report summary(ARGV, TOC - TIC) 4. Summary 本篇文章详细记录两种方式实现K-means方法，并且记录如何选取K值，如何评估聚类质量。本文最终涉及的代码都会在Jupyter notebook找到,并且使用脚本程序 谢谢观看，欢迎交流！😎 ","link":"https://cauliyang.github.io/post/k-means-cluster-by-python/"},{"title":"PCA by Python ","content":"This article documents two methods of PCA analysis using Python, and visualizes 2-dimensional results. img{ width: 70%; padding-left: 1%; } 1.Intrduction 1.1 What's PCA? When it comes to methods of reducing dimension, PCA that is an unsupervised linear transformation technique, must be not ignored. Moreover, if you want to know the subtle relationships among data set and reduce the computational complexity in downstream analysis, the PCA may be your best choice! Meanwhile, if you would like to present your data in a 2-dimension or 3-dimension coordinate system, and PCA would sweep your problems! What is reducing dimension? I will show you an example as follows: First, suppose you have a five-dimensional data set : Id 1-d 2-d 3-d 4-d 5-d data-1 1 2 3 4 5 data-2 6 7 8 9 10 .. .. .. .. .. Then, you could pick up PC1 and PC2 after PCA to reduce dimension for plotting: Id PC1 PC2 data-1 0.3 0.6 data-2 0.1 1.2 .. .. .. PC1 and PC2 are the result obtained through data is projection on the unit vectors, which enable result to have the most biggest variance(means its distribution is wide) and to be irrelevant(covariance = 0). 1.2 Algorithm Normalize ddd dimension raw data Creat the covariance matrix Calculate the eigenvalues of the covariance matrix and the corresponding eigenvectors The eigenvectors are sorted in the matrix according to the corresponding feature value, and the first k rows are formed into a matrix WWW.(k&lt;&lt;dk&lt;&lt;dk&lt;&lt;d) Y=xWY = xWY=xW is the result after reducing dimension to k dimension Note: There are two prerequisites for conducting PCA Raw data has no NA The raw data should be normalized 2. PCA from scratch Importing necessary modules import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Creating raw data # get data set df_wine = pd.read_csv( 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None, engine='python') # check data df_wine.head() Creating train and test data set # creat train and test data set X, y = df_wine.iloc[:, 1:], df_wine.iloc[:, 0] x_train,x_test,y_train,y_test = \\ train_test_split(X,y,test_size = 0.3 , stratify= y, random_state = 0 ) Standarding the features # create standard instance sc = StandardScaler() # standard data x_train_std = sc.fit_transform(x_train) x_test_std = sc.fit_transform(x_test) Creating the covariance matrix and Getting eigenvectors and eigenvalues the calculation of the covriance matrix : σjk=1n∑i=1n(xj(i)−μj)(xk(i)−μk)\\sigma_{jk} = \\frac{1}{n} \\sum^{n}_{i=1}\\bigg(x_{j}^{(i)} - \\mu_j\\bigg)\\bigg(x_{k}^{(i)} - \\mu_k\\bigg) σjk​=n1​i=1∑n​(xj(i)​−μj​)(xk(i)​−μk​) Then, using numpy.cov and numpy.linalg.eig to get the covariance matrix and eigenvectors respectively # calculate the covariance matrix cov_mat = np.cov(x_train_std.T) # Getting eigenvectors and eigenvalues eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) NOTE: there are 13 eigenvectors totally, the number of eigenvalues might be not as same as the number of features sometimes. Firstly, plotting the Variance interpretation ratio, which is obtained through eigenvalue λj\\lambda_jλj​ divided by the sum of all the eigenvalues: λj∑j=1dλj\\frac{\\lambda_j}{\\sum^d_{j=1}\\lambda_j} ∑j=1d​λj​λj​​ # get sum of all the eigenvalues tot = sum(eigen_vals) # get variance interpretation ratio var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) Besides, plotting the result to get in-depth understanding: plt.figure() # creat plot # creat bar plot plt.bar( range(1, 14), var_exp, alpha=0.5, label='individual explained variance', ) # creat step plot plt.step(range(1, 14), cum_var_exp, where='mid', label='cumulative explained variance') # add label plt.ylabel('Explained variance ratio') plt.xlabel('Principal component index') # add legend plt.legend(loc='best') # save picture plt.savefig('pca_index.png', format='png', bbox_inches='tight', dpi=300) We can conclude that PC1 only takes account for about 40%. Furthermore, the sum of PC1 and PC2 have 60% variance. Selecting the first k values to form matrix WWW # integrate eigenvalues and eigenvectors eigen_paris = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))] # sort according to eigenvalues eigen_paris.sort(key=lambda x: x[0], reverse=True) # pick up the first 2 eigenvalues w = np.hstack( [eigen_paris[0][1][:, np.newaxis], eigen_paris[1][1][:, np.newaxis]]) # check matrix x w Tranforming raw data # reduce dimension x_train_pca = x_train_std.dot(w) # check resulted data x_train_pca.shape (124, 2) Then plotting the result and putting the label in terms of original info, but keeping in mind PCA is unsupervised learning skill without labels # init colors and markers colors = ['r', 'b', 'g'] markers = ['s', 'x', 'o'] # plot scatter for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(x_train_pca[y_train == l, 0], x_train_pca[y_train == l, 1], c=c, label=1, marker=m) # add label and legend plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.savefig('distribution.png', format='png', bbox_inches='tight', dpi=300) 3. PCA by scikit-learn we can conduct PCA easily by sklearn Importing modules from sklearn.decomposition import PCA from matplotlib.colors import ListedColormap from sklearn.linear_model import LogisticRegression Defining function of plot_decision_region def plot_dicision_regions(X, y, classifier, resolution=0.02): # init markers and colors markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # creat info for plot region x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) # test classifier's accurate z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) z = z.reshape(xx1.shape) # plot desicion region plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap) # set x,y length plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot result for idx, cl in enumerate(np.unique(y)): plt.scatter( x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.6, color=cmap(idx), edgecolor='black', marker=markers[idx], label=cl, ) PCA by sklearn # creat pca instance pca = PCA(n_components = 2 ) # creat classifier instance lr = LogisticRegression() # reduce dimension for data set x_train_pca = pca.fit_transform(x_train_std) x_test_pca = pca.transform(x_test_std) # classify x_train_pca lr.fit(x_train_pca,y_train) # plot dicision region plot_dicision_regions(x_train_pca,y_train,classifier=lr) # add info plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.show() We can see that the classifier's accurate is excellent according to actual labels TIPS: You can set n_components = None, and the result would retain all principle components. Moreover, you could call explained_variance_ration_ to use variance explanation ratio. 3.Summary All the above are the main content, welcome everybody communicates with me! 🤠 Reference book : Python marchine learning ","link":"https://cauliyang.github.io/post/pca-by-python-2/"},{"title":"Blog Ideas ","content":"This Blog record learning process, welcome everyone to communicate with me!😊 Ideas This blog is used as a public note to record my learning process. I also hope to help others~. If you have an infringement problem, then you can contact me. if it is true ,I will delete it immediately. ","link":"https://cauliyang.github.io/post/bo-ke-xiang-fa/"},{"title":"About ","content":"My name is Yangyang Li , I would like to take advantage of mathematics and programming to change the world and detect the secrets of life, welcome everybody to communicate with me! 🤓 ","link":"https://cauliyang.github.io/post/about/"}]}