{"posts":[{"title":"T-SNE by Python","content":"This note records T-SNE by Python and the differences between PCA and T-SNE I am writing...... ","link":"https://cauliyang.github.io/post/t-sne-by-python/"},{"title":"Bin map ","content":"æœ¬æ–‡è®°å½•èŠ¯ç‰‡æ•°æ®å¦‚ä½•ç­›é€‰ï¼Œä»¥åŠä½¿ç”¨bin mapæ–¹æ³•æ„å»ºé—ä¼ å›¾è°±ã€‚ èŠ¯ç‰‡æ•°æ®çš„ç­›é€‰å¤§è‡´åˆ†ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š æ ‡è®°QC 1.1 è®¡ç®—æ ‡è®°çš„æ£€å‡ºç‡ï¼Œä¿ç•™&gt;80%ï¼Œåˆ é™¤ç¼ºå¤±ç‡å¤§äº20%çš„æ ‡è®° å¯ä»¥é€‚å½“è°ƒæ•´é˜ˆå€¼æ ¹æ®è‡ªå·±çš„æ•°æ®é‡ 1.2 è®¡ç®—æ ‡è®°çš„æ‚åˆç‡ï¼ˆå¦‚æœæ‚åˆç‡å¾ˆé«˜çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªSNPå¯èƒ½è¿˜æ˜¯æœ‰é—®é¢˜çš„ï¼‰ï¼Œä½†æ˜¯æ‚åˆç‡çš„è¯„åˆ¤æ ‡å‡†ä¾æ®ä¸åŒç¾¤ä½“æœ‰æ‰€ä¸åŒï¼Œå¦‚F2æˆ–BC1ç¾¤ä½“ï¼Œæ ‡è®°æ‚åˆç‡æ™®éåœ¨40-60%ä¹‹é—´ï¼Œä½†RILç¾¤ä½“æˆ–DHç¾¤ä½“å°±ä¼šä½å¾ˆå¤šï¼Œå¯ä»¥é’ˆå¯¹è‡ªå·±çš„ç¾¤ä½“ï¼Œè®¾ç½®åˆé€‚çš„é˜ˆå€¼ 1.3 åˆ é™¤åŒäº²ä¸ºæ‚åˆä¸”æ˜¯ç¼ºå¤±çš„æ ‡è®° 1.4 è®¡ç®—åŒäº²çš„æ— å¤šæ€æ€§ï¼Œåˆ é™¤åŒäº²æ²¡æœ‰å¤šæ€æ€§çš„æ ‡è®° 1.5 è®¡ç®—æ ‡è®°çš„MAFï¼Œä¿ç•™MAF&gt;0.05 1.6 ç¾¤ä½“å†…å®¶ç³»åŸºå› å‹æ ¹æ®ä¸¤äº²æœ¬æ›´æ”¹ä¸º A B H ( ç¼ºå¤±ä¸å˜ï¼‰ 1.7 åˆ é™¤å¤šæ‹·è´SNPï¼Œæ³¨é‡Šä¿¡æ¯ä¸­chr hit number&gt;1çš„æ ‡è®°ï¼Œè¿™ä¸ªä¸€èˆ¬é’ˆå¯¹é‚£äº›éå¸¸å·®çš„æ ‡è®°ï¼Œå¯ä»¥å¿½ç•¥æ­¤æ­¥éª¤ æ ·æœ¬QC 2.1 ç¡®å®šæ ‡è®°åï¼Œç”¨å‰©ä¸‹çš„æ ‡è®°è¿›è¡Œæ ·æœ¬è´¨æ§ï¼Œè®¡ç®—æ¯ä¸ªææ–™çš„ç¼ºå¤±ç‡å’Œæ‚åˆç‡ï¼Œç„¶åå¯¹ææ–™è¿›è¡Œç­›é€‰ å¡æ–¹æµ‹éªŒ 3.1 è¿›è¡Œå¡æ–¹æµ‹éªŒï¼Œè®¡ç®—æ ‡è®°çš„åˆ†ç¦»æ¯”ï¼Œåˆ é™¤ä¸¥é‡ååˆ†ç¦»çš„æ ‡è®°ï¼Œè¿™ä¸€æ­¥ä¹Ÿå¯ä»¥åœ¨JoinMapï¼Œrqtl,AsMapä¸­æ“ä½œã€‚ ","link":"https://cauliyang.github.io/post/xin-pian-shu-ju-gou-jian-yi-chuan-lian-suo-tu-biao-ji-shai-xuan-liu-cheng/"},{"title":"Vcftools  Manual","content":"æ•´ç†å¹¶è®°å½•å¤„ç†VCfæ–‡ä»¶æ ¼å¼çš„å·¥å…·vcftoolsçš„ä½¿ç”¨æ–¹æ³•ï¼Œä¸»è¦ç”¨äºè‡ªå·±ä½¿ç”¨ã€‚ï¼ˆä¾µæƒï¼Œç«‹å³åˆ ã€‚ğŸ˜Šï¼‰ 1. æ–‡ä»¶çš„è¯»å…¥å’Œè¾“å‡º å‚æ•° è¯´æ˜ --vcf è¯»å…¥vcfç±»å‹æ–‡ä»¶ï¼Œå¦‚ä¸è¾“å…¥å…¶ä»–å‚æ•°åˆ™ç»Ÿè®¡ä½ç‚¹ï¼Œä¸ªä½“ä¿¡æ¯ --gzvcf è¯»å…¥vcf.gzç±»å‹æ–‡ä»¶ï¼Œå¦‚ä¸è¾“å…¥å…¶ä»–å‚æ•°åˆ™ç»Ÿè®¡ä½ç‚¹ï¼Œä¸ªä½“ä¿¡æ¯ --bcf è¯»å…¥bcfç±»å‹æ–‡ä»¶ï¼Œå¦‚ä¸è¾“å…¥å…¶ä»–å‚æ•°åˆ™ç»Ÿè®¡ä½ç‚¹ï¼Œä¸ªä½“ä¿¡æ¯ --out è¾“å‡ºæ–‡ä»¶ï¼Œéœ€è¦æ·»åŠ  --recode å‚æ•°(é‡æ„vcfç»“æœè¾“å‡ºINFOåˆ—ä¿¡æ¯ï¼‰ --stdout è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºï¼Œå¯é…åˆbgzipï¼Œgzipè¿›è¡Œå‹ç¼© --recode ä¸€èˆ¬ä¸--outä¸€èµ·ä½¿ç”¨ï¼ŒåŠ å…¥--recode-INFO-allåˆ™ä¿ç•™åŸæ–‡ä»¶INFO For instance: æå–ä¸€å·æŸ“è‰²ä½“ä¸Šçš„å˜å¼‚ä¿¡æ¯åˆ°filenameæ–‡ä»¶ vcftools --vcf infile.vcf --chr 1 --recode --out filename å°†ä¸€å·æŸ“è‰²ä½“ä¸Šçš„å˜å¼‚ä¿¡æ¯è¾“å‡ºåˆ°å±å¹• vcftools --vcf infile.vcf --chr 1 --recode --stdout ç»Ÿè®¡ä¸ªä½“ä¸ªæ•°å’Œçªå˜ä½ç‚¹æ€»æ•° vcftools --vcf infile.vcf æå–æŸ“è‰²ä½“A01ä¸Šçš„SNPï¼Œè¾“å‡ºåˆ°A01.vcfã€‚éœ€è¦å¸¦--recodeå‚æ•° vcftools --vcf infile.vcf --chr A01 --from-bp 1000000 --to-bp 2000000 --recode --recode-INFO-all --out A01.vcf 2.vcftools å¯¹snpæ•°æ®è¿‡æ»¤ å‚æ•° è¯´æ˜ --chrï¼Œ--not-chr ä¿ç•™æˆ–è€…å»æ‰æŸæ¡æŸ“è‰²ä½“ --keep-only-indelsï¼Œ--remove-indels ä¿ç•™indelsï¼Œæˆ–è€…å»æ‰indelsä¿ç•™snps --indvï¼Œ--remove-indv ä¿ç•™æˆ–è€…å»æ‰æŸç§æ ·å“ --keepï¼Œ--remove ä¿ç•™æˆ–è€…å»æ‰æŸäº›æ ·å“ --max-maf æœ€å¤§ç­‰ä½åŸºå› é¢‘ç‡ --maf æœ€å°ç­‰ä½åŸºå› é¢‘ç‡ --max-missing å®Œæ•´åº¦(0-1ä¹‹é—´ï¼‰= 1 - ç¼ºå¤±åº¦ --minDP æœ€å°æ·±åº¦ --snps ï¼Œ--exclude æ ¹æ®SNPä½ç‚¹è¿‡æ»¤ --min-alleles æœ€å°ç­‰ä½åŸºå› æ•°é‡ --max-alleles æœ€å¤§ç­‰ä½åŸºå› æ•°é‡ --remove-filtered-all åˆ é™¤FILTERåˆ—ä¸æ˜¯PASS --SNPdensity è®¡ç®—snpåœ¨binå†…çš„å¯†åº¦ï¼Œå…¶åæ¥binå¤§å° --extract-FORMAT-info æå–ä½ æƒ³è¦çš„infoï¼Œe.g. GT --get-INFO å¤šæ¬¡æå–infoï¼Œe.g. --get-INFO NS --get-INFO DB -------------------; For instance: æ·±åº¦è®¾ç½®ä¸º2ï¼Œæ¯ä¸ªSNPä½ç‚¹å®Œæ•´åº¦è®¾ç½®ä¸º0.7ï¼Œæœ€å°ç­‰ä½åŸºå› é¢‘ç‡è®¾ç½®ä¸º0.05 vcftools --vcf infile.vcf --minDP 2 --maf 0.05 --max-missing 0.7 --recode --out o utfile ä»æ‰€æœ‰æ ·å“ä¸­æå–S1å’ŒS2 vcftools --vcf infile.vcf --indv S1 --indv S2 --recode --out outfile ä»æ‰€æœ‰æ ·å“ä¸­æ‰¹é‡æå–æ ·å“ vcftools --vcf infile.vcf --keep sample.list --recode --out outfile æå–ä¸¤ä¸ªç­‰ä½åŸºå› çš„SNPä½ç‚¹ vcftools --vcf infile.vcf --min-alleles 2 --max-alleles 2 --recode --out outfile 3.Vcftools åœ¨ç¾¤ä½“é—ä¼ å­¦ä¸­çš„åº”ç”¨ è®¡ç®—é—ä¼ å¤šæ ·æ€§å‚æ•° ï¼š Fstï¼ŒÏ€ï¼ŒTajima'sD ç­‰ 3.1 Fstè®¡ç®— Fstæ˜¯è¡¡é‡ç¾¤ä½“é—´åˆ†åŒ–ç¨‹åº¦çš„é‡è¦å‚æ•°ï¼ŒFstè¶Šå¤§ï¼Œè¡¨æ˜ç¾¤ä½“åˆ†åŒ–ç¨‹åº¦è¶Šé«˜ï¼Œå—é€‰æ‹©ç¨‹åº¦è¶Šé«˜ã€‚åŸºäºFstå¯ä»¥è¿›è¡Œé€‰æ‹©æ€§æ¶ˆé™¤åˆ†æã€‚ For instance: è®¡ç®—ä¸¤ä¸ªç¾¤ä½“é—´fstå€¼ï¼ŒS1.txtå’ŒS2.txtæ˜¯åŒ…å«äº†å„ç¾¤ä½“çš„æ ·å“å vcftools --vcf all.vcf --weir-fst-pop S1.txt --weir-fst-pop S2.txt ---fst-window-size 200000 --fst-window-step 100000 --out outfile 3.2 æ ¸è‹·é…¸å¤šæ€æ€§ç»Ÿè®¡ æ ¸è‹·é…¸å¤šæ ·æ€§Ï€åæ˜ äº†ç¾¤ä½“çš„å¤šæ€æ€§ã€‚ä¸€èˆ¬æ¥è¯´å—é€‰æ‹©ç¨‹åº¦è¶Šé«˜çš„ç¾¤ä½“ï¼Œé—ä¼ å¤šæ ·æ€§è¶Šå•ä¸€ï¼›é‡ç”Ÿç¾¤ä½“é—ä¼ å¤šæ ·æ€§è¾ƒé«˜ã€‚åŸºäºÏ€å¯ä»¥è¿›è¡Œé€‰æ‹©æ€§æ¶ˆé™¤åˆ†æã€‚ For instance: è®¡ç®—ç¾¤ä½“æ ¸è‹·é…¸å¤šæ€æ€§ vcftools --vcf infile.vcf --window-pi 1000 --window-pi-step 1000 --out filename è®¡ç®—æ‰€æœ‰å•ç‚¹æˆ–æ‰€é€‰å¤šç‚¹(--positions)çš„æ ¸è‹·é…¸å¤šæ€æ€§ vcftools --vcf infile.vcf --site-pi (--positions SNP_list.txt) --out filename 3.3 Tajima's Dè®¡ç®— Tajima's Dè¡¡é‡ç¾¤ä½“ä¸­æ€§è¿›åŒ–ç†è®ºçš„æŒ‡æ ‡ï¼Œè¶Šåç¦»0ï¼Œå—é€‰æ‹©ç¨‹åº¦è¶Šé«˜ã€‚ For instance: è®¡ç®—Tajima's D vcftools --vcf infile.vcf --TajimaD 100000 --out filename 3.4 LDç­‰å…¶ä»–ç»Ÿè®¡å‚æ•° å‚æ•° è¯´æ˜ --hap-r2ï¼Œ--geno-r2 LDç»Ÿè®¡ --TsTV SNPè½¬æ¢/é¢ æ¢ç»Ÿè®¡ --freq æ¯ä¸ªSNPç­‰ä½åŸºå› é¢‘ç‡ç»Ÿè®¡ --counts æ¯ä¸ªSNPç­‰ä½åŸºå› ä¸ªæ•°ç»Ÿè®¡ --SNPdensity SNPé¢‘ç‡ --missing-indv æ ·å“ç­‰ç¼ºå¤±ç‡ç»Ÿè®¡ --missing-site SNPç¼ºå¤±ç‡ç»Ÿè®¡ --depth ä¸ªä½“æ·±åº¦ç»Ÿè®¡ --site-depthï¼Œ--site-mean-depth ä½ç‚¹æ·±åº¦ï¼Œå¹³å‡æ·±åº¦ç»Ÿè®¡ 4.å…¶ä»– 4.1 æ¯”è¾ƒä¸¤ä¸ªvcfæ–‡ä»¶ æ¯”è¾ƒ2ä¸ªç¾¤ä½“çš„vcfæ–‡ä»¶ï¼ŒåŒ…å«å¤šä¸ªé€‰é¡¹ï¼Œå¸¸ç”¨çš„å¦‚ â€“diff-site â€“diff-indv vcftools --gzvcf input_file1.vcf.gz --gzdiff input_file2.vcf.gz --diff-site --out in1_v_in2 4.2 VCFè½¬åŒ–plink vcftoolsè¿˜å¯ä»¥è½¬åŒ–å¤šç§æ ¼å¼ï¼Œå¸¸ç”¨çš„æ˜¯è½¬åŒ–æˆplinkæ ¼å¼ã€‚ å‚æ•°ï¼š--012ï¼Œ--IMPUTEï¼Œ--ldhat-genoï¼Œ--BEAGLE-GLï¼Œ--BEAGLE-PLï¼Œ--plink vcftools --vcf infile.vcf --plink --chr 1 --out output_in_plink åˆå¹¶å¤šä¸ªvcfæ–‡ä»¶(ä¹Ÿå¯ä»¥ä½¿ç”¨GATKï¼‰ bcftools merge A.vcf B.vcf &gt; AB.vcf 4.3 å…¶ä»–æ ¼å¼è½¬æ¢ --012 --IMPUTE --ldhat --ldhat-geno --BEAGLE-GL --BEAGLE-PL ---; å…·ä½“ä½¿ç”¨æ–¹æ³•å¯ä»¥çœ‹ vcftoolsè¯´æ˜ 5.æ€»ç»“ æœ¬æ–‡æ—¨åœ¨å­¦ä¹ å’Œæ•´ç†ç”¨åˆ°çš„å¤§éƒ¨åˆ†vcftoolsæ–¹æ³•ï¼Œå¦‚æœæœ‰æ‰€çº°æ¼æ¬¢è¿æŒ‡æ­£ï¼ğŸ˜Š ","link":"https://cauliyang.github.io/post/vcftools-1/"},{"title":"K-means  by Python","content":"æœ¬ç¯‡æ–‡ç« ï¼Œè¯¦ç»†è®°å½•å¦‚ä½•ä½¿ç”¨Pythonè¿›è¡ŒK-meansï¼Œåˆ†åˆ«ç”¨ä¸¤ç§æ–¹æ³•å®ç°ï¼Œå¹¶è®°å½•å¦‚ä½•é€‰å–Kå€¼ï¼Œå¹¶è¿›è¡Œå¯è§†åŒ–è¯„ä¼°ç»“æœã€‚ img{ width: 80%; padding-left: 10%; } 1.K-meansæ¦‚å¿µä»‹ç» 1.1 åŸºç¡€æ¦‚å¿µ K-meansæ˜¯ä¸€ç§å¸¸ç”¨çš„æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œç”¨äºåœ¨æ— æ³•çŸ¥é“æ­£ç¡®ç­”æ¡ˆä¸‹å‘ç°æ•°æ®ä¸­éšè—çš„ç»“æ„ï¼Œèšç±»çš„ç›®æ ‡æ˜¯åœ¨æ•°æ®ä¸­æ‰¾åˆ°è‡ªç„¶åˆ†ç»„ï¼Œç¡®ä¿ç›¸åŒé›†ç¾¤ä¸­å…ƒç´ æ¯”ä¸åŒçš„é›†ç¾¤ä¸­å…ƒç´ æ›´åŠ ç›¸ä¼¼ã€‚K-meansæ–¹æ³•éå¸¸æ“…é•¿è¯†åˆ«çƒå½¢æ•°æ®ï¼Œå…¶ç¼ºç‚¹æ˜¯å¿…é¡»æŒ‡å®šé›†ç¾¤æ•°Kã€‚å¦‚æœé€‰æ‹©Kå€¼ä¸å½“ä¼šé€ æˆåˆ†ç¾¤æ•ˆæœä¸å¥½ï¼Œåæ–‡å°†ä¼šä»‹ç»ä¸¤ç§æ–¹æ³•ç”¨æ¥è¯„ä¼°Kå€¼åŠåˆ†ç¾¤æ•ˆæœã€‚å¹¶ä¸”æœ¬æ–‡é‡‡ç”¨ä¸¤ç§æ–¹å¼å®ç°K-means ä½¿ç”¨scikit-learnæ¨¡å—è¿›è¡ŒK-meansèšç±»åˆ†æã€‚ ä»å¤´æ‰‹å†™K-meansæ–¹æ³•ã€‚ 1.2 ç®—æ³•åŸç† éšæœºåœ¨æ ·æœ¬ä¸­é€‰å–Kè´¨å¿ƒä½œä¸ºèµ·å§‹èšç±»çš„ä¸­å¿ƒã€‚ å°†æ¯ä¸ªæ ·æœ¬æ ¹æ®æ¬§å¼è·ç¦»åˆ†åˆ°æœ€è¿‘çš„è´¨å¿ƒÎ¼\\muÎ¼æ‰€åœ¨çš„ç¾¤ä¸­ã€‚ å°†æ‰€æœ‰æ ·æœ¬åˆ†ç¾¤åï¼Œé‡æ–°è®¡ç®—ä»¥æ¯ä¸ªç¾¤çš„ä¸­å¿ƒä½œä¸ºæ–°çš„è´¨å¿ƒã€‚ é‡å¤2ï¼Œ3 ä¸¤æ­¥ï¼ŒçŸ¥é“è´¨å¿ƒä¸å†æ”¹å˜ï¼Œæˆ–è€…è¾¾åˆ°ç”¨æˆ·è‡ªå®šä¹‰çš„é˜ˆå€¼æˆ–æœ€å¤§è¿­ä»£æ•°ã€‚ æ¬§å¼è·ç¦»çš„è®¡ç®—æ–¹æ³•ä¸ºï¼š d(x,y)2=âˆ‘j=1m(xjâˆ’yj)2=âˆ£âˆ£xâˆ’yâˆ£âˆ£22d(x,y)^2 = \\sum^{m}_{j = 1}(x_j - y_j )^2 = ||x - y||^{2}_{2} d(x,y)2=j=1âˆ‘mâ€‹(xjâ€‹âˆ’yjâ€‹)2=âˆ£âˆ£xâˆ’yâˆ£âˆ£22â€‹ å…¶ä¸­jjjä»£è¡¨æ•°æ®çš„çº¬åº¦ã€‚ åŸºäºæ¬§å¼è·ç¦»æˆ‘ä»¬å¯ä»¥æŠŠåˆ†ç¾¤çš„è¿‡ç¨‹æè¿°ä¸ºä¸€ä¸ªä¼˜åŒ–çš„é—®é¢˜ï¼Œæ˜¯ä¸€ç§æœ€å°åŒ–ç¾¤å†…è¯¯å·®å¹³æ–¹å’Œï¼ˆSSEï¼‰çš„è¿­ä»£æ–¹æ³•ä¹Ÿè¢«ç§°ä¸ºç¾¤æƒ¯æ€§ã€‚ SSE=âˆ‘i=1nâˆ‘j=1kw(i,j)âˆ£âˆ£x(i)âˆ’Î¼(i)âˆ£âˆ£22SSE = \\sum^{n}_{i =1 } \\sum^{k}_{j=1 } w^{(i,j)}||x^{(i)} - \\mu^{(i)}||^{2}_{2} SSE=i=1âˆ‘nâ€‹j=1âˆ‘kâ€‹w(i,j)âˆ£âˆ£x(i)âˆ’Î¼(i)âˆ£âˆ£22â€‹ å…¶ä¸­ iiiä»£è¡¨æ ·æœ¬ç´¢å¼• jjjä»£è¡¨åˆ†ç¾¤ç´¢å¼• 2.ä½¿ç”¨scikit-learnå®ç°K-meansæ–¹æ³• 2.1 åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶å®ç°ç®—æ³• é¦–å…ˆå¯¼å…¥æ‰€éœ€è¦çš„æ¨¡å—ï¼š # import module import numpy as np from matplotlib import cm from sklearn.datasets import make_blobs from sklearn.cluster import KMeans import matplotlib.pyplot as plt from sklearn.metrics import silhouette_samples å› ä¸ºäºŒç»´æ•°æ®å¯æ˜¯ç®€å•çš„ç»˜åˆ¶åœ¨ç¬›å¡å°”åæ ‡ç³»ä¸Šï¼Œæ‰€ä»¥ç”ŸæˆäºŒç»´æµ‹è¯•æ•°æ®è¿›è¡Œæµ‹è¯•ï¼š # creat tested data X, y = make_blobs(n_samples=150, # volume of data n_features=2, # number of feature centers=3, # number of centroid cluster_std=0.5, # distribution of data shuffle=True, random_state=0) ç»˜å›¾æŸ¥çœ‹åŸå§‹æ•°æ®ï¼š # plot tested data plt.figure() plt.scatter(X[:, 0], X[:, 1], c='white', marker='o', edgecolor='black', s=50) plt.grid() plt.show() ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºåˆ›å»ºçš„æµ‹è¯•æ•°æ®æœ‰æ˜æ˜¾çš„åˆ†ç¾¤æƒ…å†µ,å½“ç„¶åœ¨çœŸå®çš„æ•°æ®å½“ä¸­åŸå§‹æ•°æ®å¯èƒ½æ²¡æœ‰è¿™ä¹ˆç†æƒ³ã€‚æˆ‘ä»¬å…ˆåœ¨æ²¡æœ‰æ¨ç†çš„æƒ…å†µä¸‹ç¡®å®šKçš„å€¼ä¸º3ã€‚ # k-means km = KMeans(n_clusters=3, # K value init='random', n_init=10, # number of repeatation max_iter=300, tol=1e-4, random_state=0) # predict labels y_km = km.fit_predict(X) æˆ‘ä»¬è¿›è¡Œå¯è§†åŒ–åˆ†ç¾¤ç»“æœï¼š #creating function of ploting graph for reusing def plot_res(y_km, X, n_cluster): # init colors and markers colors = ['lightgreen', 'orange', 'lightblue'][:n_cluster] markers = ['s', 'o', 'v'][:n_cluster] # plot the cluster to comfirm the result of k-meams by sklearn for i, (c, m) in enumerate(zip(colors, markers)): plt.scatter(X[y_km == i, 0], X[y_km == i, 1], s=50, c=c, marker=m, edgecolor='black', label=f'cluster {i}') # plot centroipd of different clusters plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker='*', c='red', edgecolors='black', label='centroids') # plot lengend plt.legend(scatterpoints=1) # plot grid plt.grid() plt.show() å¯ä»¥æ˜æ˜¾çœ‹åˆ°åˆ†ç¾¤æ•ˆæœååˆ†æ˜æ˜¾ã€‚ä¸è¿‡å…¶ä¸­è¿˜æœ‰è®¸å¤šé—®é¢˜ï¼š å¦‚ä½•ç¡®å®Kå€¼ å¦‚ä½•è¯„ä¼°åˆ†ç¾¤è´¨é‡ 2.2 å¦‚ä½•é€‰å–Kå€¼ ä¸‹é¢ä»‹ç»å¦‚ä½•ä½¿ç”¨è‚˜è§£æ³•é€‰å–åˆé€‚çš„Kå€¼ï¼Œè‚˜è§£æ³•ç›®çš„æ˜¯æ‰¾å‡ºSSEå˜åŒ–å¹…åº¦æœ€å¤§çš„Kå€¼ã€‚ä½¿ç”¨km.inertia_ å³å¯è°ƒå‡ºSSESSESSEçš„å€¼ã€‚ distortions = [] # test different numbers of cluster to pick up the best K for i in range(1, 11): km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0) km.fit(X) distortions.append(km.inertia_) æµ‹è¯•1-11çš„Kå€¼é€‰å–ï¼Œå¹¶è¿›è¡Œå¯è§†åŒ–æŸ¥çœ‹ç»“æœã€‚ # plot the tested result for the best K plt.plot(range(1, 11), distortions, marker='o') plt.xlabel('Number of clusters') plt.ylabel('Distortion') plt.show() ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºåœ¨Kå€¼ä¸º3çš„æ—¶å€™ï¼ŒSSESSESSEå˜åŒ–å¹…åº¦æœ€å¤§ï¼Œå³å¯å¾—çŸ¥Kä¸º3æ˜¯æœ€ä¼˜è§£ã€‚ 2.3 å¦‚ä½•è¯„ä¼°åˆ†ç¾¤çš„è´¨é‡ è¯„ä»·èšç±»è´¨é‡çš„ä¸€ç§æ–¹æ³•æ˜¯è½®å»“åˆ†æï¼Œä»–å¯ä»¥åº”ç”¨äºå…¶ä»–èšç±»ç®—æ³•ï¼Œåº¦é‡å…¶ç´§å¯†ç¨‹åº¦ã€‚è®¡ç®—è½®å»“ç³»æ•°çš„æ­¥éª¤ä¸ºï¼š è®¡ç®—é›†ç¾¤å†…èšåº¦a(i)a^{(i)}a(i)ï¼Œå³æ ·æœ¬x(i)x^{(i)}x(i)äºé›†ç¾¤å†…æ‰€æœ‰å…¶ä»–ç‚¹ä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚ è®¡ç®—é›†ç¾¤ä¸æœ€è¿‘é›†ç¾¤çš„åˆ†ç¦»åº¦b(i)b^{(i)}b(i),å³æ ·æœ¬x(i)x^{(i)}x(i)ä¸æœ€è¿‘é›†ç¾¤å†…æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è·ç¦»ã€‚ è®¡ç®—è½®å»“ç³»æ•°s(i)s^{(i)}s(i)ï¼Œå³é›†ç¾¤å†…èšåº¦ä¸é›†ç¾¤åˆ†ç¦»åº¦ä¹‹å·®ï¼Œé™¤ä»¥ä¸¤è€…ä¸­è¾ƒå¤§çš„ä¸€ä¸ªã€‚ s(i)=b(i)âˆ’a(i)maxâ¡{b(i),a(i)}s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{\\max \\{{b^{(i)},a^{(i)}}\\}} s(i)=max{b(i),a(i)}b(i)âˆ’a(i)â€‹ è½®å»“ç³»æ•°çš„èŒƒå›´åœ¨-1åˆ°1ä¹‹é—´ï¼Œå¦‚æœé›†ç¾¤åˆ†ç¦»åº¦å’Œé›†ç¾¤å†…èšåº¦ç›¸ç­‰ï¼Œå³b(i)=a(i)b^{(i)}=a^{(i)}b(i)=a(i)ã€‚é‚£ä¹ˆè½®å»“ç³»æ•°ä¸º0ï¼Œå¦‚æœb(i)&gt;&gt;a(i)b^{(i)} &gt;&gt; a^{(i)}b(i)&gt;&gt;a(i) åˆ™æ¥è¿‘ç†æƒ³çš„è½®å»“ç³»æ•° 1 ã€‚ å¯ä»¥ä½¿ç”¨scikit-learnä¸­metricä¸­çš„silhouette_samplesè®¡ç®—æ ·æœ¬çš„è½®å»“ç³»æ•°ã€‚ä¹Ÿå¯ä»¥æ›´æ–¹ä¾¿çš„ä½¿ç”¨silhouette_scoresç›´æ¥è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è½®å»“ç³»æ•°ã€‚ä¸‹é¢æ˜¾ç¤ºKå€¼åŸºäº3çš„åˆ†ç¾¤ç»“æœã€‚ # we can use the graph of silhouette to evaluate result km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=10, tol=1e-04, random_state=0) y_km = km.fit_predict(X) ç»˜åˆ¶è½®å»“å›¾è¿›è¡Œå¯è§†åŒ–ï¼Œç›´è§‚çš„æŸ¥çœ‹ç¾¤å†…è½®å»“ç³»æ•°ã€‚ # difining fuction of plot-silhouette for reusing # plot the graph of silhouette def plot_sil(y_km, X): cluster_lables = np.unique(y_km) n_clusters = cluster_lables.shape[0] # using function of silhouette in sklearn to get silhouete scores silhouette_vals = silhouette_samples(X, y_km, metric='euclidean') # plot the graph y_ax_lower, y_ax_upper = 0, 0 yticks = [] for i, c in enumerate(cluster_lables): # get values of each cluster c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() # sort value for ploting y_ax_upper += len(c_silhouette_vals) color = cm.jet(float(i) / n_clusters) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color) yticks.append((y_ax_lower + y_ax_upper) / 2) y_ax_lower += len(c_silhouette_vals) silhouette_avg = np.mean(silhouette_vals) # get the label of yticks plt.axvline(silhouette_avg, color='red', linestyle='--') # plot the avaerage of silhouette plt.yticks(yticks, labels=cluster_lables) plt.ylabel('Cluster') plt.xlabel('Silhouette coefficient') plt.show() ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºè½®å»“ç³»æ•°ä¸æ¥è¿‘äº0ï¼Œä¸”æ¥è¿‘äº1è¡¨æ˜æˆ‘ä»¬çš„åˆ†ç¾¤ç»“æœè‰¯å¥½ã€‚ä¸”åœ¨å›¾ä¸­è½®å»“ç³»æ•°çš„é«˜åº¦ä»£è¡¨ç¾¤å†…æ ·æœ¬æ•°é‡ï¼Œå¦‚æœæ ·æœ¬æ•°é‡ç›¸å·®å¤ªå¤§ï¼Œè¯´æ˜åˆ†ç¾¤æ•ˆæœä¸æ˜¯å¾ˆå¥½ã€‚å›¾ä¸­è™šçº¿è¡¨ç¤ºå¹³å‡è½®å»“ç³»æ•°ã€‚ ä¸ºæ›´å¥½çš„ç†è§£è½®å»“ç³»æ•°çš„ä½¿ç”¨ï¼Œå°†Kå€¼å˜ä¸º2ï¼Œè¿›è¡Œèšç±»ã€‚ km = KMeans( n_clusters=2, # value of k has changed init='k-means++', n_init=10, max_iter=10, tol=1e-04, random_state=0) y_km = km.fit_predict(X) ä½¿ç”¨ä¸Šæ–¹ä½œå›¾å‡½æ•°ï¼Œå…ˆè§‚å¯Ÿåˆ†ç¾¤æ•ˆæœã€‚ ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºåˆ†ç¾¤æ•ˆæœå¾ˆå·®ï¼Œå¯è§†åŒ–è½®å»“ç³»æ•°æŸ¥çœ‹ç»“æœã€‚ ä¸¤ä¸ªç¾¤çš„é«˜åº¦ä¸ä¸€è‡´è¡¨æ˜åˆ†ç¾¤æ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ï¼Œä¸”æœ‰çš„æ ·æœ¬è½®å»“ç³»æ•°æä½æ¥è¿‘äº0ã€‚è¡¨ç¤ºåˆ†ç¾¤æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œéœ€è¦é‡æ–°æ€è€ƒKå€¼çš„é€‰å–ã€‚ 3. K-means from scratch æˆ‘ä»¬æ ¹æ®ç®—æ³•åŸç†ä½¿ç”¨Pythonä¸€æ­¥æ­¥å®ç°K-meansï¼Œé¦–å…ˆå±•ç¤ºæˆ‘ä»¬æ‰€ç”¨åˆ°çš„æ•°æ®é›†ï¼Œæœ‰å…³åŸºå› åœ¨ä¸åŒæ¡ä»¶ä¸‹å¤„ç†çš„è¡¨è¾¾æ•°æ®ï¼Œå…¶ä¸­åŸºå› æ•°é‡ä¸ºæ ·æœ¬æ•°é‡ï¼Œå¤„ç†æ–¹å¼ä¸ºçº¬åº¦ã€‚å¹¶ä¸”è®¾è®¡ä¸ºTerminalç«¯ä½¿ç”¨ã€‚ ç»ˆç«¯ä½¿ç”¨æ–¹æ³•ä¸ºï¼š Usage : python k_mean.py k data max_it (cetroids) å…¶ä¸­ k_mean.py ä¸ºç¨‹åºè„šæœ¬ k ä¸ºåˆ†ç¾¤æ•°é‡ data ä¸ºåŸå§‹æ•°æ®æ–‡ä»¶ max_it ä¸ºæœ€å¤§é€’å½’æ¬¡æ•° centroids ä¸ºåˆå§‹çš„è´¨å¿ƒï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©æä¾›æˆ–è€…ä¸æä¾› åŸå§‹æ•°æ®ï¼š gene_expression treat_1 treat_2 ... g_1 0.2 0.5 ... g_2 1.4 1.6 ... ... 4.2 2.1 ... 3.1 Get parameters from terminal å¯¼å…¥æ‰€éœ€çš„æ¨¡å— # import modules import sys import time import numpy as np from collections import Counter from operator import itemgetter ä»ç»ˆç«¯è·å–ç”¨æˆ·ä¼ é€’å‚æ•°ï¼š # defining function for getting parameters from terminal def get_argv(): ''' get the parameters entered by user and return the dictionary parameters ''' # get parameters argv_list = sys.argv # init parameters argv_name = ( 'data', 'init_cetroids', 'gene_num', # numbers of row 'ndim', 'max_it', # max numbers of iter 'k') # determine whether user provide init-centroids according numbers of parameters if len(argv_list) == 4: # if not provide init-centroid _, k, file, max_it = argv_list # get information of file argv_tuple = get_Cetroid(file, int(k)) + (int(max_it), int(k)) elif len(argv_list) == 5: # if provide init-centroid _, k, file, max_it, cetroid_file = argv_list # get information argv_tuple = get_Cetroid( file, int(k), cetroid_file=cetroid_file) + (int(max_it), int(k)) elif len(argv_list) &lt; 4: # if numbers of parameters is less than need parameters then print help print(''' ------------------------------------------------- Requirement : numpy Usage : python k_mean.py k data max_it (cetroids) Result_file : kmeans.out Contact : &lt;liyangyang&gt; &lt;yangyangli.vince@gmail.com&gt; ------------------------------------------------- ''') sys.exit(0) # return dictionary parameters return dict(zip(argv_name, argv_tuple)) 3.2 Creating function of report # difining function of reporting summary def summary(kw, tim, kmeanout='kmeans.out'): ''' Create a summary function, count recursive times, run time, etc.ã€‚ ''' # statistics for each Cluster data def print_cluster(kmean=kmeanout): # evaluate data counter = Counter(np.loadtxt(kmean, dtype=int)[:, 1]) # produce report for clu, num in counter.most_common(): print(f' Cluster_{clu} : {num}') # creat statistic header print('{:-^40}\\n'.format('Summary')) # print statstic report of each cluster print_cluster() # print overall information print(f''' Max_iter_number : {kw['max_it']} Cluster_number :{kw['k']} Time : {tim:.2f}s Date : {time.asctime()}''') # creat statistic tial print('{:-&lt;40}\\n'.format('-')) 3.3 Calculating Euclidean distance # defining function to calculate Euclidean distance def eucl_Distance(init_cetroids, piece_data): ''' Calculate the Euclidean distance between each data and the centroid ''' distance = np.sqrt(np.sum((init_cetroids - piece_data)**2, axis=1)) # return euclidean distance return distance 3.4 Getting centroid information and recursive function def get_Cetroid(file, k, cetroid_file=None): ''' This function is used to get raw data file information: raw data, centroid, data volume, feature dimension ''' # get content of file data = np.loadtxt(file) # get information: data volume, feature dimension gene_num, ndim = data.shape # Determine whether the user provides a centroid, and randomly if not provided if not (cetroid_file): # init centroid init_cetroids = np.zeros((k, ndim)) # provied centroid randomly for i in range(k): index = int(np.random.uniform(0, gene_num)) init_cetroids[i, :] = data[index, :] else: # if users provide centroid init_cetroids = np.loadtxt(cetroid_file) # return information return (data, init_cetroids, gene_num, ndim) def iter_Cetroid(**argv): ''' Iterative clustering results ''' # get neccessary parameters data, init_cetroids, gene_num, ndim, max_it, k = argv.values() # init results Result = np.zeros((gene_num, 2), dtype=int) # grouping data according to Euclidean distance for i in range(gene_num): # get Euclidean distance distance = eucl_Distance(init_cetroids, data[i, :]) # get the label of shortest distance cluster = distance.argmin() # grouping Result[i, :] = np.array([i, cluster]) # verify that the results of the iteration are stable and return a new centroid Handle, argv['init_cetroids'] = assert_Result(data, init_cetroids, Result, k) # return informattion return Result, Handle.all(), argv, max_it 3.5 Creating Body function and Main function def run(arg_dict, it_num=0): ''' the body of k-means ''' # perform an iteration and verify that the results are stable # then calculate the new centroid to be returned in dictionary form Result, handle, arg_dict, max_it = iter_Cetroid(**arg_dict) # determine whether the condition of end iteration is reached if not (handle) and (it_num &lt; max_it): # if not reach and the iteration continues it_num += 1 # print numbers of iteration print(f'...ing Iter Number :{it_num}') # recursive iteration run(arg_dict, it_num=it_num) # if reach condition else: # change lable,like change the lables from 0,1,2 to 1,2,3 Result = Result + 1 count_1 = Counter(Result[:, 1]) # save result file np.savetxt('kmeans.out', Result, fmt='%d') def main(): ''' the program main function, integrate workflow, and generate reports ''' # get start time TIC = time.time() # get parameter through terminal ARGV = get_argv() # running the body function of k-means run(ARGV) # get end time TOC = time.time() # generate report summary(ARGV, TOC - TIC) 4. Summary æœ¬ç¯‡æ–‡ç« è¯¦ç»†è®°å½•ä¸¤ç§æ–¹å¼å®ç°K-meansæ–¹æ³•ï¼Œå¹¶ä¸”è®°å½•å¦‚ä½•é€‰å–Kå€¼ï¼Œå¦‚ä½•è¯„ä¼°èšç±»è´¨é‡ã€‚æœ¬æ–‡æœ€ç»ˆæ¶‰åŠçš„ä»£ç éƒ½ä¼šåœ¨Jupyter notebookæ‰¾åˆ°,å¹¶ä¸”ä½¿ç”¨è„šæœ¬ç¨‹åº è°¢è°¢è§‚çœ‹ï¼Œæ¬¢è¿äº¤æµï¼ğŸ˜ ","link":"https://cauliyang.github.io/post/k-means-cluster-by-python/"},{"title":"PCA by Python ","content":"This article documents two methods of PCA analysis using Python, and visualizes 2-dimensional results. img{ width: 70%; padding-left: 1%; } 1.Intrduction 1.1 What's PCA? When it comes to methods of reducing dimension, PCA that is an unsupervised linear transformation technique, must be not ignored. Moreover, if you want to know the subtle relationships among data set and reduce the computational complexity in downstream analysis, the PCA may be your best choice! Meanwhile, if you would like to present your data in a 2-dimension or 3-dimension coordinate system, and PCA would sweep your problems! What is reducing dimension? I will show you an example as follows: First, suppose you have a five-dimensional data set : Id 1-d 2-d 3-d 4-d 5-d data-1 1 2 3 4 5 data-2 6 7 8 9 10 .. .. .. .. .. Then, you could pick up PC1 and PC2 after PCA to reduce dimension for plotting: Id PC1 PC2 data-1 0.3 0.6 data-2 0.1 1.2 .. .. .. PC1 and PC2 are the result obtained through data is projection on the unit vectors, which enable result to have the most biggest variance(means its distribution is wide) and to be irrelevant(covariance = 0). 1.2 Algorithm Normalize ddd dimension raw data Creat the covariance matrix Calculate the eigenvalues of the covariance matrix and the corresponding eigenvectors The eigenvectors are sorted in the matrix according to the corresponding feature value, and the first k rows are formed into a matrix WWW.(k&lt;&lt;dk&lt;&lt;dk&lt;&lt;d) Y=xWY = xWY=xW is the result after reducing dimension to k dimension Note: There are two prerequisites for conducting PCA Raw data has no NA The raw data should be normalized 2. PCA from scratch Importing necessary modules import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Creating raw data # get data set df_wine = pd.read_csv( 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None, engine='python') # check data df_wine.head() Creating train and test data set # creat train and test data set X, y = df_wine.iloc[:, 1:], df_wine.iloc[:, 0] x_train,x_test,y_train,y_test = \\ train_test_split(X,y,test_size = 0.3 , stratify= y, random_state = 0 ) Standarding the features # create standard instance sc = StandardScaler() # standard data x_train_std = sc.fit_transform(x_train) x_test_std = sc.fit_transform(x_test) Creating the covariance matrix and Getting eigenvectors and eigenvalues the calculation of the covriance matrix : Ïƒjk=1nâˆ‘i=1n(xj(i)âˆ’Î¼j)(xk(i)âˆ’Î¼k)\\sigma_{jk} = \\frac{1}{n} \\sum^{n}_{i=1}\\bigg(x_{j}^{(i)} - \\mu_j\\bigg)\\bigg(x_{k}^{(i)} - \\mu_k\\bigg) Ïƒjkâ€‹=n1â€‹i=1âˆ‘nâ€‹(xj(i)â€‹âˆ’Î¼jâ€‹)(xk(i)â€‹âˆ’Î¼kâ€‹) Then, using numpy.cov and numpy.linalg.eig to get the covariance matrix and eigenvectors respectively # calculate the covariance matrix cov_mat = np.cov(x_train_std.T) # Getting eigenvectors and eigenvalues eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) NOTE: there are 13 eigenvectors totally, the number of eigenvalues might be not as same as the number of features sometimes. Firstly, plotting the Variance interpretation ratio, which is obtained through eigenvalue Î»j\\lambda_jÎ»jâ€‹ divided by the sum of all the eigenvalues: Î»jâˆ‘j=1dÎ»j\\frac{\\lambda_j}{\\sum^d_{j=1}\\lambda_j} âˆ‘j=1dâ€‹Î»jâ€‹Î»jâ€‹â€‹ # get sum of all the eigenvalues tot = sum(eigen_vals) # get variance interpretation ratio var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) Besides, plotting the result to get in-depth understanding: plt.figure() # creat plot # creat bar plot plt.bar( range(1, 14), var_exp, alpha=0.5, label='individual explained variance', ) # creat step plot plt.step(range(1, 14), cum_var_exp, where='mid', label='cumulative explained variance') # add label plt.ylabel('Explained variance ratio') plt.xlabel('Principal component index') # add legend plt.legend(loc='best') # save picture plt.savefig('pca_index.png', format='png', bbox_inches='tight', dpi=300) We can conclude that PC1 only takes account for about 40%. Furthermore, the sum of PC1 and PC2 have 60% variance. Selecting the first k values to form matrix WWW # integrate eigenvalues and eigenvectors eigen_paris = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))] # sort according to eigenvalues eigen_paris.sort(key=lambda x: x[0], reverse=True) # pick up the first 2 eigenvalues w = np.hstack( [eigen_paris[0][1][:, np.newaxis], eigen_paris[1][1][:, np.newaxis]]) # check matrix x w Tranforming raw data # reduce dimension x_train_pca = x_train_std.dot(w) # check resulted data x_train_pca.shape (124, 2) Then plotting the result and putting the label in terms of original info, but keeping in mind PCA is unsupervised learning skill without labels # init colors and markers colors = ['r', 'b', 'g'] markers = ['s', 'x', 'o'] # plot scatter for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(x_train_pca[y_train == l, 0], x_train_pca[y_train == l, 1], c=c, label=1, marker=m) # add label and legend plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.savefig('distribution.png', format='png', bbox_inches='tight', dpi=300) 3. PCA by scikit-learn we can conduct PCA easily by sklearn Importing modules from sklearn.decomposition import PCA from matplotlib.colors import ListedColormap from sklearn.linear_model import LogisticRegression Defining function of plot_decision_region def plot_dicision_regions(X, y, classifier, resolution=0.02): # init markers and colors markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # creat info for plot region x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) # test classifier's accurate z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) z = z.reshape(xx1.shape) # plot desicion region plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap) # set x,y length plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot result for idx, cl in enumerate(np.unique(y)): plt.scatter( x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.6, color=cmap(idx), edgecolor='black', marker=markers[idx], label=cl, ) PCA by sklearn # creat pca instance pca = PCA(n_components = 2 ) # creat classifier instance lr = LogisticRegression() # reduce dimension for data set x_train_pca = pca.fit_transform(x_train_std) x_test_pca = pca.transform(x_test_std) # classify x_train_pca lr.fit(x_train_pca,y_train) # plot dicision region plot_dicision_regions(x_train_pca,y_train,classifier=lr) # add info plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.show() We can see that the classifier's accurate is excellent according to actual labels TIPS: You can set n_components = None, and the result would retain all principle components. Moreover, you could call explained_variance_ration_ to use variance explanation ratio. 3.Summary All the above are the main content, welcome everybody communicates with me! ğŸ¤  Reference book : Python marchine learning ","link":"https://cauliyang.github.io/post/pca-by-python-2/"},{"title":"Blog Ideas ","content":"This Blog record learning process, welcome everyone to communicate with me!ğŸ˜Š Ideas This blog is used as a public note to record my learning process. I also hope to help others~. If you have an infringement problem, then you can contact me. if it is true ,I will delete it immediately. ","link":"https://cauliyang.github.io/post/bo-ke-xiang-fa/"},{"title":"About ","content":"My name is Yangyang Li , I would like to take advantage of mathematics and programming to change the world and detect the secrets of life, welcome everybody to communicate with me! ğŸ¤“ ","link":"https://cauliyang.github.io/post/about/"}]}