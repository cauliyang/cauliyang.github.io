[{"content":" Latex # LaTeX functions as an intricate tool for the meticulous creation of scientific documents, enriched by a comprehensive ecosystem and an extensive assortment of templates. I habitually utilize LaTeX for a plethora of activities, including the annotation of notes, the authoring of scholarly articles, and the formulation of thesis. GitHub serves as an indispensable medium for version management and collaborative endeavors. As such, automating the assembly procedure of LaTeX documents through GitHub would confer significant benefits.\nGitHub Action # In an era where automation reigns supreme, GitHub Actions distinguish themselves as a paramount utility for streamlining software operations. This blog explores the nuances of configuring a GitHub Action to autonomously compile LaTeX manuscripts, archive the resulting files, and initiate subsequent code commits. Embark with us on this enlightening journey. Utilizing the actions/checkout action, we commence by crafting a rudimentary YAML file designed to trigger the compilation sequence for the LaTeX manuscript.\nactions/checkout Action for checking out a repo TypeScript 4482 1499 name: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 To elevate the compilation workflow, the xu-cheng/latex-action action can be enlisted. This grants the flexibility to delineate auxiliary parameters, such as the root file and the destination directory for output.\nxu-cheng/latex-action :octocat: GitHub Action to compile LaTeX documents Shell 928 127 name: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 - name: Compile main.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;main.tex\u0026#34; latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=.\u0026#34; In this configuration, we utilize lualatex to compile main.tex, incorporating the shell_escape feature, with the output directed to the current directory. It is often advantageous to arrange the resultant files systematically. To illustrate this, the example demonstrates how to modify the output directory for the LaTeX documents, designating ./gallery as the target location.\nname: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 - name: Compile main.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;main.tex\u0026#34; latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; Simultaneously, GitHub Actions simplifies the process of archiving the generated PDF for future reference or in-depth scrutiny.\nname: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 - name: Compile main.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;main.tex\u0026#34; latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=.\u0026#34; - name: Upload PDF file uses: actions/upload-artifact@v3 with: name: PDF path: main.pdf In this step, we upload the compiled main.pdf file, making it accessible for download from the artifact section of the corresponding GitHub Action, as illustrated in the subsequent figure.\nartifact Compilation of Multiple Documents # Additionally, the setup affords us the flexibility to compile an array of .tex files situated in nested directories. In this instance, we compile both main.tex and all .tex files residing within the source/ directory.\nroot_file: \u0026#34;source/*.tex\u0026#34; glob_root_file: true The resultant artifacts are neatly stored in the ./gallery directory.\nname: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 - name: Compile main.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;main.tex\u0026#34; latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; - name: Compile source/*.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;source/*.tex\u0026#34; glob_root_file: true latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; Committing Recent Modifications # Significantly, the GitHub actions also enable us to automate the task of committing alterations back to the repository.\nad-m/github-push-action GitHub actions to push back to repository eg. updated code JavaScript 1079 219 name: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 - name: Compile main.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;main.tex\u0026#34; latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; - name: Compile source/*.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;source/*.tex\u0026#34; glob_root_file: true latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; - name: Commit files run: | git config --local user.email \u0026#34;41898282+github-actions[bot]@users.noreply.github.com\u0026#34; git config --local user.name \u0026#34;github-actions[bot]\u0026#34; git status git add . git commit -m \u0026#34;update example and readme\u0026#34; -a - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} branch: main Utilizing Caching Mechanisms # Streamlining the workflow is effortlessly achieved through the deployment of GitHub Actions\u0026rsquo; caching capabilities, thereby reducing execution time and enhancing operational efficiency.\nname: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v3 - name: Cache LaTeX dependencies uses: actions/cache@v3 with: path: /usr/local/share/texmf key: ${{ runner.os }}-texmf-${{ hashFiles(\u0026#39;**/*.tex\u0026#39;) }} - name: Cache auxiliary files uses: actions/cache@v3 with: path: ./gallery key: ${{ runner.os }}-auxfiles-${{ hashFiles(\u0026#39;**/*.tex\u0026#39;) }} restore-keys: | ${{ runner.os }}-auxfiles- - name: Compile main.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;main.tex\u0026#34; latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; - name: Compile source/*.tex uses: xu-cheng/latex-action@v2 with: root_file: \u0026#34;source/*.tex\u0026#34; glob_root_file: true latexmk_use_lualatex: true latexmk_shell_escape: true args: \u0026#34;-output-directory=./gallery\u0026#34; - name: Commit files run: | git config --local user.email \u0026#34;41898282+github-actions[bot]@users.noreply.github.com\u0026#34; git config --local user.name \u0026#34;github-actions[bot]\u0026#34; git status git add . git commit -m \u0026#34;update example and readme\u0026#34; -a - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} branch: main Examples # Utilizing GitHub Actions, we can construct a nuanced, yet efficient, workflow for the compilation of LaTeX manuscripts. The aforementioned examples merely act as foundational elements; truly, the possibilities are boundless.\ncauliyang/learn_tikz learn_tikz TeX 1 0 cauliyang/learning_notes learning_notes TeX 1 0 ","date":"1 September 2023","permalink":"/latex/002-autonamte-latex-build/","section":"LaTex Typesetting","summary":"Latex # LaTeX functions as an intricate tool for the meticulous creation of scientific documents, enriched by a comprehensive ecosystem and an extensive assortment of templates.","title":"Autonamte Latex Build Processing"},{"content":"","date":"1 September 2023","permalink":"/tags/ci/","section":"Tags","summary":"","title":"CI"},{"content":"","date":"1 September 2023","permalink":"/tags/latex/","section":"Tags","summary":"","title":"Latex"},{"content":"","date":"1 September 2023","permalink":"/series/latex-typesetting/","section":"Series","summary":"","title":"LaTex Typesetting"},{"content":" With LaTeX, the beauty of knowledge finds its worthy canvas. ","date":"1 September 2023","permalink":"/latex/","section":"LaTex Typesetting","summary":"With LaTeX, the beauty of knowledge finds its worthy canvas.","title":"LaTex Typesetting"},{"content":"","date":"1 September 2023","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 September 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" ","date":"1 September 2023","permalink":"/","section":"Welcome to my blog!","summary":" ","title":"Welcome to my blog!"},{"content":"","date":"29 August 2023","permalink":"/tags/c++/","section":"Tags","summary":"","title":"C++"},{"content":"","date":"29 August 2023","permalink":"/tags/cuda/","section":"Tags","summary":"","title":"CUDA"},{"content":" 1. Deep Learning Inference # Currently, both Rust and C++ are emerging as noteworthy contenders in the realm of deep learning, primarily due to their efficiency despite Python\u0026rsquo;s prevailing dominance in model training. While Python continues to commandeer the training phase, it lags in performance during inference. Large Language Models (LLMs), such as ChatGPT, have burgeoned since their inception, instigating a competitive frenzy among corporations and research organizations alike. This has led to an explosion of various LLMs, although not all exhibit equal utility or value.\nFurthermore, expansive image generation models, such as stable diffusion and midjourney, are capturing considering attention. Both LLMs and these advanced image generators possess a common characteristic: they are engineered with billions of parameters, occupying gigabytes of memory.\nPrivacy remains a paramount concern for users of LLMs. The prevailing sentiment is a reluctance to have our data harvested by these computational behemoths without compensation. Recent open-source entrants like llama aim to challenge established players like ChatGPT while prioritizing user privacy. The optimal course of action in terms of privacy is to operate these models on personal devices. However, the hardware constraints, particularly the lack of powerful CPUs or GPUs, pose a challenge. Therefore, there is an urgent impetus within the community to optimize model execution speed. The llama.cpp inference framework, written in C/C++ and utilizing advanced techniques like SIMD, quantization, mixed precision, and acceleration for different backend (GPU, MKL, etc.), addresses this need. Contributions from the open-source community have enabled popular models to be inferenced via llama.cpp, thereby democratizing access to LLMs across various devices.\nWhile C++ and llama.cpp off sbstantial benefits, Rust carves out its own niche, particularly excelling in WebAssembly and the development of web or GUI application1. Server nascent yet rapidly evolving deep learning framework, such as dfdx, burn, and candle, are implemented purely in Rust and offer cross-platform compatibility with various accerlated backends.\nIn summary, upon completion of the model training phase, deployment can be effectively handled by either llama.cpp or Rust-based solutions. This facilitates the accessibility of large-scale deep learning models across an array of devices and platforms. As for my personal setup, I utilize a MacBook Pro with an M1 chip and rely on a remote High-Performance Computing (HPC) cluster for CUDA capabilities. Further details about the HPC setup are delineated in the subsequent figure.\nHPC 2. Check your CUDA driver version # Unfortunately, I continue to encounter issues while attempting to configure the CUDA environment on a High-Performance Computing (HPC) cluster. This blog post will chronicle my journey to surmount these obstacles without administrative right.\nIn order to establish a functional CUDA environment for both Rust and C++, it\u0026rsquo;s imperative to confirm that all requisite CUDA libraries and headers are correctly installed. Subsequently, the build system must be configured to link against these specific libraries. Moreover, it may be necessary to delineate the appropriate compiler flags and paths to facilitate seamless CUDA integration.\nFor illustrative purposes, consider my personal device configuration:\nnvidia-smi CUDA Driver In the remote High-Performance Computing (HPC) cluster, the CUDA Driver is version 11.7. Typically, we lack administrative access, precluding us from updating the driver to a newer version. Consequently, it becomes necessary to install a matching CUDA 11.7 suite to ensure compatibility.\n3. Use CONDA to install CUDA and gcc/g++ # I advocate for the use of mamba as an alternative to conda, given its superior efficiency in resolving and installing dependencies. For the purposes of the ensuing example, I shall designate /home/mambaforge as the installation directory for mamba.\nDo not forget to change to your own installation location when you plan to give it a try. Create new environment # Let\u0026rsquo;s establish a fresh environment to circumvent any dependency conflicts. Python 3.10 currently serves as the stable release.\nmamba create -n cuda python=3.10 mamba activate cuda Install CUDA # We install the CUDA package from the [nvidia] channel, opting for a specific version by employing designated labels. In this instance, we are installing CUDA 11.7.\nmamba install cuda -c nvidia/label/cuda-11.7.0 Install compiler # Additionally, it\u0026rsquo;s imperative to install a compatible compiler; failing to do so may result in the utilization of system-default compilers, such as /usr/bin/gcc, during the code compilation process.\nmamba install gcc=11.0 gxx=11.0 cmake It\u0026rsquo;s essential to define the environment variable CUDA_ROOT in order to effectively utilize CUDA.\nCUDA_ROOT=/home/mambaforg/envs/cuda RUSTFLAGS=\u0026#34;-L/home/mambaforge/envs/cuda/lib/stubs\u0026#34; cargo run CUDA_ROOT=/home/mambaforg/envs/cuda g++ -o test test.cpp Let\u0026rsquo;s configure an environment-specific variable so as to obviate the need for setting CUDA_ROOT repeatedly.\nmamba env config vars set CUDA_ROOT=/home/mambaforg/envs/cuda mamba env config vars set RUSTFLAGS=\u0026#34;-L/home/mambaforge/envs/cuda/lib/stubs\u0026#34; Reactivating the environment to make the environment \u0026ldquo;alive\u0026rdquo;.\nmamba activate cuda 4. Quick start for candle # candle is a deep learning framework crafted in Rust, a programming language that excels in WebAssembly development. A suite of robust and maturing full-stack WebAssembly libraries, including leptos and dixous, further underscores Rust\u0026rsquo;s capabilities. Therefore, if the objective is to create a web application underpinned by deep learning technologies, Rust emerges as the significant choice. Let\u0026rsquo;s proceed to experiment with candle in conjunction with CUDA.\ncargo new test_candle cd test_candle Incorporate candle into the project\u0026rsquo;s dependencies, specifying CUDA as a featured attribute.\ncargo add candle_core --features cuda Let\u0026rsquo;s modify the src/main.rs file and initially conduct a test run on the CPU.\nuse candle_core::{Device, Tensor}; fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let device = Device::Cpu; let a = Tensor::randn(0f32, 1., (2, 3), \u0026amp;device)?; let b = Tensor::randn(0f32, 1., (3, 4), \u0026amp;device)?; let c = a.matmul(\u0026amp;b)?; println!(\u0026#34;{c}\u0026#34;); Ok(()) } cargo run Execute the code: In this instance, we are utilizing the GPU.\nuse candle_core::{Device, Tensor}; fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let device = Device::new_cuda(0)?; let a = Tensor::randn(0f32, 1., (2, 3), \u0026amp;device)?; let b = Tensor::randn(0f32, 1., (3, 4), \u0026amp;device)?; let c = a.matmul(\u0026amp;b)?; println!(\u0026#34;{c}\u0026#34;); Ok(()) } $ cargo run [[ 0.6323, -0.8924, 0.7706, 2.3862], [-0.1840, 0.1122, -0.3946, -0.9851]] Tensor[[2, 4], f32, cuda:0] CUDA Try more examples of candle # git clone https://github.com/huggingface/candle.git cd candle Whisper # Let\u0026rsquo;s assume that we have already configured the environment variables CUDA_ROOT and RUSTFLAGS.\ncargo run --examples whisper --features cuda --realease Alternatively, employ temporary environment variables for the session.\nCUDA_ROOT=\u0026#34;/home/mambaforge/env/cuda\u0026#34; RUSTFLAGS=\u0026#34;-L/home/mambaforge/env/cuda/lib/stubs\u0026#34; cargo run --examples whisper --features cuda --realease whisper Stable Diffusion # cargo run --example stable-diffusion --release --features cuda -- --prompt \u0026#34;a rusty robot holding a fire torch\u0026#34; Stable Diffusion The generated image:\ngenerated image 5. Quick start for llama.cpp # llama.cpp will be coming soon.\n6. Bonus # A bash script is used to apply an interactive computing node using slurm. Changing -p b1171 --account=b1171 if you use the script.\n#!/bin/bash set -e set -u set -o pipefail # Set default values use_gpu=0 gpu_number=1 # Display help message display_help() { local script_name=$(basename \u0026#34;$0\u0026#34;) echo \u0026#34;Usage: $script_name \u0026lt;time\u0026gt; \u0026lt;memory\u0026gt; [gpu_number]\u0026#34; echo echo \u0026#34; time The time parameter value in hours\u0026#34; echo \u0026#34; memory The memory parameter value in GB\u0026#34; echo \u0026#34; gpu_number The number of GPUs (default: 1)\u0026#34; exit 1 } # Check if both parameters are provided if [ \u0026#34;$#\u0026#34; -lt 2 ]; then display_help fi time=\u0026#34;$1\u0026#34; memory=\u0026#34;$2\u0026#34; # If third parameter exists, assign its value to gpu_number and set the use_gpu flag if [ \u0026#34;$#\u0026#34; -ge 3 ]; then gpu_number=\u0026#34;$3\u0026#34; use_gpu=1 fi echo \u0026#34;Time: $time hours\u0026#34; echo \u0026#34;Memory: $memory GB\u0026#34; if [ \u0026#34;$use_gpu\u0026#34; -eq 1 ]; then echo \u0026#34;Using GPU with GPU Number: $gpu_number\u0026#34; srun -n 8 -p b1171 --account=b1171 -t ${time}:00:00 --gres=gpu:a100:$gpu_number --mem ${memory}g --pty bash else echo \u0026#34;Using CPU only\u0026#34; srun -n 8 -p b1171 --account=b1171 -t ${time}:00:00 --mem ${memory}g --pty bash fi 7. Canveat # undefined reference to memcpy@GLIBC_2.14'`\nChecking the glibc installed in the conda environment.\n$ mamba list |rg sys (cuda) sysroot_linux-64 2.12 he073ed8_16 conda-forge After investigation, I found that conda ships with GLIBC_2.14 whatever the compiler version. Hence, the solution is to use module 👻\ncheck the available version of CUDA module spider cuda load cuda that is comp module load cuda/gcc-11.3.0 8. Q \u0026amp; A # why not to use module load module load is great but we canont control everything 🤪.\nhttps://github.com/flosse/rust-web-framework-comparison\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"29 August 2023","permalink":"/posts/022-cuda-configuration-for-rust-and-cpp/","section":"Posts","summary":"1.","title":"CUDA for Deep Learning Inference in Rust and C++"},{"content":" Life is in the code, whether DNA or binary. ","date":"29 August 2023","permalink":"/posts/","section":"Posts","summary":"Life is in the code, whether DNA or binary.","title":"Posts"},{"content":"","date":"29 August 2023","permalink":"/tags/rust/","section":"Tags","summary":"","title":"Rust"},{"content":"","date":"26 August 2023","permalink":"/projects/021-aurora-nlgraph-visilization/","section":"Projects","summary":"","title":"Aurora: A Web Application for Visualizing Non-linear Graph"},{"content":"","date":"26 August 2023","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"","date":"26 August 2023","permalink":"/tags/visualization/","section":"Tags","summary":"","title":"visualization"},{"content":"","date":"26 August 2023","permalink":"/tags/webapp/","section":"Tags","summary":"","title":"webapp"},{"content":"","date":"5 July 2023","permalink":"/tags/development/","section":"Tags","summary":"","title":"Development"},{"content":" my desktop snapshot 1 hidden desktop icons on macOS # defaults write com.apple.finder CreateDesktop false killall Finder Otherwise, you want to show the icons again\ndefaults write com.apple.finder CreateDesktop true killall Finder ","date":"5 July 2023","permalink":"/posts/020-develop-tips/","section":"Posts","summary":"my desktop snapshot 1 hidden desktop icons on macOS # defaults write com.","title":"Development Tips"},{"content":"","date":"25 June 2023","permalink":"/tags/bioinformatics/","section":"Tags","summary":"","title":"Bioinformatics"},{"content":"","date":"25 June 2023","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"25 June 2023","permalink":"/categories/external/","section":"Categories","summary":"","title":"External"},{"content":"","date":"25 June 2023","permalink":"/projects/001-pxblat/","section":"Projects","summary":"","title":"PxBLAT:An Efficient and Ergonomics Python Binding Library for BLAT"},{"content":"","date":"25 June 2023","permalink":"/categories/python/","section":"Categories","summary":"","title":"Python"},{"content":"","date":"2 June 2023","permalink":"/tags/tikz/","section":"Tags","summary":"","title":"Tikz"},{"content":" Overview # Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. Why # Welcome to \u0026lsquo;My TikZ-Learning Journey\u0026rsquo;, a space where I document my exploration with the wonderful TikZ library in LaTeX. LaTeX is a typesetting system used widely in academia and industries for the production of scientific and technical documents. Of its many features, the one I\u0026rsquo;ve recently been fascinated with is the TikZ library - a versatile package for crafting detailed and professional-grade diagrams.\nThis open-source repository is more than just a personal notebook. It\u0026rsquo;s an arena where I intend to share my collection of TikZ codes with you all. This collection, which will gradually expand, is not just about sharing the product of my learning process but about giving you an insight into my journey of learning and experimenting with TikZ.\nWhether you are a beginner just starting out with LaTeX and TikZ, or an experienced user searching for a fresh perspective, I hope my shared learning path will inspire and assist you.\nWhat is TikZ? # TikZ is a higher-level drawing language built on top of the LaTeX document preparation system. It allows the creation of complex, beautiful graphics, all at the heart of LaTeX. Despite its power, TikZ can be daunting to learn due to its comprehensive nature. From creating simple geometrical figures to developing intricate diagrams such as flowcharts, network diagrams, or logic circuits, TikZ provides the tools for us to implement our imagination in a precise, structured manner.\nWhat can you expect from this repository? # This repository is intended to be a living testament to my learning journey. As I delve deeper into the intricacies of TikZ, I\u0026rsquo;ll be documenting and sharing my codes, the challenges I faced, and how I overcame them. You can expect to see the evolution of my work from simple drawings to intricate, multifaceted diagrams.\nWhile I\u0026rsquo;ll try to annotate my code and explain my thought process wherever possible, please bear in mind that this project is primarily a reflection of my self-directed learning. It isn\u0026rsquo;t structured as a course or guide. However, I do believe that by seeing the practical application of TikZ commands and observing how they can be manipulated to generate diverse outputs, you\u0026rsquo;ll gain valuable insights and understanding of TikZ.\nI encourage you all to experiment with the shared codes, tweak them, and make them your own. By sharing my journey, I aim to inspire you to embark on your own exploration of TikZ and LaTeX.\nJoin me on this journey # As we navigate the complexities of learning TikZ together, we\u0026rsquo;ll inevitably stumble, err, and grow. Remember, this learning journey isn\u0026rsquo;t about perfection. It\u0026rsquo;s about the experience of trying, learning, and improving. With that in mind, I warmly invite you to join me on this adventure of understanding and mastering TikZ in LaTeX.\nLet\u0026rsquo;s dive into the fascinating world of technical diagrams together, one line of code at a time.\ncauliyang/learn_tikz learn_tikz TeX 1 0 Happy learning!\nNote: This blog will be updated periodically to reflect my ongoing progress with TikZ. Do keep checking back for fresh codes and insights. ","date":"2 June 2023","permalink":"/latex/020-tikz-learn-jouney/","section":"LaTex Typesetting","summary":"Overview # Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here.","title":"TikZ-Learning Journey"},{"content":"","date":"12 March 2023","permalink":"/tags/algorithm/","section":"Tags","summary":"","title":"Algorithm"},{"content":" Background # In bioinformatics, researchers frequently analyze various types of genomic data, such as DNA sequencing data, RNA sequencing data, and epigenetic data. Manipulating genomic intervals is a crucial task in comprehending the genetic basis of diseases and identifying potential therapeutic targets. Genomic intervals are defined as regions that span from a starting position to an ending position and can encompass genes, regulatory elements, and other functional elements of the genome. One primary application of genomic interval manipulation is analyzing ChIP-seq data. Moreover, manipulating genomic intervals allows for the integration of ChIP-seq data with other genomic data types, such as gene expression and genetic variations. This integration provides a more comprehensive understanding of biological processes and their contribution to normal development or disease. However, integrating these data types into a single data structure can pose challenges, especially when handling large datasets. Cache Oblivious Interval Trees (COITree), with cache-oblivious design and efficient query algorithms, have the potential to handle and integrate multiple types of genomic data into a single data structure. It stores the intervals in contiguous memory and employs in-order van Emde Boas layout to enhance query performance ( Citation: Emde Boas,\u0026#32;Kaas \u0026amp; al.,\u0026#32;1976 Emde Boas,\u0026#32; P.,\u0026#32; Kaas,\u0026#32; R.\u0026#32;\u0026amp;\u0026#32;Zijlstra,\u0026#32; E. \u0026#32; (1976). \u0026#32;Design and implementation of an efficient priority queue. Math. Systems Theory,\u0026#32;10(1).\u0026#32;99–127. https://doi.org/10.1007/BF01683268 ) . The tree is designed to optimize cache performance by reducing the number of cache misses during traversal ( Citation: Emde Boas,\u0026#32;1977 Emde Boas,\u0026#32; P. \u0026#32; (1977). \u0026#32;Preserving order in a forest in less than logarithmic time and linear space. Inf. Process. Lett.,\u0026#32;6(3).\u0026#32;80–82. https://doi.org/10.1016/0020-0190(77)90031-X ) . However, COITree still suffer from performance bottlenecks, particularly when dealing with large datasets. One approach to addressing this bottleneck is to use Single Instruction Multiple Data (SIMD), which is optimized for vector operations, to improve the performance of COITree. Thus, I hypothesize that the approach is a viable solution for improving the speed and efficiency of genomic interval analysis.\nHow transfer avx2 to neon # Neon instruction set is a specialized instruction set available in arm64 architecture that enables Single Instruction Multiple Data (SIMD) processing. It facilitates executing a single instruction on multiple pieces of data simultaneously, resulting in a significant performance improvement. This feature enables more accurate and comprehensive analyses of large-scale genomic data, leading to novel insights into the genetic basis of diseases and the development of more effective treatments. Furthermore, it can reduce the need for computational resources, which is especially important in processing large datasets. Overall, I intend to develop optimized COITree to address a critical need in bioinformatics for faster and more efficient methods of analyzing gnomic data.\nResult # To evaluate the performance benefits of using neon instruction set in COITree, I will conduct bench-marking tests with and without the neon instruction set, as well as existing tools including BEDTools (Quinlan and Hall, 2010), Augmented interval list ( Citation: Feng,\u0026#32;Ratan \u0026amp; al.,\u0026#32;2019 Feng,\u0026#32; J.,\u0026#32; Ratan,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Sheffield,\u0026#32; N. \u0026#32; (2019). \u0026#32;Augmented Interval List: a novel data structure for efficient genomic interval search. Bioinformatics,\u0026#32;35(23).\u0026#32;4907–4911. https://doi.org/10.1093/bioinformatics/btz407 ) , and Bedtk ( Citation: Li\u0026#32;\u0026amp;\u0026#32;Rong,\u0026#32;2021 Li,\u0026#32; H.\u0026#32;\u0026amp;\u0026#32;Rong,\u0026#32; J. \u0026#32; (2021). \u0026#32;Bedtk: finding interval overlap with implicit interval tree. Bioinformatics,\u0026#32;37(9).\u0026#32;1315–1316. https://doi.org/10.1093/bioinformatics/btaa827 ) . The evaluation task involves interval search problem that is fundamental to genomic data analysis. For benchmarking data, I will use stratification BED files from the Global Alliance for Genomics and Health (GA4GH) Benchmarking ( Citation: Krusche,\u0026#32;Trigg \u0026amp; al.,\u0026#32;2019 Krusche,\u0026#32; P.,\u0026#32; Trigg,\u0026#32; L.,\u0026#32; Boutros,\u0026#32; P.,\u0026#32; Mason,\u0026#32; C.,\u0026#32; De La Vega,\u0026#32; F.,\u0026#32; Moore,\u0026#32; B.,\u0026#32; Gonzalez-Porta,\u0026#32; M.,\u0026#32; Eberle,\u0026#32; M.,\u0026#32; Tezak,\u0026#32; Z.,\u0026#32; Lababidi,\u0026#32; S.,\u0026#32; Truty,\u0026#32; R.,\u0026#32; Asimenos,\u0026#32; G.,\u0026#32; Funke,\u0026#32; B.,\u0026#32; Fleharty,\u0026#32; M.,\u0026#32; Chapman,\u0026#32; B.,\u0026#32; Salit,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Zook,\u0026#32; J. \u0026#32; (2019). \u0026#32;Best practices for benchmarking germline small-variant calls in human genomes. Nat. Biotechnol.,\u0026#32;37.\u0026#32;555–560. https://doi.org/10.1038/s41587-019-0054-x ) . By comparing the effectiveness of COITree with neon instruction set against state-of-the-art tools, we can determine whether this approach can significantly enhance the speed and efficiency of genomic data analysis\nA genomic interval \\(r\\) is defined by two coordinates that represent the start and end locations of a feature on a chromosome. The general interval search problem is defined as follows ( Citation: Feng,\u0026#32;Ratan \u0026amp; al.,\u0026#32;2019 Feng,\u0026#32; J.,\u0026#32; Ratan,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Sheffield,\u0026#32; N. \u0026#32; (2019). \u0026#32;Augmented Interval List: a novel data structure for efficient genomic interval search. Bioinformatics,\u0026#32;35(23).\u0026#32;4907–4911. https://doi.org/10.1093/bioinformatics/btz407 ) .\nGiven a set of \\(N\\) intervals in a \\(R = {r_1, r_2, \\dots,r_N} ; for ; N \\gg 1 \\), and a query interval \\( q \\), find the subset of \\( S \\) of \\( R \\) that intersect \\(q\\). If we define all intervals to be half-open, \\( S \\) can be represented as:\n$$ S(q) = \\{ r \\in R \\;| \\; (r.start \u0026lt; q.end \\wedge r.end \u0026gt; q.start)\\} $$\nI have implemented the optimized COITree in Rust. To evaluate its performance, I used two genomic interval datasets A and B from GA4GH. Dataset A and B contain 4816112 and 44426501 genomic intervals, respectively. I compared the performance with and without the neon instruction set as well as existing tools including BEDTools (v2.30.0), Augmented interval list (v0.1.1), and Bedtk (v0.0-r25dirty). I query every genomic interval of dataset A on dataset B, and the total overlapping genomic intervals is 35 032 849. As BEDTools and bedtk provide enrich features, I used subcommand coverage of BEDTools and subcommand cov of Bedtk to find overlapping intervals. Other tools are designed to the problem so that I do not need to use subcommand. I used hyperfine ( Citation: Peter,\u0026#32;2022 Peter,\u0026#32; D.\u0026#32; (2022). \u0026#32; hyperfine. \u0026#32;Retrieved from\u0026#32; https://github.com/sharkdp/hyperfine ) , which is command-line benchmarking tool, to evaluate the performance.\nFor each tool, I warmed up the tool three times before executing it ten times. All experiments were run on a computer with macOS 12.6.6.3 2 21G320 arm64 and 32 GB of memory. In the case of sorted datasets, the optimized COITree outperformed all other tools, as shown in Table 1.\nTable 1: Runtime of tools on the sorted dataset\nCommand Mean [s] Min [s] Max [s] Relative coitree-default 4.36 4.28 4.44 1.24 coitree-neon 3.53 3.50 3.57 1.00 ailist 5.63 5.54 5.87 1.59 bedtk cov 4.70 4.67 4.75 1.33 bedtools coverage -counts -sorted 13.48 13.40 13.64 3.82 Table 2: Runtime of tools on the unsorted dataset\nCommand Mean [s] Min [s] Max [s] Relative coitree-neon 5.46 5.43 5.50 1.00 coitree-default 6.41 6.35 6.44 1.17 ailist 6.49 6.48 6.49 1.19 bedtk cov 7.11 6.97 7.18 1.30 bedtools coverage -counts 256.08 244.48 276.65 46.88 Since sorted datasets reduce the complexity of the problem, we may not observe a relatively significant speedup. Therefore, I shuffled interval dataset B and repeated the experiment. As shown in Table 2, the optimized COITree demonstrated the best performance, and was about 46 times faster than BEDTools. Our results demonstrate a substantial performance improvement when using the Neon instruction set, especially on unsorted datasets. The tests also showed that the performance gain was particularly noticeable when working with large datasets.\nIn conclusion, incorporating SIMD in COITree can significantly enhance its performance. This strategy can also be applied to other data structures, providing a way to optimize performance. By leveraging the power of specialized instruction sets such as Neon, we can achieve more efficient and performant algorithms. The optimized COITree will empower researchers to mine genomics data more ergonomically and efficiently. The code for optimized implementation in Rust is freely available.\ncauliyang/coitrees A very fast interval tree data structure Rust 4 0 I also gave a presentation for the work if you are interested in that please check the slides.\nReference # Feng,\u0026#32; Ratan\u0026#32;\u0026amp;\u0026#32;Sheffield (2019) Feng,\u0026#32; J.,\u0026#32; Ratan,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Sheffield,\u0026#32; N. \u0026#32; (2019). \u0026#32;Augmented Interval List: a novel data structure for efficient genomic interval search. Bioinformatics,\u0026#32;35(23).\u0026#32;4907–4911. https://doi.org/10.1093/bioinformatics/btz407 Krusche,\u0026#32; Trigg,\u0026#32; Boutros,\u0026#32; Mason,\u0026#32; De La Vega,\u0026#32; Moore,\u0026#32; Gonzalez-Porta,\u0026#32; Eberle,\u0026#32; Tezak,\u0026#32; Lababidi,\u0026#32; Truty,\u0026#32; Asimenos,\u0026#32; Funke,\u0026#32; Fleharty,\u0026#32; Chapman,\u0026#32; Salit\u0026#32;\u0026amp;\u0026#32;Zook (2019) Krusche,\u0026#32; P.,\u0026#32; Trigg,\u0026#32; L.,\u0026#32; Boutros,\u0026#32; P.,\u0026#32; Mason,\u0026#32; C.,\u0026#32; De La Vega,\u0026#32; F.,\u0026#32; Moore,\u0026#32; B.,\u0026#32; Gonzalez-Porta,\u0026#32; M.,\u0026#32; Eberle,\u0026#32; M.,\u0026#32; Tezak,\u0026#32; Z.,\u0026#32; Lababidi,\u0026#32; S.,\u0026#32; Truty,\u0026#32; R.,\u0026#32; Asimenos,\u0026#32; G.,\u0026#32; Funke,\u0026#32; B.,\u0026#32; Fleharty,\u0026#32; M.,\u0026#32; Chapman,\u0026#32; B.,\u0026#32; Salit,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Zook,\u0026#32; J. \u0026#32; (2019). \u0026#32;Best practices for benchmarking germline small-variant calls in human genomes. Nat. Biotechnol.,\u0026#32;37.\u0026#32;555–560. https://doi.org/10.1038/s41587-019-0054-x Li\u0026#32;\u0026amp;\u0026#32;Rong (2021) Li,\u0026#32; H.\u0026#32;\u0026amp;\u0026#32;Rong,\u0026#32; J. \u0026#32; (2021). \u0026#32;Bedtk: finding interval overlap with implicit interval tree. Bioinformatics,\u0026#32;37(9).\u0026#32;1315–1316. https://doi.org/10.1093/bioinformatics/btaa827 Peter (2022) Peter,\u0026#32; D.\u0026#32; (2022). \u0026#32; hyperfine. \u0026#32;Retrieved from\u0026#32; https://github.com/sharkdp/hyperfine Emde Boas,\u0026#32; Kaas\u0026#32;\u0026amp;\u0026#32;Zijlstra (1976) Emde Boas,\u0026#32; P.,\u0026#32; Kaas,\u0026#32; R.\u0026#32;\u0026amp;\u0026#32;Zijlstra,\u0026#32; E. \u0026#32; (1976). \u0026#32;Design and implementation of an efficient priority queue. Math. Systems Theory,\u0026#32;10(1).\u0026#32;99–127. https://doi.org/10.1007/BF01683268 Emde Boas (1977) Emde Boas,\u0026#32; P. \u0026#32; (1977). \u0026#32;Preserving order in a forest in less than logarithmic time and linear space. Inf. Process. Lett.,\u0026#32;6(3).\u0026#32;80–82. https://doi.org/10.1016/0020-0190(77)90031-X ","date":"12 March 2023","permalink":"/posts/019-efficient-genoimc-interval-search/","section":"Posts","summary":"Background # In bioinformatics, researchers frequently analyze various types of genomic data, such as DNA sequencing data, RNA sequencing data, and epigenetic data.","title":"Efficient Genomic Interval Search Using SIMD-Enhanced COITree"},{"content":" 1. Introduction # Noodles and Rust-htslib are two widely used Rust libraries for genomic data handling. While both libraries are designed to work with genomic data, they take different approaches to achieve this goal. This blog explores Noodles and compares it with Rust-htslib, while also discussing its potential pitfalls.\nNoodles is a Rust library built on top of Rust\u0026rsquo;s IO and byte manipulation tools, designed for reading, writing, and manipulating genomic data files. It offers high-level performance and scalability, as well as a high degree of modularity, providing users with many useful tools for working with genomic data.\nOn the other hand, Rust-htslib is a Rust library that provides a high-level interface to the [HTSlib] C library. It is specifically designed to work with BAM and VCF files, offering a robust set of functions for working with these types of data.\nWhen comparing these two libraries, there are several key differences to consider. Noodles is a more modern library that takes full advantage of Rust\u0026rsquo;s advanced features, such as iterators and closures. This makes Noodles highly flexible and adaptable to different use cases. Rust-htslib, on the other hand, is a more specialized library designed specifically for working with BAM and VCF files.\n2. Usage # 2.1 Use noodles # The first step is to add noodles as dependencies by using cargo add noodles --featues bam sam bgzf core. Or we can edit Cargo.toml directly and add the following line:\nnoodles = {version = \u0026#34;0.32.0\u0026#34;, features = [\u0026#34;bam\u0026#34;, \u0026#34;sam\u0026#34;, \u0026#34;bgzf\u0026#34;, \u0026#34;core\u0026#34;]} 2.2 Read bam file # We can employ the noodles library to read BAM files, and the library offers various methods to access BAM files. What\u0026rsquo;s more, we can read files asynchronously and process records concurrently. Additionally, it can be combined with the [rayon] library, which offers powerful parallelism features for Rust.\nTo open a BAM file and read all the records in the file is quite simple:\nuse noodles::bam; use noodles::sam; use std::fs::File; fn read_bam(path: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut reader = File::open(path).map(bam::Reader::new)?; let header: sam::Header = reader.read_header()?.parse()?; reader.read_reference_sequences()?; reader .records(\u0026amp;header) .map(|r| r.unwrap()) .for_each(|record| { println!(\u0026#34;read name: {}\u0026#34;, record.read_name().unwrap()); }); Ok(()) } Before reading records, we need to consume header and reference sequences to direct file handler to the first record. Furthermore, we can read records asynchronously:\nuse noodles::bam; use noodles::sam; use std::fs::File; fn read_bam_async(path: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut reader = File::open(path).map(bam::Reader::new)?; let header: sam::Header = reader.read_header()?.parse()?; reader.read_reference_sequences()?; reader .lazy_records() .map(|r| r.unwrap()) .for_each(|record| { println!(\u0026#34;read name: {}\u0026#34;, record.read_name().unwrap().unwrap()); }); Ok(()) } I utilize hyperfine to conduct benchmarking. The results show that read_async() is 1.5 times faster than read_bam() when reading the bam file contains 144309 records.\nOne difference is that we use lazy_records() instead of records(), and read_name().unwrap().unwrap() instead of read_name().unwrap() to get read name. That is because lazy_records() will return noodles::bam::reader::LazyRecords. However, records() will return noodles::sam::reader::Records These two types have different methods, and Records have more methods compared to LazyRecords. For instance, the cigar object return from LazyRecords is not usable in comparison with cigar object return from Records. Consequently, we need to reconstruct some data structures from LazyRecords to Records. For example:\n// File: read_bam_async use anyhow::Context; use noodles::bam; use noodles::sam; use std::fs::File; use sam::record::cigar::Cigar; use sam::record::data::Data; fn read_bam_async(path: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut reader = File::open(path).map(bam::Reader::new)?; let header: sam::Header = reader.read_header()?.parse()?; reader.read_reference_sequences()?; reader .lazy_records() .map(|r| r.unwrap()) .for_each(|record| { let read_name = record.read_name().unwrap().unwrap(); let data = Data::try_from(record.data()) .with_context(|| format!(\u0026#34;failed to get data {}\u0026#34;, read_name)) .unwrap(); let cigar = Cigar::try_from(record.cigar()) .with_context(|| format!(\u0026#34;failed to get cigar {}\u0026#34;, read_name)) .unwrap(); let sequence = sam::record::Sequence::try_from(record.sequence()) .with_context(|| format!(\u0026#34;failed to get sequence {}\u0026#34;, read_name)) .unwrap(); println!(\u0026#34;read name: {}, cigar: {}\u0026#34;, read_name, cigar); }); Ok(()) } The purpose of code is to reconstruct Data, Cigar and Sequence of nsam::record from the data return by LazyRecords. This is necessary because the data from LazyRecords do not have enough methods to manipulate. As a result, we can access the tag or field in Data and Cigar through the reconstructed data structure. Another trick is to convert the Sequence object return by Records or LazyRecords to use noodles::fasta::record::Sequence since we can get reverse complement sequence easily by let rev_comp: Sequence = sequence.complement().rev().collect::\u0026lt;Result\u0026lt;_, _\u0026gt;\u0026gt;()?; After executing the code block with sample input, we will see following output:\nYou may notice that we use anyhow::Context here to provide enrich message if there is a bug. Anyhow is an amazing library, which allow user to handle error more easily.\n2.3 Process records in parallel # Rust is a programming language that enables fearless concurrency. Its features allow us to safely parallelize programs. [Rayon] is an excellent Rust library that seamlessly provides parallel iterators. We can use this library to parallelize our current program and accelerate its execution without too much effort. Before using Rayon, make sure to add its dependency using cargo add rayon.\n// File: read_bam_async_rayon use anyhow::Context; use noodles::bam; use noodles::sam; use rayon::prelude::*; use std::fs::File; use sam::record::cigar::Cigar; use sam::record::data::Data; use std:🧵:sleep; fn read_bam_async_rayon(path: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut reader = File::open(path).map(bam::Reader::new)?; let header: sam::Header = reader.read_header()?.parse()?; reader.read_reference_sequences()?; reader .lazy_records() .par_bridge() // convert to parallel iterators .map(|r| r.unwrap()) .for_each(|record| { let read_name = record.read_name().unwrap().unwrap(); let data = Data::try_from(record.data()) .with_context(|| format!(\u0026#34;failed to get data {}\u0026#34;, read_name)) .unwrap(); let cigar = Cigar::try_from(record.cigar()) .with_context(|| format!(\u0026#34;failed to get cigar {}\u0026#34;, read_name)) .unwrap(); let sequence = sam::record::Sequence::try_from(record.sequence()) .with_context(|| format!(\u0026#34;failed to get sequence {}\u0026#34;, read_name)) .unwrap(); sleep(std::time::Duration::from_millis(1000)); // for benchmarking println!(\u0026#34;read name: {}, cigar: {}\u0026#34;, read_name, cigar,); }); Ok(()) } For the sake of the benchmarking, I will add another line sleep(std::time::Duration::from_millis(1000)); to simulate labor work. In this implementation, I am using 4 threads to process an input that has three records. We obtain a speedup of three times faster than the version without using threads, which is reasonable considering the overhead of launching and joining threads.\nI conducted a benchmarking test for reading records without using asynchronous programming.\n2.4 Query certain region # An index file is required if you want to query records in specific regions. Similar to rust-htslib, noodles provides a feature to assist users in fetching records from specific regions.\nFor example,\nuse anyhow::{Context, Result}; use noodles::bam; use noodles::sam; use std::path::Path; fn query\u0026lt;T\u0026gt;(path: T) -\u0026gt; Result\u0026lt;()\u0026gt; where T: AsRef\u0026lt;Path\u0026gt;, { let mut reader = bam::indexed_reader::Builder::default() .build_from_path(path.as_ref()) .with_context(|| { format!( \u0026#34;failed to read bam file and index not existed {:?} \u0026#34;, path.as_ref() ) })?; let header: sam::Header = reader .read_header() .context(\u0026#34;failed to read bam reader\u0026#34;)? .parse() .context(\u0026#34;failed to parse bam rader\u0026#34;)?; let reference = reader .read_reference_sequences() .context(\u0026#34;failed to read reference sequences\u0026#34;)?; let region = \u0026#34;chr17:79778148-79778149\u0026#34; .parse() .expect(\u0026#34;failed to parse region\u0026#34;); let count = reader.query(\u0026amp;header, \u0026amp;region).unwrap().count(); println!(\u0026#34;{} records found\u0026#34;, count); Ok(()) } fn main() { let path = std::env::args().nth(1).unwrap(); query(path).unwrap(); } Please note that the IndexReader assumes that the index file\u0026rsquo;s name is file_name.bam.bai instead of file_name.bai. If your index file does not follow this naming convention, you may encounter an error such as file does not exist. We can also use [rayon] to speed up the code by:\nlet count = reader.query(\u0026amp;header, \u0026amp;region).unwrap().par_bridge().count(); // let count = reader.query(\u0026amp;header, \u0026amp;region).unwrap().count(); 3. Pitfall # 3.1 Bam/Sam header format # When utilizing noodles to parse BAM/SAM files, adherence to the standard SAM format is crucial compared to rust-htslib. Otherwise, parsing may result in errors such as the \u0026ldquo;Invalid ReadGroup for PL\u0026rdquo; message. In this instance, the PL value belonging to the RG tag does not comply with the standard. According to the standard, PICBIO is one of the correct values to use for PL, with a defined set of values available for it. A noodles issue has been discussed regarding the strictness of parsing headers. To resolve this issue, we recommend replacing the RG tag in place using the samtools addreplacerg -r \u0026quot;@RG\\tID:test\\tSM:hs\\tLB:ga\\tPL:PACBIO\u0026quot; -w input.bam -o output.bam before processing the file. Do not forget to index the new file if you want to query certain region.\n3.2 IndexReader # As previously mentioned, using IndexReader to read SAM/BAM files eliminates the need to read the index separately. However, it is important to note that IndexReader does not expose the same API as Reader, and the data structures for Cigar and Data are different from those used by Reader. The workaround for this is to reconstruct the relevant data structures used by Reader from those used by IndexReader, as previously mentioned.\n3.3 Read file multiple times # It is important to note that seeking to the first record is necessary when you want to read the file again, but not required when you want to query it again. However, before reading the first record, it is crucial to consume the header and reference to help forward the file handler to the position of the first record. It is not possible to iterate through all the records twice using one file handler since it moves the current file handler to the end of the file.\nTo overcome this issue, we have two solutions. Firstly, we can reopen the file handler and consume the header and references respectively. Secondly, we can move the current file handler to the beginning of the file. Unfortunately, noodles does not provide an API to do so, and therefore, we need to create our own version based on the unexposed version.\nuse noodles::bam; use noodles::sam; use noodles::bgzf; use std::io::{self, Read, Seek}; pub trait NoodleBamIndexReaderExt { fn seek_to_first_record(\u0026amp;mut self) -\u0026gt; io::Result\u0026lt;bgzf::VirtualPosition\u0026gt;; } impl\u0026lt;R\u0026gt; NoodleBamIndexReaderExt for bam::indexed_reader::IndexedReader\u0026lt;bgzf::Reader\u0026lt;R\u0026gt;\u0026gt; where R: Read + Seek, { fn seek_to_first_record(\u0026amp;mut self) -\u0026gt; io::Result\u0026lt;bgzf::VirtualPosition\u0026gt; { // seek to first record let areader = self.get_mut(); areader.seek(bgzf::VirtualPosition::default())?; self.read_header()?; self.read_reference_sequences()?; Ok(self.get_ref().virtual_position()) } } We have created an extension trait for IndexReader, which enables us to reset the file handler using the seek_to_first_record() method.\nFor example,\nfn count\u0026lt;T\u0026gt;(path: T) -\u0026gt; Result\u0026lt;()\u0026gt; where T: AsRef\u0026lt;Path\u0026gt;, { let mut reader = bam::indexed_reader::Builder::default() .build_from_path(path.as_ref()) .with_context(|| { format!( \u0026#34;failed to read bam file and index not existed {:?} \u0026#34;, path.as_ref() ) })?; let header: sam::Header = reader .read_header() .context(\u0026#34;failed to read bam reader\u0026#34;)? .parse() .context(\u0026#34;failed to parse bam rader\u0026#34;)?; let reference = reader .read_reference_sequences() .context(\u0026#34;failed to read reference sequences\u0026#34;)?; let count1 = reader.lazy_records().count(); println!(\u0026#34;first count: {}\u0026#34;, count1); let count2 = reader.lazy_records().count(); println!(\u0026#34;second count: {}\u0026#34;, count2); Ok(()) } The output will be:\nAfter using our extension, it will be changed to:\nfn count\u0026lt;T\u0026gt;(path: T) -\u0026gt; Result\u0026lt;()\u0026gt; where T: AsRef\u0026lt;Path\u0026gt;, { let mut reader = bam::indexed_reader::Builder::default() .build_from_path(path.as_ref()) .with_context(|| { format!( \u0026#34;failed to read bam file and index not existed {:?} \u0026#34;, path.as_ref() ) })?; let header: sam::Header = reader .read_header() .context(\u0026#34;failed to read bam reader\u0026#34;)? .parse() .context(\u0026#34;failed to parse bam rader\u0026#34;)?; let reference = reader .read_reference_sequences() .context(\u0026#34;failed to read reference sequences\u0026#34;)?; let count1 = reader.lazy_records().count(); println!(\u0026#34;first count: {}\u0026#34;, count1); reader.seek_to_first_record().unwrap(); // important let count2 = reader.lazy_records().count(); println!(\u0026#34;second count: {}\u0026#34;, count2); Ok(()) } After resetting the file handler, we are now able to iterate over the records as before. However, it is not necessary to do this when fetching records from specific regions. Let\u0026rsquo;s take a look at the output:\n3.4 Off one error # The noodles library uses a 1-based position and employs a range index syntax that includes both the left and right endpoints, [start, end], to retrieve a sequence. In contrast, Rust uses a 0-based position and the default range syntax is left-open and right-closed, [start, end). Therefore, you must add 1 to the starting position when using noodles to retrieve a sequence, otherwise, you may encounter an off-by-one error.\n3.5 Get reference name # In Noodles, it is not intuitive to get reference name.\nfn get_reference_name(references: \u0026amp;ReferenceSequences, reference_sequence_id: usize) -\u0026gt; String { references .get_index(reference_sequence_id) .map(|(name, _)| name.as_str()) .unwrap() .to_owned() } We can get reference_sequence_id by record.reference_sequence_id().\n4. Conclusion # One potential issue to consider when using Noodles is its relatively new status in comparison to rust-htslib, which has been available for a longer period of time and is widely used in many projects. As a result, Noodles may contain bugs or other problems that have not yet been discovered. On the other hand, rust-htslib has undergone extensive testing and has proven to be a reliable and high-performance option.\nIn summary, both Noodles and rust-htslib are valuable Rust libraries for managing genomic data, and each has its own advantages and disadvantages. Choosing between the two depends on the specific needs of the project at hand. Noodles, being a pure Rust implementation, may be the better option if flexibility and adaptability are desired. The sample code can be found at the repository.\ncauliyang/noodles_blog Rust 1 0 ","date":"4 March 2023","permalink":"/posts/001-rust-noodles/","section":"Posts","summary":"1.","title":"How to Use Noodles Library in Rust"},{"content":" Why # This blog will evolve to including more valuable content. Using nvim tag to filter more content about nvim and others.\n001 Using command mode in visual block. # After you select a visual block in vim, you can execute operation on the block by typing :normal ops. ops means the operations you intend to do. For instance, let\u0026rsquo;s assume that you have a text as shown below, and you have already select them in virtual mode.\nAfter typing :normal Atest that mean appends \u0026ldquo;test\u0026rdquo; to every line at the end. You will see:\n002 Mapping \u0026lt;C-d\u0026gt; and \u0026lt;C-u\u0026gt; to \u0026lt;C-d\u0026gt;zz and \u0026lt;C-u\u0026gt;zz. # \u0026lt;C-d\u0026gt; is key combinations of Control and d. In normal mode, \u0026lt;C-d\u0026gt; means half page down and \u0026lt;C-u\u0026gt; means half page up. zz is key combinations of double z, which help center cursor to middle position of current buffer. The new mapping is combinations of these two operations. Even though page is moved to up or down, the focus will always be center.\n003 Repeat and Reverse Operation 1 # 004 daw or daW delete one word and cW or cw delete one word then # 005 \u0026lt;C-h\u0026gt; and \u0026lt;C-w\u0026gt; to delete words in insert mode. # For example, assuming current buffer contains this is a test. \u0026lt;C-h\u0026gt; will delete one character and \u0026lt;C-w\u0026gt; will delete one word.\n006 \u0026lt;C-o\u0026gt; enter Insert normal mode # Insert normal Mode means that the buffer will leave at insert mode after using Norm mode.\nFor example, we assume current buffer is this is a test,\n007 gv select recent visual block # 008 Vr + character replace one line with same character # 009 use * to search for each occurrence # You can use * to search the word at which the current cursor locate.\n010 Powerful Command Line Mode # 011 read !{cmd} to load output of cmd to current buffer # 012 write !{cmd} to send current buffer to input of cmd # 013 use \u0026lt;C-o\u0026gt; as backward jump and \u0026lt;C-i\u0026gt; as forward jump # Practical Vim\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"21 December 2022","permalink":"/pde/000-nvim-tips/","section":"Personal Development Environment","summary":"Why # This blog will evolve to including more valuable content.","title":"Collections of Amazing Vim Tips"},{"content":"","date":"21 December 2022","permalink":"/tags/neovim/","section":"Tags","summary":"","title":"Neovim"},{"content":"","date":"21 December 2022","permalink":"/series/personal-development-environment/","section":"Series","summary":"","title":"Personal Development Environment"},{"content":" In the garden of personal development, patience yields the ripest fruits. ","date":"21 December 2022","permalink":"/pde/","section":"Personal Development Environment","summary":"In the garden of personal development, patience yields the ripest fruits.","title":"Personal Development Environment"},{"content":"","date":"20 December 2022","permalink":"/categories/macos/","section":"Categories","summary":"","title":"macOS"},{"content":" 1. Aims # The philosophy of Vim\u0026rsquo;s motion has had a profound influence on me, leading me to fully embrace Neovim. The notion of having to reposition windows using a mouse is anathema to me. I greatly value the experience of utilizing solely the keyboard. As such, I set out to find a tool to aid me in managing windows seamlessly. Eventually, I discovered a quartet of exceptional tools that I highly recommend giving a try. To this day, they continue to surpass my expectations and enhance my productivity.\nConfiguration is time-consuming, and you can give up if you do not like that.** However, that means you have already missed one amazing thing. 2. Tools # yabai # Yabai is tiling windows manager akin to i3, falling under the category of BSP window managers. Its capabilities are robust enough to provide a fluid experience. It can be tailored to your preferences through binding any key to execute commands in yabai. You can view the video linked below to witness the power of yabai. Another notable tool, Skhd, is developed by the same talented individual who created yabai.\nSkhd # Skhd is key mapping tool that can allow you bind keys to execute command line tool. For example ctrl + alt - left: yabai -m display --focus west || yabai -m display --focus recent, it means that ctrl + alt - left will cause yabai to move focus to difference monitors.\nMore importantly, skhd allows for the creations of multiple modes, similar to Vim\u0026rsquo;s modal editing. Each mode can have its won set of motion commands, distinguished by their unique prefix. This is an incredibly powerful feature, as it reduces the need for memorizing a plethora of complex key combination. You need only remember the prefix of the mode you\u0026rsquo;re currently in. Skhd seamlessly integrates with Yabai and sketchybar, resulting in an unparalleled user experience.\nStackline # Stackline is a tool that enables the visualization of stack indicators, allowing you to easily discern which program is currently at the top of the stack and make changes accordingly. The read arrow serves as the stackline indicator.\nSketchyBar # SketchyBar is menu bar tool that can be paired with yabai and skhd to display a wealth of information. As seen in the top portion of the previous screenshot, it can be customized to include any pertinent information you desire.\n3. Take home message # I must acknowledge that the configuration process can be both tedious and frustrating, a necessary trade-off for attaining a high degree of customization. These tools all possess an array of fancy features, made possible through their high degrees of customization. I will share my personal configuration and their documentation also provide ample information to assist in the process. Patience is key, as the time invested in configuring these tools will ultimately pay off in terms of the enhanced functionality they provide. If you have any other recommendations or alternatives, please do not hesitate to share them.\n","date":"20 December 2022","permalink":"/posts/014-macos-tiling-windows-management/","section":"Posts","summary":"1.","title":"Tiling Windows Management in macOS"},{"content":"","date":"20 December 2022","permalink":"/tags/windows-management/","section":"Tags","summary":"","title":"Windows Management"},{"content":"I recently made a change to my terminal setup, switching from using iTerm2 + Zsh + Tmux to Alacritty + Fish + Zellij. Now I use Wezterm + Fish + Zellij. I discovered that my new combination is extremely powerful and versatile. The flexibility to highly customize each of these tools was a significant factor in my decision to keep them.\nconfiguring these tools can be a time-consuming and frustrating process. If you\u0026rsquo;re interested in trying them out for yourself, you can click on the links provided to download the relevant tools. It\u0026rsquo;s worth noting that these tools are open-source 🚀\n0. Wezterm # Welcome to the world of command-line interfaces and terminal emulators. If you\u0026rsquo;re a developer, system administrator, or a tech enthusiast, you\u0026rsquo;re likely no stranger to the terminal. But did you know that not all terminals are created equal? In this article, we\u0026rsquo;ll introduce Wezterm, a modern, GPU-accelerated terminal emulator that takes your command-line work to the next level.\nfrom wezerm website Wezterm is a fast, highly customizable, and cross-platform terminal emulator that focuses on delivering a seamless user experience. It\u0026rsquo;s built with a slew of features that make it a standout choice for those seeking a high-performance terminal emulator. Wezterm is developed in Rust and uses both OpenGL and DirectWrite for GPU acceleration to achieve its impressive performance.\nHere are some of the most compelling features of Wezterm:\nCross-Platform Support: Wezterm works on various platforms, including Windows, macOS, Linux, and FreeBSD. This versatility makes it an excellent choice for those who work across different operating systems. GPU-Accelerated Rendering: Leveraging the power of your GPU, Wezterm ensures smooth scrolling and typing, even under heavy loads. Highly Configurable: Wezterm allows you to tailor your terminal environment to your liking, offering various customization options, including colors, fonts, transparency, and key bindings. Multiplexer Support: Wezterm includes built-in support for terminal multiplexing, similar to popular tools like tmux, allowing you to run and manage multiple terminal sessions within a single window. Shell Integration: With Wezterm\u0026rsquo;s shell integration, you can enjoy features like automatic directory changing and local echo. Installing Wezterm is straightforward. For example, on macOS, you can use Homebrew to install Wezterm by running the following command in your terminal:\nbrew install wezterm For other platforms like Windows, Linux, and FreeBSD, you can download the appropriate installer or package from the Wezterm GitHub releases page. Once installed, you can launch Wezterm just like any other terminal emulator. Simply type wezterm in your existing terminal, or find it in your system\u0026rsquo;s application menu. Wezterm\u0026rsquo;s configuration file is written in Lua and typically resides in your home directory. You can customize various aspects of Wezterm by editing this configuration file. For example, to change the default font size, you can add the following to your configuration file:\nwezterm = { font_size = 11.0, } Wezterm represents the next generation of terminal emulators, offering a slew of features that go above and beyond the capabilities of traditional terminals. Whether you\u0026rsquo;re seeking better performance, more customization, or just a more pleasant terminal experience, Wezterm is worth checking out. Get ready to explore the world of command-line interfaces like never before with Wezterm!\n1. Alacritty # After installing Alacritty, you can create a configuration file ~/.alacritty.yml, which is used to configure the terminal emulator. I use GitHub to host all of my configurations, including Alacritty\u0026rsquo;s. My configuration file includes detailed comments to assist others in understanding the various options available.\nOne important aspect to keep in mind is that you may need to change the key mapping in order to map the alt key to the option or meta key on macOS. Information on how to address this issue can be found in This issue. Fortunately, alacrity release new version (0.12.0) now, and we do not need to remapping keys one by one to use alt as option. We just use the following configuration simply.\nwindow: option_as_alt: Both Additionally, you can also change the color theme to your preference. I enjoy using base 16 themes.\n2. Zellij # Previously, I was using Tmux, which is a widely popular tool for managing terminal windows. Recently, I\u0026rsquo;ve been learning Rust and have been drawn to tools implemented using this language. Zellij is one such tool that caught my attention due to its user-friendliness compared to Tmux, which offers built-in key-mapping and helpful tips directly in the terminal. This eliminates the need to constantly reference a cheat-sheet. The configuration file for Zellij can be found at ~/.config/zellij/config.kdl. To be honest, you may not need to add any configurations at all. Similar to Alacritty, you can also change the color theme of Zellij as per your preference.\nPrevious Nextsads 3. Fish # I have grown to truly appreciate Fish, it has saved me a lot of time. In comparison to Zsh, Fish does not require the installation of numerous plugins to access powerful features. Fish provides out-of-the-box features such as auto-suggestion, searching through command history and fancy completion making it much more user-friendly. Of course, if desired, you can install plugins, but I find that it is not necessary. Fisher is a plugins\u0026rsquo; management tool for Fish shell, and you can look it up to find recommended plugins. Fish also allows for high degree of customization, such as changing the prompt and greeting message according to your preference. I use [Tide] prompt, it is a clean and visually appealing prompt.\nPrevious Nextsads 4. Summary # The provided documentation for these tools should be sufficient to get you started on using them. You can use my configuration as a reference for modifying your own.\ncauliyang/dotfiles dotfiles Shell 1 0 I use a lot of powerful and fancy terminal applications and I have shared my setup in my blog, specifically in my desktop setup post.\n","date":"15 December 2022","permalink":"/posts/012-make-a-powerful-ternimal/","section":"Posts","summary":"I recently made a change to my terminal setup, switching from using iTerm2 + Zsh + Tmux to Alacritty + Fish + Zellij.","title":"Make A Powerful Terminal Workspace"},{"content":"","date":"15 December 2022","permalink":"/tags/terminal/","section":"Tags","summary":"","title":"Terminal"},{"content":" Gallery # Previous Nextsads TODO # add latex add lsd add fd add fzf add ripgrep add procs add zoxide add gitui add btop TLDR # alacritty office aldente one switch alfred pdf expert alttab picgo bartender pycharm cheat.sh rectangle chrome reeder 5 clion rust conda snippetslab default folder x soundsource docker spacevim dust time sink ferdi tldr fish tmux git tmuxinator hyperfine vim imagine xcode ishot xmind iterm2 zellij lunarvim zoom magnet zotero mamba zsh micromamba fisher miniforge ouch monitorcontrol topgrade monodraw imagemagick neovim youtobe-dl notion jetbrains google drive transmit homebrew tree fluent reader wezterm iina vs code 1. Introduction # Given the time and effort required for migrating configurations, I have decided to create a blog documenting the entire process. The configurations will be divided into two sections, the first of which will cover the software that I frequently use, and the second will contain the configuration files themselves. As I have recently acquired a MacBook Pro M1 model, here is a list of the tools I am currently utilizing.\n2. Package Manager # Homebrew # If you\u0026rsquo;re a macOS user looking to expand your software options beyond what\u0026rsquo;s available in the App Store, Homebrew is an excellent solution. Homebrew is a free and open-source package manager that simplifies the process of installing software on your Mac.\nHere\u0026rsquo;s how to get started with Homebrew:\nOpen the Terminal app, which you can find in the Utilities folder within the Applications folder. In the Terminal window, paste the following command and hit enter to install Homebrew: /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You may be prompted to enter your password during the installation process. This is normal and required to give Homebrew the necessary permissions to install software on your system. Once the installation is complete, you can start using Homebrew to install software packages. For example, if you want to install the popular text editor vim you can type the following command into the Terminal window: brew install vim Homebrew will then download and install the latest version of vim, along with any dependencies that it requires.\nTo update Homebrew itself and all installed packages, use the following command: brew update \u0026amp;\u0026amp; brew upgrade This will update Homebrew to the latest version and upgrade all installed packages to their latest versions. Overall, Homebrew is an excellent tool for macOS users who want to expand their software options beyond what\u0026rsquo;s available in the App Store. It\u0026rsquo;s simple to install and use, and it can save you a lot of time and effort when it comes to installing and managing software on your Mac.\n3. Terminal Working Space # The first part of the blog features an overview of the applications and tools in my workflow, along with brief descriptions. For those with a specific interest in a particular one, links will be provided for further reading. This allows readers to learn more about the tools and potentially discover new resources for their own use.\niterm2 → alacritty -\u0026gt; wezterm zsh → fish tmux → zellij However, I still leave information about previous tools. I also write another blog to talk about how to move to new terminal setting.\nMake A Powerful Terminal Workspace 964 words\u0026middot;5 mins\u0026middot; 0 \u0026middot; 0 macOS Terminal Alacritty # Alacritty is a free and open-source terminal emulator that is designed to be both fast and lightweight. It\u0026rsquo;s written in Rust, a high-performance programming language, and is available for multiple operating systems, including macOS, Linux, and Windows.\nHere\u0026rsquo;s how to get started with Alacritty on macOS:\nFirst, make sure that we have Homebrew installed by following the steps in previous tutorial. Homebrew is the easiest way to install Alacritty on macOS. Open the Terminal app and type the following command to install Alacritty brew install --cask alacritty This command will install Alacritty and all of its dependencies on our system.\nOnce the installation is complete, we can launch Alacritty by typing \u0026ldquo;alacritty\u0026rdquo; in the Terminal window. By default, Alacritty uses a simple and minimalistic configuration. We can customize it by creating a configuration file at the following location: ~/.config/alacritty/alacritty.yml Here, we can set things like the font size, color scheme, and other preferences to suit our needs. We can find more information on how to customize Alacritty on the official website or in the documentation.\nOverall, Alacritty is an choice for users who want a fast and lightweight terminal emulator that\u0026rsquo;s easy to use and customize. With its simple installation process and easy-to-use configuration options, it\u0026rsquo;s a great alternative to other popular terminal emulators on macOS.\nZellij # Zellij is a free and open-source terminal workspace that allows us to create and manage multiple terminal sessions within a single window. Compared to tmux, zellij provide more friendly interface.\nHere\u0026rsquo;s how to get started with Zellij:\nbrew install zellij By default, Zellij uses a simple and minimalistic configuration. We can customize it by creating a configuration file at the following location:\n~/.config/zellij/config.kdl Here, we can set things like the keybindings, color scheme, and other preferences. We can find more information on how to customize Zellij on the official website or in the documentation. With its simple installation process and easy-to-use configuration options, it\u0026rsquo;s a great alternative to other popular terminal workspaces.\nFish # Fish, or the \u0026ldquo;Friendly Interactive SHell\u0026rdquo; is a free and open-source command-line shell for Unix-based operating systems like macOS and Linux. It\u0026rsquo;s designed to be both easy to use and highly customizable, with a modern and user-friendly interface. It includes some valuable features including autosuggestion by default in comparison with zsh. Hence, we can use it out of box without any efforts for configuration.\nHere\u0026rsquo;s how to get started with Fish on macOS:\nFirst, make sure that we have Homebrew installed on our system by following the steps in my previous tutorial. Homebrew is the easiest way to install Fish on macOS. Open the alacritty and type the following command to install Fish: brew install fish Once the installation is complete, we can launch Fish by typing \u0026ldquo;fish\u0026rdquo; in the Terminal window. This will start a new Fish shell session. By default, Fish uses a simple and minimalistic configuration. We can customize it by creating a configuration file at the following location: ~/.config/fish/config.fish Fish has plugins system as well, and I use fisher to manage plugins. Here, we can set things like the prompt, aliases, and other preferences. We can find more information on how to customize Fish on the official website or in the documentation.\nOne of the unique features of Fish is its auto-suggestion system, which suggests commands as we type based on command history. This can save our time and effort when working with the command line. 4. Command Line Application # Git # Git is a free and open-source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Also, Git a t tool used in the terminal to download and upload data or code to the GitHub. Similarly, Git is shipped with macOS, so we may need to update that by brew upgrade git.\nHere\u0026rsquo;s how to get started with git on macOS:\nbrew install git Conda # Conda is a package, dependency, and environment management for any language such as Python, R, Ruby, C/C++, and more. Conda is an open-source package management system and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies. Conda easily creates, saves, loads, and switches between environments. It was created for Python programs, but it can package and distribute software for any language.\nIn addition, I recommend to use mamba to wrap Conda to accelerate running speed. However, we should install miniforge that is a minimal installer for conda with some pre-configured features if using M1 model. miniforge emphasis on supporting various CPU architectures including Apple M1. We can also use mamba or micromamba to install packages in conda environment.\nTree tree is a recursive directory listing program that produces a depth-indented listing of files. With no arguments, tree lists the files in the current directory. When directory arguments are given, tree lists all the files or directories found in the given directories each in turn. Upon completion of listing all files and directories found, tree returns the total number of files.\ntree Here\u0026rsquo;s how to get started with tree on macOS:\nbrew install tree cheat.sh # Cheat.sh is a free and open-source web service that provides quick access to a wide range of cheat sheets and examples for various programming languages and command-line tools. It\u0026rsquo;s designed to be fast, lightweight, and accessible from any device with an internet connection.\nCheat.sh is an excellent choice for users who want quick and easy access to cheat sheets and examples for various programming languages and command-line tools. With its simple and intuitive interface, it\u0026rsquo;s a great resource for both beginners and experienced users alike.\ndust # dust = du + rust. It like du but more intuitive. Dust is a free and open-source utility for analyzing disk usage. It\u0026rsquo;s designed to be fast, flexible, and easy to use, with a simple command-line interface that allows us to identify and analyze disk usage patterns.\nHere\u0026rsquo;s how to get started with Dust:\nFirst, make sure that Homebrew is installed by following the steps in my previous tutorial. Homebrew is the easiest way to install Dust on macOS. Open the alacritty and type the following command to install Dust: brew install dust hyperfine # Hyperfine is a free and open-source command-line benchmarking utility. It\u0026rsquo;s designed to be fast, flexible, and easy to use, with a simple command-line interface that allows you to quickly measure the performance of your shell commands and scripts.\nHere\u0026rsquo;s how to get started with Hyperfine:\nbrew install hyperfine Hyperfine also provides several options for customizing the benchmarking process, such as specifying the number of runs, warmup iterations, and statistical confidence interval. You can find more information on how to use these options in the official documentation or by running \u0026ldquo;hyperfine \u0026ndash;help\u0026rdquo; in the terminal.\nouch # ouch stands for obvious unified compression helper, and it support tar, .zip, .gz, .xz, .lzma, .bz, .bz2, .lz4, .sz and .zst. ouch decompress a.zip for decompressing, ouch cmopress one.txt two.txt archive.zip for compression.\nHere\u0026rsquo;s how to get started with ouch:\ncargo install ouch topgrade # Topgrade is a free and open-source utility for upgrading all your packages. It\u0026rsquo;s designed to be fast, flexible, and easy to use, with a simple command-line interface that allows you to quickly upgrade your packages without worrying about dependencies or conflicts.\nHere\u0026rsquo;s how to get started with Topgrade:\nbrew install topgrade imagemagick # ImageMagick is a free and open-source command-line utility for manipulating and converting images. It\u0026rsquo;s designed to be fast, flexible, and easy to use, with a simple command-line interface that allows you to perform a wide range of image processing tasks.\nHere\u0026rsquo;s how to get started with ImageMagick:\nbrew install imagemagick Once the installation is complete, you can use ImageMagick by running the \u0026ldquo;convert\u0026rdquo; command in the Terminal window, followed by the name of the input file, the name of the output file, and any optional parameters that you want to use. For example, to resize an image and save it as a JPEG file, you can type the following:\nconvert input.jpg -resize 800x600 output.jpg This will resize the \u0026ldquo;input.jpg\u0026rdquo; file to 800x600 pixels and save the result as \u0026ldquo;output.jpg\u0026rdquo;. We can find more information on how to use the \u0026ldquo;convert\u0026rdquo; command and its various parameters in the official documentation or by running \u0026ldquo;man convert\u0026rdquo; in the terminal.\nIn addition to the \u0026ldquo;convert\u0026rdquo; command, ImageMagick provides a wide range of other commands for performing various image processing tasks, such as \u0026ldquo;identify\u0026rdquo; for displaying image metadata, \u0026ldquo;composite\u0026rdquo; for compositing multiple images, and \u0026ldquo;montage\u0026rdquo; for creating image montages. We can find a full list of ImageMagick commands and their descriptions in the official documentation. ImageMagick also provides a powerful and flexible API that we can use to integrate image processing functionality into our own programs and scripts. The API is available in several programming languages, including C, Perl, Python, and Ruby, and provides a wide range of functions for manipulating and converting images.\nyoutobe-dl # [youtube-dl] is a free and open-source command-line utility for downloading videos from YouTube and other video sharing websites. It\u0026rsquo;s designed to be fast, flexible, and easy to use, with a simple command-line interface that allows us to download videos with various options.\nHere\u0026rsquo;s how to get started with youtube-dl:\nbrew install youtube-dl Once the installation is complete, you can use youtube-dl by running the \u0026ldquo;youtube-dl\u0026rdquo; command in the Terminal window, followed by the URL of the video that you want to download. For example, to download a YouTube video, you can type the following:\nyoutube-dl https://www.youtube.com/watch?v=VIDEO_ID Replace \u0026ldquo;VIDEO_ID\u0026rdquo; with the actual ID of the video that you want to download. youtube-dl will automatically detect the best available format and quality for the video, and save it to your current working directory. In addition to downloading videos, youtube-dl provides a wide range of other options for customizing the download process, such as selecting a specific video format, downloading only the audio, downloading subtitles, and downloading entire playlists or channels. You can find more information on how to use these options in the official documentation or by running \u0026ldquo;youtube-dl \u0026ndash;help\u0026rdquo; in the terminal.\nlsd # 5. Windows Management # If you\u0026rsquo;re looking for a powerful and flexible way to manage windows on Mac, you might want to consider using yabai, skhd, and sketchybar. These are a set of open-source utilities that provide advanced window management features, allowing to control the layout, positioning, and sizing of windows with ease.\nI have blog to talk about how to use the tools. Tiling Windows Management in macOS 497 words\u0026middot;3 mins\u0026middot; 0 \u0026middot; 0 macOS Windows Management Here\u0026rsquo;s how to get started:\nFirst, we need to install Homebrew on Mac if it haven\u0026rsquo;t already. We can follow the steps in my previous tutorial to do this. Once Homebrew is installed, open the Terminal app ( alacritty) and type the following command to install [yabai], [skhd], and [skychybar]: brew install koekeishiya/formulae/yabai brew install koekeishiya/formulae/skhd brew tap FelixKratz/formulae brew install sketchybar Once the installations are complete, we need to create configuration files for [yabai] and [skhd]. These files define the keyboard shortcuts and settings, which are used to control windows.\nAnd that\u0026rsquo;s it! With [yabai], [skhd], and [skychybar], we can now manage windows on Mac using keyboard shortcuts. These tools offer a lot of flexibility, so don\u0026rsquo;t be afraid to experiment and find\nAlternative combination:\nMagnet A tool is used to manage windows for different applications Rectangle A tool to move and resize windows in macOS AltTab is a good tool to manage windows for different applications 6. Editor # When it comes to coding on macOS, we have a variety of options for text editors, including Neovim, VSCode, and JetBrains. Each editor has its own strengths and weaknesses, and choosing the right one will depend on personal preferences and needs. In this section, I\u0026rsquo;ll introduce each editor and compare them to help to decide which one is right.\nNeovim # Neovim is a fork of the popular text editor Vim, with the goal of modernizing and improving upon Vim\u0026rsquo;s functionality. It\u0026rsquo;s a powerful text editor that\u0026rsquo;s highly customizable, with a strong focus on keyboard shortcuts and extensibili great choice if you\u0026rsquo;re looking for a lightweight, fast, and highly configurable editor. It\u0026rsquo;s great for coding in a terminal, with a vast array of plugins available for customizing workflow. It does have a steeper learning curve than some other editors. Neovim is my favorite tool and I have written a blog about personal developed environment.\nVS Code # [VSCode] is a popular open-source text editor developed by Microsoft. It\u0026rsquo;s built on top of the Electron framework and provides a modern, customizable user interface. It supports a wide range of programming languages and has a vast collection of extensions available.\n[VSCode] is a great choice if you\u0026rsquo;re looking for a powerful and user-friendly editor that supports a wide range of programming languages. It\u0026rsquo;s a popular choice among developers for its ease of use, extensive plugin ecosystem, and powerful debugging features.\nJetBrains # JetBrains is a company that develops a variety of popular IDEs, including IntelliJ IDEA, PyCharm, and Clion. These IDEs provide a complete development environment, with powerful code editors, debugging tools, and support for a wide range of programming languages and frameworks. JetBrains is a great choice if you\u0026rsquo;re looking for a complete development environment that includes everything you need to build complex applications. It\u0026rsquo;s particularly useful if you\u0026rsquo;re working with a large codebase or complex projects, as it provides powerful refactoring tools and an intelligent code editor.\nHere\u0026rsquo;s a quick rundown of how the editors compare:\nNeovim is highly customizable and has a strong focus on keyboard shortcuts, but has a steeper learning curve. [VSCode] is user-friendly and supports a wide range of programming languages, with a vast collection of plugins available, but can be slow and resource-intensive. JetBrains provides a complete development environment with powerful code editors, debugging tools, and support for a wide range of programming languages and frameworks, but can be costly and resource-intensive.\nOverall, each editor has its own strengths and weaknesses, and choosing the right one will depend on your personal preferences and needs. Neovim is a great choice if you\u0026rsquo;re looking for a highly customizable and fast editor, while [VSCode] is a popular choice if you\u0026rsquo;re looking for a user-friendly editor that supports a wide range of programming languages. JetBrains is a great choice if you\u0026rsquo;re looking for a complete development environment.\n7. Application # This part list variety of applications used for different goals. Every application has a one-word description. Some of them can be installed by Homebrew. My config files and installation commands are kept in GitHub.\nAlfred A tool can give your different control and efficiency in mac Default Folder x A tool can empower default finder Docker is a tool to create a safe environment for development or production Chrome There is no reason not to use it. ❤️ IINA Great tool that is used to play video Imagine Compress images before you upload somewhere. light and powerful! Office 😄 MonitorControl It manages brightness and sound for different monitors Monodraw A tool is used to design fancy ASCII strings PDF Expert Best PDF reader in Mac PicGo A tool is used to upload images to Web service like GitHub. It is beneficial for writing blogs. SnippetsLab My favorite tool stored code snippets, and it can be integrated with Alfred Xcode 😄 Zoom MEETING! Xmind always makes your creative and keeps your minds clears Transmit Upload, download and manage files on servers with beautiful and powerful UI Time Sink is a good tool to record your using time to track your behavior SoundSource can help you get truly powerful control over all the audio Reeder 5 A RSS reader and keep control of your reading Notion A excellent notion tool One Switch can help you finish some progress like keep awake and hide icons on one button iShot A great tool to take screenshots Google Drive is a cloud storage service that allows you to store and share files with anyone Ferdi can integrate other tools like Gmail, Slack, or others to allow you manage information in one place Bartender is a great application to manage icons of all your working tools AlDente is able to keep your battery healthy by controlling the power consumption Zotero is my favorite tools to manage research papers Fluent Reader is modern desktop RSS reader ","date":"26 November 2022","permalink":"/posts/002-macos-configuration/","section":"Posts","summary":"Gallery # Previous Nextsads TODO # add latex add lsd add fd add fzf add ripgrep add procs add zoxide add gitui add btop TLDR # alacritty office aldente one switch alfred pdf expert alttab picgo bartender pycharm cheat.","title":"macOS Setup for Development and Research"},{"content":"The library is a collection of algorithms and data structures that are designed for modern C++ bioinformatics applications. You can use the library in your own projects or as a part of a larger project.\nThe library will include efficient data structure and algorithm implemented by Modern C++.\nThe design philosophy of the library:\nEmbrace C++20 standard Supports modern C++ features Support concurrency and thread safety Priority is given to safety and clean design Hard to use wrongly Testing extensively Python bindings ylab-hi/BINARY BIoiNformatics Algorithms libRarY aka BINARY C\u0026#43;\u0026#43; 2 0 The current project is developing and evolving, and changes will be made to the library as time goes on.\n","date":"26 September 2022","permalink":"/projects/011-bioinformatics-algorithm-library/","section":"Projects","summary":"The library is a collection of algorithms and data structures that are designed for modern C++ bioinformatics applications.","title":"Bioinformatics Algorithm Library aka BINARY"},{"content":"","date":"26 September 2022","permalink":"/tags/data-structure/","section":"Tags","summary":"","title":"Data Structure"},{"content":"BOSS is a bioinformatics toolbox, which will contain efficient tools. It is written in modern C++ and is tested exhaustively. It is designed to be easy to use and time-efficient. BOSS is a free software and is distributed under the terms of the GNU General Public License V3.\nCurrently, BOSS contains the following tools:\nboss-fqsp : A fast tool for splitting fastq files into paired files. boss-squeue: Summary Status of Jobs in HPC Queues cauliyang/boss BioinfOrmaticS toolboxeS aka boss C\u0026#43;\u0026#43; 3 2 It is now evolving and will be updated frequently. Please check the GitHub repository for the latest version.\n","date":"25 September 2022","permalink":"/projects/009-new-tool-bioinformatics-toolbox-aka-boss/","section":"Projects","summary":"BOSS is a bioinformatics toolbox, which will contain efficient tools.","title":"Bioinformatics Toolbox Aka Boss"},{"content":"","date":"25 September 2022","permalink":"/tags/develop/","section":"Tags","summary":"","title":"Develop"},{"content":" Get Random numbers # #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;iterator\u0026gt; #include \u0026lt;random\u0026gt; int main() { std::random_device rd; std::mt19937 rng(rd()); std::uniform_int_distribution\u0026lt;int\u0026gt; dist6(1, 6); std::generate_n(std::ostream_iterator\u0026lt;int\u0026gt;(std::cout, \u0026#34; \u0026#34;), 10, [\u0026amp;dist6, \u0026amp;rng]() { return dist6(rng); }); return 0; } Get random numbers between min and max # // Generate a random number between min and max (inclusive) // Assumes std::srand() has already been called // Assumes max - min \u0026lt;= RAND_MAX int getRandomNumber(int min, int max) { static constexpr double fraction { 1.0 / (RAND_MAX + 1.0) }; // static used for efficiency, so we only calculate this value once // evenly distribute the random number across our range return min + static_cast\u0026lt;int\u0026gt;((max - min + 1) * (std::rand() * fraction)); } Clear Input Stream # if (std::cin.fail()) // has a previous extraction failed or overflowed? { // yep, so let\u0026#39;s handle the failure std::cin.clear(); // put us back in \u0026#39;normal\u0026#39; operation mode std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); // and remove the bad input } Compare Float Number # #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cmath\u0026gt; bool approximatelyEqualAbsRel(double a, double b, double absEpsilon, double relEpsilon) { // Check if the numbers are really close -- needed when comparing numbers near zero. double diff{ std::abs(a - b) }; if (diff \u0026lt;= absEpsilon) return true; // Otherwise fall back to Knuth\u0026#39;s algorithm return (diff \u0026lt;= (std::max(std::abs(a), std::abs(b)) * relEpsilon)); } Shelang for Python # #!/usr/bin/env python3 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; @author: YangyangLi @contact: yangyang.li@northwestern.edu @version: 0.0.1 @license: MIT Licence @file: cli.py.py @time: 2020/12/28 10:21 PM \u0026#34;\u0026#34;\u0026#34; Shelang for Bash # #!/bin/bash set -e set -u set -o pipefail CLI for Rust # #[allow(unused)] use clap::Parser; use env_logger::Builder; use human_panic::setup_panic; use log::info; #[derive(Parser, Debug)] #[command(name = \u0026#34;sv2gf\u0026#34;)] #[command(version = \u0026#34;0.1.0\u0026#34;)] struct Args { /// The input file of svs #[arg(short = \u0026#39;s\u0026#39;, long)] sv: PathBuf, /// The input file of gfs #[arg(short = \u0026#39;g\u0026#39;, long)] gf: PathBuf, /// The Distance threshold #[arg(short = \u0026#39;d\u0026#39;, long, default_value = \u0026#34;1000000\u0026#34;)] dis_threshold: u32, #[clap(flatten)] verbose: clap_verbosity_flag::Verbosity, } fn cli() -\u0026gt; (PathBuf, PathBuf, u32) { let args = Args::parse(); let sv = args.sv; let gf = args.gf; let dis_threshold = args.dis_threshold; info!(\u0026#34;sv: {:?}, gf: {:?}\u0026#34;, sv, gf); (sv, gf, dis_threshold) } fn main() { setup_panic!(); let args = Args::parse(); Builder::new() .filter_level(args.verbose.log_level_filter()) .init(); let (input_sv, input_gf, dis) = cli(); } Export port from two layers nodes # ssh -L 8899:localhost:8899 quest ssh -N -L 8899:localhost:8899 qgpu0101 Timer for C++ # #include \u0026lt;chrono\u0026gt; // for std::chrono functions class Timer { private: // Type aliases to make accessing nested type easier using clock_type = std::chrono::steady_clock; using second_type = std::chrono::duration\u0026lt;double, std::ratio\u0026lt;1\u0026gt; \u0026gt;; std::chrono::time_point\u0026lt;clock_type\u0026gt; m_beg { clock_type::now() }; public: void reset() { m_beg = clock_type::now(); } double elapsed() const { return std::chrono::duration_cast\u0026lt;second_type\u0026gt;(clock_type::now() - m_beg).count(); } }; Gradient Clipping # def grad_clipping(net, theta): if isinstance(net, nn.Module): params = [p for p in net.parameters() if p.requires_grad] else: params = net.params norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params )) # include all parameters if norm \u0026gt; theta: for param in params: param.grad[:] *= theta/nor RST Docstring directive # :Example: \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;hello world!\u0026#34;) hello world! .. note:: can be useful to emphasize .. seealso:: :class:`MainClass2` .. warning:: arg2 must be non-zero. .. todo:: check that arg2 is non zero. Git remove large files # git filter-repo --force --strip-blobs-bigger-than 100M Stop Colab Disconnect # function KeepClicking() { console.log(\u0026#34;Clicking\u0026#34;); document.querySelector(\u0026#34;colab-connect-button\u0026#34;).click(); } setInterval(KeepClicking, 60000); Open Colab # [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) Srun for interactive shell # srun -n 1 -t 1:00:00 -p gpu --gres=gpu:tesla:1 --pty bash Bash info for Slurm # #!/bin/bash -l #SBATCH --job-name=MLHW3 # job name #SBATCH --output=MLHW3_%j.log # log name #SBATCH --time=8:00:00 # time #SBATCH --ntasks=1 # tasks number #SBATCH --cpus-per-task=8 # cpu number #SBATCH --ntasks-per-node=1 # node number #SBATCH --mem=30G # total memory #SBATCH --tmp=30G # #SBATCH --mail-type=ALL #SBATCH --mail-user=li002252@umn.edu #SBATCH --gres=gpu:v100:1 # gpu #SBATCH -p v100 # partitions cd $SLURM_SUBMIT_DIR date;hostname;pwd Conda export env # conda env export | grep -v \u0026#39;^prefix\u0026#39; \u0026gt; freeze.yml conda env create -f freeze.yml Multiple output in Jupyter # from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026#34;all\u0026#34; Multiple Images in Latex # \\usepackage{subcaption} \\begin{figure}[h] \\begin{subfigure}{0.5\\textwidth} \\includegraphics[width=0.9\\linewidth, height=5cm]{rna_ttest.png} \\caption{T-test for RNA-Seq data} \\label{fig:subim1} \\end{subfigure} \\begin{subfigure}{0.5\\textwidth} \\includegraphics[width=0.9\\linewidth, height=5cm]{rna_ranksum.png} \\caption{Rank-sum for RNA-Seq data} \\label{fig:subim2} \\end{subfigure} \\begin{subfigure}{0.5\\textwidth} \\includegraphics[width=0.9\\linewidth, height=5cm]{micro_ttest.png} \\caption{T-test for Microarray data} \\label{fig:subim1} \\end{subfigure} \\begin{subfigure}{0.5\\textwidth} \\includegraphics[width=0.9\\linewidth, height=5cm]{micro_ranksum.png} \\caption{Rank-sum for Microarray data} \\label{fig:subim2} \\end{subfigure} \\caption{Histogram of the p-values of all genes} \\end{figure} Show fonts in Jupyter # import matplotlib.font_manager from IPython.core.display import HTML def make_html(fontname): return \u0026#34;\u0026lt;p\u0026gt;{font}: \u0026lt;span style=\u0026#39;font-family:{font}; font-size: 24px;\u0026#39;\u0026gt;{font}\u0026lt;/p\u0026gt;\u0026#34;.format(font=fontname) code = \u0026#34;\\n\u0026#34;.join([make_html(font) for font in sorted(set([f.name for f in matplotlib.font_manager.fontManager.ttflist]))]) HTML(\u0026#34;\u0026lt;div style=\u0026#39;column-count: 2;\u0026#39;\u0026gt;{}\u0026lt;/div\u0026gt;\u0026#34;.format(code)) Display image in Jupyter # from IPython.display import Image from IPython.core.display import HTML PATH = \u0026#34;tree_default_max_depth.png\u0026#34; Image(filename = PATH , width=900, height=900) ","date":"22 September 2022","permalink":"/posts/023-cpp-snippet/","section":"Posts","summary":"Get Random numbers # #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;iterator\u0026gt; #include \u0026lt;random\u0026gt; int main() { std::random_device rd; std::mt19937 rng(rd()); std::uniform_int_distribution\u0026lt;int\u0026gt; dist6(1, 6); std::generate_n(std::ostream_iterator\u0026lt;int\u0026gt;(std::cout, \u0026#34; \u0026#34;), 10, [\u0026amp;dist6, \u0026amp;rng]() { return dist6(rng); }); return 0; } Get random numbers between min and max # // Generate a random number between min and max (inclusive) // Assumes std::srand() has already been called // Assumes max - min \u0026lt;= RAND_MAX int getRandomNumber(int min, int max) { static constexpr double fraction { 1.","title":"Code Snippets"},{"content":" Alert # {{\u0026lt; alert \u0026gt;}} **Warning!** This action is destructive! {{\u0026lt; /alert \u0026gt;}} Warning! This action is destructive! {{\u0026lt; alert cardColor=\u0026#34;red\u0026#34; \u0026gt;}} **Warning!** This action is destructive! {{\u0026lt; /alert \u0026gt;}} Warning! This action is destructive! {{\u0026lt; alert textColor=\u0026#34;red\u0026#34; \u0026gt;}} **Warning!** This action is destructive! {{\u0026lt; /alert \u0026gt;}} Warning! This action is destructive! {{\u0026lt; alert \u0026#34;twitter\u0026#34; \u0026gt;}} **Warning!** This action is destructive! {{\u0026lt; /alert \u0026gt;}} Do not forget to follow me {{\u0026lt; alert icon=\u0026#34;fire\u0026#34; cardColor=\u0026#34;#e63946\u0026#34; iconColor=\u0026#34;#1d3557\u0026#34; textColor=\u0026#34;#f1faee\u0026#34; \u0026gt;}} **Warning!** This action is destructive! {{\u0026lt; /alert \u0026gt;}} Do not forget to follow me Article # {{\u0026lt; article link=\u0026#34;/posts/002-macos-configuration/\u0026#34; \u0026gt;}} macOS Setup for Development and Research Updated: 2 September 2023\u0026middot;3281 words\u0026middot;16 mins\u0026middot; 0 \u0026middot; 0 macOS Development Badge # {{\u0026lt; badge \u0026gt;}} New article! {{\u0026lt; /badge \u0026gt;}} New article! Button # {{\u0026lt; button href=\u0026#34;#button\u0026#34; target=\u0026#34;_self\u0026#34; \u0026gt;}} Call to action {{\u0026lt; /button \u0026gt;}} Call to action {{\u0026lt; button href=\u0026#34;https://blowfish.page/docs/shortcodes/#icon\u0026#34; target=\u0026#34;_self\u0026#34; \u0026gt;}} Call to action {{\u0026lt; /button \u0026gt;}} Open external links Carousel # carousel is used to showcase multiple images in an interactive and visually appealing way. This allows a user to slide through multiple images while only taking up the vertical space of a single one. All images are displayed using the full width of the parent component and using one of the predefined aspect ratios of 16:9, 21:9 or 32:9.\nParameter Description images Required. A regex string to match image names. aspectRatio Optional. The aspect ratio for the carousel. Either 16-9, 21-9 or 32-9. It is set to 16-9 by default. interval Optional. The interval for the auto-scrooling, specified in milliseconds. Defaults to 2000 (2s) {{\u0026lt; carousel images=\u0026#34;{gallery/03.jpg, gallery/01.jpg, gallery/02.jpg, gallery/04.jpg}\u0026#34; \u0026gt;}} Previous Nextsads Example 2: 21:9 aspect ratio and regex-ed list of images\n{{\u0026lt; carousel images=\u0026#34;gallery/*\u0026#34; aspectRatio=\u0026#34;21-9\u0026#34; interval=\u0026#34;2500\u0026#34; \u0026gt;}} Previous Nextsads Chart # chart uses the Chart.js library to embed charts into articles using simple structured data. It supports a number of different chart styles and everything can be configured from within the shortcode. Simply provide the chart parameters between the shortcode tags and Chart.js will do the rest.\nRefer to the official Chart.js docs for details on syntax and supported chart types.\n{{\u0026lt; chart \u0026gt;}} type: \u0026#39;bar\u0026#39;, data: { labels: [\u0026#39;Tomato\u0026#39;, \u0026#39;Blueberry\u0026#39;, \u0026#39;Banana\u0026#39;, \u0026#39;Lime\u0026#39;, \u0026#39;Orange\u0026#39;], datasets: [{ label: \u0026#39;# of votes\u0026#39;, data: [12, 19, 3, 5, 3], }] } {{\u0026lt; /chart \u0026gt;}} Figure # Blowfish includes a figure shortcode for adding images to content. The shortcode replaces the base Hugo functionality in order to provide additional performance benefits.\nWhen a provided image is a page resource, it will be optimised using Hugo Pipes and scaled in order to provide images appropriate to different device resolutions. If a static asset or URL to an external image is provided, it will be included as-is without any image processing by Hugo.\nThe figure shortcode accepts six parameters:\nParameter Description src Required. The local path/filename or URL of the image. When providing a path and filename, the theme will attempt to locate the image using the following lookup order: Firstly, as a page resource bundled with the page; then an asset in the assets/ directory; then finally, a static image in the static/ directory. alt Alternative text description for the image. caption Markdown for the image caption, which will be displayed below the image. class Additional CSS classes to apply to the image. href URL that the image should be linked to. default Special parameter to revert to default Hugo figure behaviour. Simply provide default=true and then use normal Hugo shortcode syntax. Blowfish also supports automatic conversion of images included using standard Markdown syntax. Simply use the following format and the theme will handle the rest:\n![Alt text](image.jpg \u0026#34;Image caption\u0026#34;) Example:\n{{\u0026lt; figure src=\u0026#34;abstract.jpg\u0026#34; alt=\u0026#34;Abstract purple artwork\u0026#34; caption=\u0026#34;Photo by [Jr Korpa](https://unsplash.com/@jrkorpa) on [Unsplash](https://unsplash.com/)\u0026#34; \u0026gt;}} Photo by Jr Korpa on Unsplash ![Abstract purple artwork](abstract.jpg \u0026#34;Photo by [Jr Korpa](https://unsplash.com/@jrkorpa) on [Unsplash](https://unsplash.com/)\u0026#34;) Photo by Jr Korpa on Unsplash Gallery # gallery allows you to showcase multiple images at once, in a responsive manner with more varied and interesting layouts.\nIn order to add images to the gallery, use img tags for each image and add class=\u0026quot;grid-wXX\u0026quot; in order for the gallery to be able to identify the column width for each image. The widths available by default start at 10% and go all the way to 100% in 5% increments. For example, to set the width to 65%, set the class to grid-w65. Additionally, widths for 33% and 66% are also available in order to build galleries with 3 cols. You can also leverage tailwind\u0026rsquo;s responsive indicators to have a responsive grid.\n{{\u0026lt; gallery \u0026gt;}} \u0026lt;img src=\u0026#34;gallery/01.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/02.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/03.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/04.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/05.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/06.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/07.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; {{\u0026lt; /gallery \u0026gt;}} GitHub Card # github allows you to quickly link a github repository, all while showing and updating in realtime stats about it, such as the number of stars and forks it has.\nParameter Description repo [String] github repo in the format of username/repo {{\u0026lt; github repo=\u0026#34;cauliyang/cauliyang\u0026#34; \u0026gt;}} cauliyang/cauliyang null 1 1 GitLab Card # gitlab allows you to quickly link a GitLab Project (GitLab\u0026rsquo;s jargon for repo). It displays realtime stats about it, such as the number of stars and forks it has. Unlike github it can\u0026rsquo;t display the main programming language of a project. Finally custom GitLab instance URL can be provided, as long as the api/v4/projects/ endpoint is available, making this shortcode compatible with most self-hosted / entreprise deployments.\nParameter Description projectID [String] gitlab numeric ProjectID baseURL [String] optional gitlab instance URL, default is https://gitlab.com/ {{\u0026lt; gitlab projectID=\u0026#34;278964\u0026#34; \u0026gt;}} GitLab.org / GitLab GitLab is an open source end-to-end software development platform with built-in version control, issue tracking, code review, CI/CD, and more. Self-host GitLab on your own servers, in a container, or on a cloud provider. 4662 8958 Icon # icon outputs an SVG icon and takes the icon name as its only parameter. The icon is scaled to match the current text size.\nExample:\n{{\u0026lt; icon \u0026#34;github\u0026#34; \u0026gt;}} Output: Icons are populated using Hugo pipelines which makes them very flexible. Blowfish includes a number of built-in icons for social, links and other purposes. Check the icon samples page for a full list of supported icons. Custom icons can be added by providing your own icon assets in the assets/icons/ directory of your project. The icon can then be referenced in the shortcode by using the SVG filename without the .svg extension. Icons can also be used in partials by calling the icon partial.\nKatex # The katex shortcode can be used to add mathematical expressions to article content using the KaTeX package. Refer to the online reference of supported TeX functions for the available syntax. To include mathematical expressions in an article, simply place the shortcode anywhere with the content. It only needs to be included once per article and KaTeX will automatically render any markup on that page. Both inline and block notation are supported. Inline notation can be generated by wrapping the expression in \\\\( and \\\\) delimiters. Alternatively, block notation can be generated using $$ delimiters.\nExample:\n{{\u0026lt; katex \u0026gt;}} \\\\(f(a,b,c) = (a^2+b^2+c^2)^3\\\\) \\(f(a,b,c) = (a^2+b^2+c^2)^3\\)\nLead # lead is used to bring emphasis to the start of an article. It can be used to style an introduction, or to call out an important piece of information. Simply wrap any Markdown content in the lead shortcode.\nExample:\n{{\u0026lt; lead \u0026gt;}} When life gives you lemons, make lemonade. {{\u0026lt; /lead \u0026gt;}} When life gives you lemons, make lemonade. List # List will display a list of recent articles. This shortcode requires a limit value to constraint the list. Additionally, it supports a where and a value in order to filter articles by their parameters. Note that this shortcode will not display its parent page but it will count for the limit value.\nParameter Description limit Required. the number of recent articles to display. title Optional title for the list, default is Recent where The variable to be used for the query of articles e.g. Type value The value that will need to match the parameter defined in where for the query of articles e.g. for where == Type a valid value could be sample The where and value values are used in the following query where .Site.RegularPages $where $value in the code of the shortcode. Check Hugo docs to learn more about which parameters are available to use. {{\u0026lt; list limit=2 \u0026gt;}} Recent Autonamte Latex Build Processing 867 words\u0026middot;5 mins\u0026middot; 0 \u0026middot; 0 Latex CI CUDA for Deep Learning Inference in Rust and C++ 1329 words\u0026middot;7 mins\u0026middot; 0 \u0026middot; 0 Rust C\u0026#43;\u0026#43; CUDA {{\u0026lt; list title=\u0026#34;Samples\u0026#34; limit=5 where=\u0026#34;Type\u0026#34; value=\u0026#34;sample\u0026#34; \u0026gt;}} Posts CUDA for Deep Learning Inference in Rust and C++ 1329 words\u0026middot;7 mins\u0026middot; 0 \u0026middot; 0 Rust C\u0026#43;\u0026#43; CUDA Development Tips 32 words\u0026middot;1 min\u0026middot; 0 \u0026middot; 0 Development Efficient Genomic Interval Search Using SIMD-Enhanced COITree 1412 words\u0026middot;7 mins\u0026middot; 0 \u0026middot; 0 Rust Algorithm Bioinformatics How to Use Noodles Library in Rust 2058 words\u0026middot;10 mins\u0026middot; 0 \u0026middot; 0 Rust Bioinformatics Tiling Windows Management in macOS 497 words\u0026middot;3 mins\u0026middot; 0 \u0026middot; 0 macOS Windows Management Mermaid # mermaid allows you to draw detailed diagrams and visualisations using text. It uses Mermaid under the hood and supports a wide variety of diagrams, charts and other output formats.\nSimply write your Mermaid syntax within the mermaid shortcode and let the plugin do the rest.\nRefer to the official Mermaid docs for details on syntax and supported diagram types.\nExample:\n{{\u0026lt; mermaid \u0026gt;}} graph LR; A[Lemons]--\u0026gt;B[Lemonade]; B--\u0026gt;C[Profit] {{\u0026lt; /mermaid \u0026gt;}} graph LR; A[Lemons]--\u003eB[Lemonade]; B--\u003eC[Profit] Swatches # swatches outputs a set of up to three different colors to showcase color elements like a color palette. This shortcode takes the HEX codes of each color and creates the visual elements for each.\n{{\u0026lt; swatches \u0026#34;#64748b\u0026#34; \u0026#34;#3b82f6\u0026#34; \u0026#34;#06b6d4\u0026#34; \u0026gt;}} Output Timeline # The timeline creates a visual timeline that can be used in different use-cases, e.g. professional experience, a project\u0026rsquo;s achievements, etc. The timeline shortcode relies on the timelineItem sub-shortcode to define each item within the main timeline. Each item can have the following properties.\nParameter Description icon the icon to be used in the timeline visuals. header header for each entry badge text to place within the top right badge subheader entry\u0026rsquo;s subheader Example:\n{{\u0026lt; timeline \u0026gt;}} {{\u0026lt; timelineItem icon=\u0026#34;github\u0026#34; header=\u0026#34;header\u0026#34; badge=\u0026#34;badge test\u0026#34; subheader=\u0026#34;subheader\u0026#34; \u0026gt;}} Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus non magna ex. Donec sollicitudin ut lorem quis lobortis. Name ac ipsum libero. Sed a ex eget ipsum tincidunt venenatis quis sed nisl. Pellentesque sed urna vel odio consequat tincidunt id ut purus. Name sollicitudin est sed dui interdum rhoncus. {{\u0026lt; /timelineItem \u0026gt;}} {{\u0026lt; timelineItem icon=\u0026#34;code\u0026#34; header=\u0026#34;Another Awesome Header\u0026#34; badge=\u0026#34;date - present\u0026#34; subheader=\u0026#34;Awesome Subheader\u0026#34; \u0026gt;}} With html code \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;Coffee\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Tea\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Milk\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; {{\u0026lt; /timelineItem \u0026gt;}} {{\u0026lt; timelineItem icon=\u0026#34;star\u0026#34; header=\u0026#34;Shortcodes\u0026#34; badge=\u0026#34;AWESOME\u0026#34; \u0026gt;}} With other shortcodes {{\u0026lt; gallery \u0026gt;}} \u0026lt;img src=\u0026#34;gallery/01.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/02.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/03.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/04.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/05.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/06.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;gallery/07.jpg\u0026#34; class=\u0026#34;grid-w33\u0026#34; /\u0026gt; {{\u0026lt; /gallery \u0026gt;}} {{\u0026lt; /timelineItem \u0026gt;}} {{\u0026lt; /timeline \u0026gt;}} header badge test subheader Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus non magna ex. Donec sollicitudin ut lorem quis lobortis. Name ac ipsum libero. Sed a ex eget ipsum tincidunt venenatis quis sed nisl. Pellentesque sed urna vel odio consequat tincidunt id ut purus. Name sollicitudin est sed dui interdum rhoncus. Another Awesome Header date - present Awesome Subheader With html code Coffee Tea Milk Shortcodes AWESOME With other shortcodes TypeIt # TypeIt is the most versatile JavaScript tool for creating typewriter effects on the planet. With a straightforward configuration, it allows you to type single or multiple strings that break lines, delete \u0026amp; replace each other, and it even handles strings that contain complex HTML.\nBlowfish implements a sub-set of TypeIt features using a shortcode. Write your text within the typeit shortcode and use the following parameters to configure the behavior you want.\nParameter Description tag [String] html tag that will be used to render the strings. classList [String] List of css classes to apply to the html element. initialString [String] Initial string that will appear written and will be replaced. speed [number] Typing speed, measured in milliseconds between each step. lifeLike [boolean] Makes the typing pace irregular, as if a real person is doing it. startDelay [number] The amount of time before the plugin begins typing after being initialized. breakLines [boolean] Whether multiple strings are printed on top of each other (true), or if they\u0026rsquo;re deleted and replaced by each other (false). waitUntilVisible [boolean] Determines if the instance will begin when loaded or only when the target element becomes visible in the viewport. The default is true loop [boolean] Whether your strings will continuously loop after completing Example 1:\n{{\u0026lt; typeit \u0026gt;}} Lorem ipsum dolor sit amet {{\u0026lt; /typeit \u0026gt;}} Example 2:\n{{\u0026lt; typeit tag=h1 lifeLike=true \u0026gt;}} Lorem ipsum dolor sit amet, consectetur adipiscing elit. {{\u0026lt; /typeit \u0026gt;}} Example 3:\n{{\u0026lt; typeit tag=h3 speed=50 breakLines=false loop=true \u0026gt;}} Lorem ipsum dolor sit amet, consectetur adipiscing elit. {{\u0026lt; /typeit \u0026gt;}} Embed PDF # Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","date":"24 June 2022","permalink":"/posts/short-code/","section":"Posts","summary":"Alert # {{\u0026lt; alert \u0026gt;}} **Warning!","title":"Short Code Example"},{"content":" 1.1 Config Compile Environment # I am currently planning to develop a tool using C++ in both Linux and macOS environments. However, I frequently encounter obstacles in the form of lacking root access to download dependencies using apt-get install -y dependencies directly in Ubuntu. Navigating the complicated dependency chain and compiling each library individually can be time-consuming, often taking a night or even a week to complete. One solution to this issue is to use a package manager such as Conda, which is primarily used in the data science domain. Conda offers support for other languages such as C++, Rust and R as well. Concrete package names may change at any time, and it\u0026rsquo;s necessary to search for the real package name. Therefore, Conda can be useful tool for installing C++ dependencies, particularly in the bioinformatics domain. It\u0026rsquo;s worth mentioning that there are several other solutions available for managing C++ dependencies such as Vcpkg, Conan, and I use CPM as an alternative option.\n1.2 Install GCC or Clang # When it comes to the compilation environment, it is important to install a compiler, and GCC or Clang may be your choices. In general, Linux systems will ship with GCC, but the version may be low (4. 9). That will not allow you to use the latest features of C++. In the meantime, you do not have root access yet. But you can use Conda to install any version of GCC or Clang by running conda install -c conda-forge gcc or conda install -c conda-forge clang. Keep in mind that you should search for GCC or clang in Conda cloud first before installation in order to install the proper version.\nAfter installation, Conda may set three significant variables for you: CFLAGS, CXXFLAGS, and LDFLAGS. You can check that by using echo $CFLAGS. You need to set that in your ~/.bashrc or ~/.zshrc if you do not find that. Here are examples:\nexport CXXFLAGS=\u0026#34;-fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /your_conda_absolute_path/miniconda3/include\u0026#34; export CFLAGS=\u0026#34;-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /your_conda_absolute_path/include\u0026#34; export LDFLAGS=\u0026#34;-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/your_conda_absolute_path/miniconda3/lib -Wl,-rpath-link,/your_conda_absolute_path/miniconda3/lib -L/your_conda_absolute_path/miniconda3/lib\u0026#34; You must change your Conda absolute path to your path of Conda. I installed Conda in the base environment, and I also recommend installing GCC or clang in the base environment.\n2. Add htslib dependencies # Htslib is a classic and well-known library used to manage bioinformatics format files, including bam, sam, vcf, and bcf, etc.\nHtslib is implemented in C so that it is able to meet high performance requirements. So far, many popular tools, for example samtools and bcftools, are all based on htslib. There are wrappers for other languages like pysam based on Python, rhtslib based on R, and [rust_htslib] based on Rust. That will enable people using different languages to apply htslib in their applications.\nZlib is only one library that htslib must depend on. In the meantime, htslib has other dependencies, which enables htslib to have more rich features. Here, I will not explain what feature dependencies will provide respectably. The detailed information can be found on the Htslib website. However, *Conda* can install htslib easily by conda install htslib. Keep in mind that we can also define the library version with conda install htslib=1.15.1. As mentioned above, htslib may have been installed in your environment if samtools or bcftools are already installed.\n2.1 Cmake Scripts # I use Cmake as a build system, and here are several useful and valuable Cmake scripts. You can use that in your project. I think it will help you fix most dependency problems with htslib.\nhtslib.cmake\nFindHTSlib.cmake\nFindDeflate.cmake\nzlib.cmake\nFirstly, I assume your directory structure looks like this:\n. ├── CMakeLists.txt ├── build ├── cmake ├── include └── source These cmake scripts should be found in the cmake directory. We can use in CmakeLists.txt\nlist(APPEND CMAKE_MODULE_PATH \u0026#34;${CMAKE_CURRENT_SOURCE_DIR}/cmake\u0026#34;) include(htslib) If htslib is found in your current environment, cmake will define the variables HTSlib_FOUND and HTSlib_INCLUDE_DIRS``andHTSlib_LIBRARIES. Otherwise, cmakewill build static htslib from sources, andcmakewill check if every dependency of htslib exists respectively. If it exists,cmakewill use the compiler flags of htslib to build a static library. Otherwise,cmakewill disable related compiler flags. However, zlib is the only one that must exist. So, if zlib does not exist,cmake will help you build zlib from source. All these CMake scripts can be found at the above link. Please feel free to explore that.\nIndeed, we use FindHTSlib.cmake to search for htslib. htslib.cmake is shown below, and I have added some comments to explain how it works.\ninclude(ExternalProject) # find htslib by using FindHTSlib.cmake find_package(HTSlib) if(HTSlib_FOUND) message(STATUS \u0026#34;HTSlib_USE_STATIC_LIBS: ${HTSlib_USE_STATIC_LIBS}\u0026#34;) # not found, try to build it to static libs from source else() set(htslib_PREFIX ${CMAKE_BINARY_DIR}/cmake-ext/htslib-prefix) set(htslib_INSTALL ${CMAKE_BINARY_DIR}/cmake-ext/htslib-install) if(CMAKE_GENERATOR STREQUAL \u0026#34;Unix Makefiles\u0026#34;) set(MAKE_COMMAND \u0026#34;$(MAKE)\u0026#34;) else() find_program(MAKE_COMMAND NAMES make gmake) endif() message(STATUS \u0026#34;Building static htslib from source\u0026#34;) message(NOTICE \u0026#34;Set ENV CFLAGS and CXXFLAGS if you use conda environment!\u0026#34;) set(disable_flags --disable-gcs --disable-s3 --disable-plugins) # find lzma if not founed the disable compiler flags find_package(LibLZMA) if(LIBLZMA_FOUND) include_directories(SYSTEM ${LIBLZMA_INCLUDE_DIRS}) list(APPEND deps_LIB ${LIBLZMA_LIBRARIES}) else() list(APPEND disable_flags --disable-lzma) endif() # find curl if not founed the disable compiler flags find_package(CURL) if(CURL_FOUND) include_directories(SYSTEM ${CURL_INCLUDE_DIRS}) list(APPEND deps_LIB ${CURL_LIBRARIES}) else() list(APPEND disable_flags --disable-libcurl) endif() #find bzip2 if not founed the disable compiler flags find_package(BZip2) if(BZIP2_FOUND) include_directories(SYSTEM ${BZIP2_INCLUDE_DIRS}) list(APPEND deps_LIB ${BZIP2_LIBRARIES}) else() list(APPEND disable_flags --disable-bz2) endif() # find defalte if not founed the disable compiler flags find_package(Deflate) if(Deflate_FOUND) include_directories(SYSTEM ${Deflate_INCLUDE_DIRS}) list(APPEND deps_LIB ${Deflate_LIBRARIES}) endif() message(STATUS \u0026#34; dependencies: ${deps_LIB}\u0026#34;) # compiler and install htslib from source ExternalProject_Add( htslib PREFIX ${htslib_PREFIX} URL https://github.com/samtools/htslib/releases/download/1.15.1/htslib-1.15.1.tar.bz2 BUILD_IN_SOURCE 1 UPDATE_COMMAND \u0026#34;\u0026#34; CONFIGURE_COMMAND autoreconf -i \u0026amp;\u0026amp; ./configure --prefix=${htslib_PREFIX} ${disable_flags} BUILD_COMMAND ${MAKE_COMMAND} lib-static INSTALL_COMMAND ${MAKE_COMMAND} install prefix=${htslib_INSTALL} ) # user pre-defined variable (-DZLIB_BUILD=ON) to control is build zlib from sources message(STATUS \u0026#34;ZLIB_BUILD: ${ZLIB_BUILD}\u0026#34;) if(ZLIB_BUILD) include(cmake/zlib.cmake) add_dependencies(htslib zlib) else() find_package(ZLIB) if(ZLIB_FOUND) include_directories(SYSTEM ${ZLIB_INCLUDE_DIRS}) list(APPEND deps_LIB ${ZLIB_LIBRARIES}) else() # build zlib from source message(STATUS \u0026#34;Building zlib from source\u0026#34;) include(cmake/zlib.cmake) add_dependencies(htslib zlib) list(APPEND deps_LIB ${zlib_LIBRARIES}) endif() endif() list(APPEND deps_LIB ${zlib_LIBRARIES}) # define two variables for usage set(HTSlib_INCLUDE_DIRS ${htslib_INSTALL}/include) set(HTSlib_LIBRARIES ${htslib_INSTALL}/lib/libhts.a ${deps_LIB}) message(STATUS \u0026#34;HTSlib_INCLUDE_DIRS: ${HTSlib_INCLUDE_DIRS}\u0026#34;) message(STATUS \u0026#34;HTSlib_LIBRARIES: ${HTSlib_LIBRARIES}\u0026#34;) endif() Now you can use the htslib like this in cmake:\nadd_library(test test.h test.cpp) # if htslib is not in your environment if(NOT HTSlib_FOUND) # if htslib build from source you need add this add_dependencies(${PROJECT_NAME} htslib) endif() target_link_libraries(test PRIVATE ${HTSlib_LIBRARIES}) target_include_directories(test PRIVATE ${HTSlib_INCLUDE_DIRS} ${HTSlib_INCLUDE_DIRS}/htslib) You can change PRIVATE to PUBLIC if you want to export htslib. It depends on your goal.\n3. Recommend Practices # In the beginning, I recommend you use a suitable directory structure. Here are a few of the best examples:\nhttps://github.com/TheLartians/ModernCppStarter https://github.com/cpp-best-practices/gui_starter_template https://github.com/filipdutescu/modern-cpp-template I prefer the first one. Using a good template can help you learn other tools in order to make the project better. After you dive into one of the templates, I think you will learn more than you imagined.\n4. Summary # In this blog, I will introduce how to install C++ dependencies and how to configure htslib in your development environment. Various cmake scripts are provided and can help the project fix htslib dependency problems smoothly. The source of these scripts at the top. If you have any questions, please feel free to reach me. Thanks for your time and reading!\n","date":"15 June 2022","permalink":"/posts/003-build-cpp-development-env-with-htslib/","section":"Posts","summary":"1.","title":"C++ Development in Bioinformatics"},{"content":" 0. Changes # Add usage 1. Introduction # I have been considering a collection of useful commands for everyday tasks, as many have requested such a tool. In order to make this collection as user-friendly as possible, I have decided to develop a project, which I hope will prove valuable for others as well. As I come across new commands that I find useful, I will continue to add them to this toolbox. Currently, the toolbox includes a command for downloading files from Google Driver, which can download both individual files and those within a folder, though downloading files within a folder requires authentication through the Google API. Additionally, this project, dubbed Pybox, as it is based on Python, includes other features listed below.\n2. Installation # The library is available on pypi so that you can install it with pip or pip3 by typing pip install pyboxes. I also have plan to publish this library on Conda so that you can install it with conda install pyboxes.\n3. Usage # $ pybox -h # show help Usage: pybox [options] \u0026lt;command\u0026gt; This tool include a bunch of useful commands: 1. Download single file or all files in a folder for Google Driver 2. Send message to Slack 3. more to come... Options: -h, --help Show this message and exit. Commands: asAyncdown Download files in terms of links asynchronously. gfile Download file in Google Driver. gfolder Download files in folders in Google Drive. slack Send message to Slack. Yangyang-Li https://yangyangli.top/ 2022 4. Features # A simple and easy to download files by sharing link A simple and easy to send message to Slack Channel Download multiple files asynchronously Download Books from Zlib in terms of Title Will come! I intend to continually expand the functionality of this toolbox by adding new commands as I discover their utility. Furthermore, I am committed to ensuring the quality and reliability of this toolbox. Please let me know if there i s specific command or feature you would like to see, and we can discuss it further.\n5. 🚌 Take a tour # A simple and easy to download files by sharing link # 1. Download single file by sharing link of Google Driver. # $ pybox gfile \u0026lt;url\u0026gt; \u0026lt;name\u0026gt; \u0026lt;size\u0026gt; 2. Download files in a folder by client id and folder id. # $ pybox gfolder \u0026lt;client_id\u0026gt; \u0026lt;folder_id\u0026gt; Detailed usage please see [Usage Documentation]\nA simple and easy to send message to Slack Channel # $ pybox slack [options] \u0026lt;webhook-url\u0026gt; Detailed usage please see [Usage Documentation]\nDownload multiple files asynchronously # 1. Download single file. # $ pybox asyncdown -u \u0026lt;url\u0026gt; -o \u0026lt;output\u0026gt; 2. Download multiple files. # $ pybox asyncdown -f \u0026lt;url-file\u0026gt; For example, suppose urls.txt in which the first column is the file name and the second column is the download url. pybox asyncdown -f urls.txt will download all files in parallel.\nENCFF888ZZV.fastq.gz https://www.encodeproject.org/files/ENCFF888ZZV/@@download/ENCFF888ZZV.fastq.gz ENCFF883SEZ.fastq.gz https://www.encodeproject.org/files/ENCFF883SEZ/@@download/ENCFF883SEZ.fastq.gz ENCFF035OMK.fastq.gz https://www.encodeproject.org/files/ENCFF035OMK/@@download/ENCFF035OMK.fastq.gz ENCFF288CVJ.fastq.gz https://www.encodeproject.org/files/ENCFF288CVJ/@@download/ENCFF288CVJ.fastq.gz 5. Contributing # If you would like to contribute to this toolbox, please feel free to fork it on GitHub. Please make sure you read the contributing guide before you start. Also, please make sure you pass all the tests before you pull request.\n","date":"3 December 2021","permalink":"/posts/001-a-wonderful-library-you-must-not-know/","section":"Posts","summary":"0.","title":"A Python Toolbox - Pybox"},{"content":"","date":"4 April 2021","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":" 1. Introduction # This article records two methods of PCA analysis using Python, and visualizes 2-dimensional results\n1.1 What\u0026rsquo;s PCA? # When it comes to methods of reducing dimension, PCA that is an unsupervised linear transformation technique, must be not ignored. Moreover, if you want to know the subtle relationships among data set and reduce the computational complexity in downstream analysis, the PCA may be your best choice! Meanwhile, if you would like to present your data in a 2-dimension or 3-dimension coordinate system, and PCA would sweep your problems!\nWhat is reducing dimension? I will show you an example as follows: First, suppose you have a five-dimensional data set :\nId 1-d 2-d 3-d 4-d 5-d data-1 1 2 3 4 5 data-2 6 7 8 9 10 .. .. .. .. .. Then, you could pick up PC1 and PC2 after PCA to reduce dimension for plotting:\nId PC1 PC2 data-1 0.3 0.6 data-2 0.1 1.2 .. .. .. PC1 and PC2 are the result obtained through data is projection on the unit vectors, which enable result to have the biggest variance(means its distribution is wide) and to be irrelevant(covariance = 0).\n1.2 Algorithm # Normalize \\(d\\) dimension raw data Create the covariance matrix Calculate the eigenvalues of the covariance matrix and the corresponding eigenvectors The eigenvectors are sorted in the matrix according to the corresponding feature value, and the first k rows are formed into a matrix \\(W\\). (\\(k\u0026lt;\u0026lt;d\\)) \\(Y = xW\\) is the result after reducing dimension to k dimension Note: There are two prerequisites for conducting PCA\nRaw data has no NA The raw data should be normalized 2. PCA from scratch # Importing necessary modules import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Creating raw data # get data set df_wine = pd.read_csv( \u0026#34;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\u0026#34;, header=None, engine=\u0026#34;python\u0026#34;, ) # check data df_wine.head() Creating train and test data set # create train and test data set X, y = df_wine.iloc[:, 1:], df_wine.iloc[:, 0] x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.3, stratify=y, random_state=0 ) Standardizing the features # create standard instance sc = StandardScaler() # standard data x_train_std = sc.fit_transform(x_train) x_test_std = sc.fit_transform(x_test) Creating the covariance matrix and Getting eigenvectors and eigenvalues the calculation of the covariance matrix :\n$$ \\sigma_{jk} = \\frac{1}{n} \\sum^{n}_{i=1}\\bigg(x_{j}^{(i)} - \\mu_j\\bigg)\\bigg(x_{k}^{(i)} - \\mu_k\\bigg) $$\nThen, using numpy.cov and numpy.linalg.eig to get the covariance matrix and eigenvectors respectively\n# calculate the covariance matrix cov_mat = np.cov(x_train_std.T) # Getting eigenvectors and eigenvalues eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) NOTE: there are 13 eigenvectors totally, the number of eigenvalues might be not as same as the number of features sometimes.\nFirstly, plotting the Variance interpretation ratio, which is obtained through eigenvalue \\(\\lambda_j\\) divided by the sum of all the eigenvalues:\n$$ \\frac{\\lambda*j}{\\sum^d_{j=1}\\lambda_j} $$\n# get sum of all the eigenvalues tot = sum(eigen_vals) # get variance interpretation ratio var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) Besides, plotting the result to get in-depth understanding:\nplt.figure() # create plot # create bar plot plt.bar( range(1, 14), var_exp, alpha=0.5, label=\u0026#34;individual explained variance\u0026#34;, ) # create step plot plt.step(range(1, 14), cum_var_exp, where=\u0026#34;mid\u0026#34;, label=\u0026#34;cumulative explained variance\u0026#34;) # add label plt.ylabel(\u0026#34;Explained variance ratio\u0026#34;) plt.xlabel(\u0026#34;Principal component index\u0026#34;) # add legend plt.legend(loc=\u0026#34;best\u0026#34;) # save picture plt.savefig(\u0026#34;pca_index.png\u0026#34;, format=\u0026#34;png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;, dpi=300) We can conclude that PC1 only takes account for about 40%. Furthermore, the sum of PC1 and PC2 have 60% variance.\nSelecting the first k values to form matrix \\(W\\) # integrate eigenvalues and eigenvectors eigen_paris = [ (np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals)) ] # sort according to eigenvalues eigen_paris.sort(key=lambda x: x[0], reverse=True) # pick up the first 2 eigenvalues w = np.hstack([eigen_paris[0][1][:, np.newaxis], eigen_paris[1][1][:, np.newaxis]]) # check matrix x w Transforming raw data # reduce dimension x_train_pca = x_train_std.dot(w) # check resulted data x_train_pca.shape (124, 2)\nThen plotting the result and putting the label in terms of original info, but keeping in mind PCA is unsupervised learning skill without labels\n# init colors and markers colors = [\u0026#34;r\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;g\u0026#34;] markers = [\u0026#34;s\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;o\u0026#34;] # plot scatter for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter( x_train_pca[y_train == l, 0], x_train_pca[y_train == l, 1], c=c, label=1, marker=m, ) # add label and legend plt.xlabel(\u0026#34;PC 1\u0026#34;) plt.ylabel(\u0026#34;PC 2\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(\u0026#34;distribution.png\u0026#34;, format=\u0026#34;png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;, dpi=300) 3. PCA by scikit-learn # we can conduct PCA easily by sklearn\nImporting modules from sklearn.decomposition import PCA from matplotlib.colors import ListedColormap from sklearn.linear_model import LogisticRegression Defining function of plot_decision_region def plot_dicision_regions(X, y, classifier, resolution=0.02): # init markers and colors markers = (\u0026#34;s\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;o\u0026#34;, \u0026#34;^\u0026#34;, \u0026#34;v\u0026#34;) colors = (\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;lightgreen\u0026#34;, \u0026#34;gray\u0026#34;, \u0026#34;cyan\u0026#34;) cmap = ListedColormap(colors[: len(np.unique(y))]) # create info for plot region x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid( np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution) ) # test classifier\u0026#39;s accurate z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) z = z.reshape(xx1.shape) # plot decision region plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap) # set x,y length plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot result for idx, cl in enumerate(np.unique(y)): plt.scatter( x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.6, color=cmap(idx), edgecolor=\u0026#34;black\u0026#34;, marker=markers[idx], label=cl, ) PCA by sklearn # create pca instance pca = PCA(n_components=2) # create classifier instance lr = LogisticRegression() # reduce dimension for data set x_train_pca = pca.fit_transform(x_train_std) x_test_pca = pca.transform(x_test_std) # classify x_train_pca lr.fit(x_train_pca, y_train) # plot decision region plot_dicision_regions(x_train_pca, y_train, classifier=lr) # add info plt.xlabel(\u0026#34;PC 1\u0026#34;) plt.ylabel(\u0026#34;PC 2\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.show() We can see that the classifier\u0026rsquo;s accurate is excellent according to actual labels\nTIPS:\nYou can set n_components = None, and the result would retain all principle components. Moreover, you could call explained_variance_ration_ to use variance explanation ratio.\n3. Summary # All the above are the main content, welcome everybody communicates with me! 🤠\nReference book : Python machine learning\n","date":"4 April 2021","permalink":"/posts/010-pca-by-python/","section":"Posts","summary":"1.","title":"PCA by Python"},{"content":"","date":"20 June 2019","permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision"},{"content":"","date":"20 June 2019","permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning"},{"content":" 1. 准备工作 # 使用Python实现需求，环境需求比较严苛，所需要的主要依赖：\nRequirements\ndlib 由 C++编写，提供了和机器学习、数值计算、图模型算法、图像处理等领域相关的一系列功能 face-recognition 已经经过训练成熟的识别人脸的工具 imutils 用来操作系统文件和文件夹 Opencv是用来操作视频流，并将视频流转换格式 1.1 环境搭建 # 通过 pip 进行安装，在使用 pip 进行安装时，应该注意切换到国内源进行下载，提高下载速度，下面分享一下当前国内的 pip 源：\n阿里云 https://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣 http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 通过 Anaconda 进行安装，但是需要注意的是在 Anaconda 的资源中并没有face-recognition，需要使用 pip 安装。在安装中可能遇到问题因此在此分享我的环境，分别在 Windows 和 Mac 环境下进行测试。\nMac 环境\nPython 3.8 dlib 19.20.0 cmake 3.18.0 face-recognition 1.3.0 imutils 0.5.3 Windows 环境\nPython 3.6 dlib 19.7.0 cmake 3.18.0 face-recognition 1.3.0 imutils 0.5.3 在 Windows 环境中安装dlib如果通过 pip 安装遇到问题，可以直接下载对应版本的 Whl文件，使用pip install dlib-19.7.0-cp36-cp36m-win_amd64.whl命令进行安装。\n2. 素材展示 # 环境搭建完毕后，该项目的目的是捕捉人物在视频中出现并记录相应时间。首先让我们熟悉一下该小项目的测试素材：\n目标人物的照片 存在目标人物的测试视频 2.1 目标人物的照片 # 1. 在本项目的需求中只存在一个目人物，照片数量越多越好，数量在 30-80 之间就可以基本实现误差度较低的识别精度，但是需要注意的是照片中应只存在一人，且表情应该尽量丰富，照片不能过于单一。\n2.2 测试视频 # 3. 实现流程 # 在视频中实现特定人物的人脸识别，需要主要两个步骤：\n对素材中照片进行编码，将每一张照片转换为一个含有 128 个元素的 1 维数组 识别视频中对人脸，转换为数组与目标数组进行比对，确定是否匹配，完成识别。 3.1 编码照片 # # 导入模块 from imutils import paths # 操作文件 import face_recognition # 识别人脸并编码 import pickle # 使用pickle文件形式储存节省空间 import cv2 # 操作视频流 # 获取照片 print(\u0026#34;[INFO] quantifying faces...\u0026#34;) imagePaths = list(paths.list_images(\u0026#34;./dataset\u0026#34;)) # dataset为储存照片文件夹 # 初始化储存所有照片编码 knownEncodings = [] # 遍历每张照片并编码 for (i, imagePath) in enumerate(imagePaths): print(\u0026#34;[INFO] processing image {}/{}\u0026#34;.format(i + 1, len(imagePaths))) # 加载图片，转换为RGB模式 image = cv2.imread(imagePath) rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 识别人脸并返回位置 boxes = face_recognition.face_locations(rgb, model=\u0026#34;hog\u0026#34;) # model 可以选择 cnn 或 hog # 编码照片并储存 encodings = face_recognition.face_encodings(rgb, boxes) knownEncodings.append(encoding) # 写入照片的编码 print(\u0026#34;[INFO] serializing encodings...\u0026#34;) data = {\u0026#34;encodings.pickle\u0026#34;: knownEncodings} with open(\u0026#34;encoding_face.pickle\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(pickle.dumps(data)) 在编码照片时要注意模式的选择：\n如果你有 GPU，可以选择卷积神经网络（CNN），因为要消耗大量的计算力。\n如果你和我一样使用 CPU，选择方向梯度直方图方法（HOG），大大地提高计算速度，处理 30 张照片大约消耗 3-5 分钟。\n将所有照片编码完成后，结果文件可以在后续不同视频中一直使用，如果要添加更多照片进行检测，那么应该重新进行编码。此外还可以采用免费的 GPU 使用例如 Google Colab和 Kaggle。\n3.2 视频识别 # 采用Opencv处理视频流，一帧一帧进行操作。\n在视频实时处理过程：\n在视频处理过程中需要注意的是三个全局参数的选择:\nTHRESHOLD = 0.5 比较视频中人脸和目标人脸是否相同，默认为 0.6，越小越严格 FILTER_STANDARD = 0.6 此阈值为视频中人脸匹配目标所有照片的比例，越大越严格 FRAME_STEP = 20 跳帧操作，加快视频处理时间，本视频中 1S 大约 24 帧，最好保证时间连续。 其次仍需注意编码过程中方法的选择，CNN 适合比较适合 GPU 计算。\n3.3 结果文件 # 将每一帧的处理结果写入文件当中，输出结果中有姓名和出现时间，后续可以对根据结果对视频进行处理。\n4. 总结 # 4.1 可以提高的地方 # 在整个实现过程中需要考虑对是硬件和软件的结合，最大程度地满足要达到的需求，在边学边做的过程当中，逐步摸索虽然初步解决了问题，但是还有很多可以改正和深挖的地方。\n首先本篇博客代码使用硬编码，这样好处是方便一些，但是不适合分享和复现，同时对文件结构有很高要求，我的文件结构如下：\n后续可以改为命令行获取参数，令代码更加灵活。其次可以修改代码使其能够同时处理多个视频。\n在全局参数的选择也存在些许问题，我并没有找到最适合的参数，可以从结果图片看出在识别的过程中存在错误，我认为有两种解决办法：\n增加目标编码照片数量，在实现当中使用大约 40 张照片，并且表情比较单一，如果后续可以丰富相应照片，就会避免错误的识别。 修改参数，可以结合最大似然估计选择合适的参数。 4.2 收获 # 在边学边做的过程中，了解视频与图片处理方法和概念。站在巨人的肩膀上，利用已经有的轮子实现。但是也不能沉浸于此，还是要增强统计相关的知识。\n最后，在此记录实现过程，便于后续参考。并且记录遇到问题以及解决办法。\n参考链接 # Pyimage ","date":"20 June 2019","permalink":"/posts/015-face-recognition-in-video/","section":"Posts","summary":"1.","title":"Face Recognition in Video"},{"content":" 1. Background # Before diving into code, the description of NJ algorithm can be found in , where first column indicates parent node, and second column is its children node, the last column is the value of edge.\n2. Neighbor Joining Algorithm # The Neighbor-joining Algorithm Given a distance matrix d compute an uprooted tree topology complete with edge lengths that tries to preserve the additive property: \\(d_{i,m} + d_{j,m} − d_{i,j} = 2d_{k,m}\\), where \\(k\\) is the k-th node on both routes from \\(i\\) and \\(j\\) to \\(m\\).\nLet the set of clusters be called \\(L\\) and initially \\(i → C_i; ∀i\\) that is \\(| C_i | = 1\\) and \\(L = C_1 , C_2, \\cdots, C_N\\). \\(d_{i,j}\\) is the distance from the initial distance matrix. Compute \u0026ldquo;normalized distance matrix\u0026rdquo; \\(D_{i,j}\\) for all \\(i, j\\) such that $$D_{i,j} = d_{i,j} − (r_i + r_j ) \\ where\\ r_i = \\frac{1}{|L| -2} \\sum_{z \\in L} d_{i,z}$$ This subtracts the average distance to all other nodes than the pair involved. Note: this is not where we use the distance identity. Use normalized distance to \\((i, j) = \\argmin D_{i,j} \\; C_i,C_j \\in L \\) Merge \\(C_i \\cup C_j \\rightarrow C_k\\) where \\(k\\) is a new cluster number. Mark old clusters as used so that effectively: \\(L \\leftarrow L − C_i − C_j\\) Compute a new normalized distance matrix including the new cluster \\(k\\) and excluding \\(i, j\\). \\(d_{k,z} = d_{z,k} = (d_{i,z} + d_{j,,z} − d_{i,j} ) \\; \\forall z ∈ L 2\\) This uses the additivity of the distances to compute the distance to the new cluster from each other node. Compute the length of the edges from \\(k\\) to \\(i\\) and \\(j\\). Even though \\(C_k\\)has assumed the role of both \\(C_i\\) and \\(C_j\\) you still need the edge length to \\(i\\) and \\(j\\) from $k$ in order to “draw” the tree. $$edge_{i,k} = (d_{i,j} + r_i − r_j)$$ $$edge_{j,k} = (d_{i,j} + r_j − r_i)$$ Define height \\(h_k = d_{i,j} /2\\) where \\(h_k\\) is the height of node that is the ancestor to all in \\(C_k\\). When drawing the tree $h_k$ is the height above the baseline (where all the leaves are). \\(L \\leftarrow L \\cup C_k\\) While there is more than two clusters left go to step 3 Join the remaining two clusters with: $$edge_{j,k} = d_{i,j} $$ Implementation Notes Consider this part of the computation: $$D_{i,j} = d_{i,j} − (r_i + r_j ) \\ where \\ r_i = \\frac{1}{|L|-2} \\sum_{z \\in L}d_{i,z}$$ The values of r*z can be computed once each time we want to compute matrix \\(D\\). This saves a vast amount of time. Furthermore, since \\(D_{i,j}\\) is only used to find the argmin of \\(D_{i,j}\\) we actually don’t have to save array \\(D\\); we need to find the argmin of it. So computing all the r and then combine the argmin step with the computation of \\(D_{i,j}\\)\n3. Implementation # I write code contained comments, and it is about 1000 lines that consumes me two whole days. Now let me show my code with rich comments. If you have any questions or recommendation, I am very glad to communicate with you! Please feel free to reach me.\n\u0026#34;\u0026#34;\u0026#34; This code integrate bootstrap, getting edge_file and tree_file USAGE: python -i fa_file -d out_distance_matrix -e out_edge -t out_tree -b number_bootstrap \u0026#34;\u0026#34;\u0026#34; # import needed library import argparse import logging import random import time from collections import defaultdict, namedtuple import pandas as pd class Timer: \u0026#34;\u0026#34;\u0026#34;construct Timer to show working time of tasks\u0026#34;\u0026#34;\u0026#34; def __init__(self, func=time.perf_counter): \u0026#34;\u0026#34;\u0026#34;init values Args: func : Defaults to time.perf_counter. \u0026#34;\u0026#34;\u0026#34; self.elapsed = 0.0 self._func = func self._start = None def start(self): \u0026#34;\u0026#34;\u0026#34;start a task Raises: RuntimeError:if task has started then raise error \u0026#34;\u0026#34;\u0026#34; if self._start is not None: raise RuntimeError(\u0026#34;Already started\u0026#34;) self._start = self._func() def stop(self): \u0026#34;\u0026#34;\u0026#34;end a task Raises: RuntimeErrorif task has not started then raise error \u0026#34;\u0026#34;\u0026#34; if self._start is None: raise RuntimeError(\u0026#34;Not started\u0026#34;) end = self._func() self.elapsed += end - self._start self._start = None def reset(self): \u0026#34;\u0026#34;\u0026#34;reset the working time\u0026#34;\u0026#34;\u0026#34; self.elapsed = 0 def running(self): \u0026#34;\u0026#34;\u0026#34;check if task is running Returns: bool \u0026#34;\u0026#34;\u0026#34; return self._start is not None def __enter__(self): \u0026#34;\u0026#34;\u0026#34;function used to address \u0026#39;with text\u0026#39;\u0026#34;\u0026#34;\u0026#34; self.start() return self def __exit__(self, *args): \u0026#34;\u0026#34;\u0026#34;function used to address \u0026#39;with text\u0026#39;\u0026#34;\u0026#34;\u0026#34; self.stop() class TreeNode: \u0026#34;\u0026#34;\u0026#34;class of tree node\u0026#34;\u0026#34;\u0026#34; def __init__(self, key=None): \u0026#34;\u0026#34;\u0026#34;init values of tree node Args: key (str, float, optional): node_label, edge. Defaults to None. self.index: node_index \u0026#34;\u0026#34;\u0026#34; self.left, self.right = None, None self.key = key self.parent, self.index = None, None self.LIST = [] def insert_left_children(self, left_object): \u0026#34;\u0026#34;\u0026#34;insert left children Args: left_object (str, float): node_label, edge \u0026#34;\u0026#34;\u0026#34; # create a new node temp = TreeNode(left_object) # if left children exist if self.left: temp.left = self.left self.left = temp # if left children does not exist else: self.left = temp def insert_right_children(self, right_object): \u0026#34;\u0026#34;\u0026#34;insert right children Args: right_object (str, float): node_label, edge \u0026#34;\u0026#34;\u0026#34; # create a new node temp = TreeNode(right_object) # if right children exist if self.right: temp.right = self.right self.right = temp # if right children does not exist else: self.right = temp def isRoot(self): \u0026#34;\u0026#34;\u0026#34;check if the node is Root node Returns: bool: if node is Root return True \u0026#34;\u0026#34;\u0026#34; handle = False if self.key0 == \u0026#34;root\u0026#34;: handle = True return handle def isLeaf(self): \u0026#34;\u0026#34;\u0026#34;check if the node is Leaf node Returns: bool: if the node is Leaf return True \u0026#34;\u0026#34;\u0026#34; handle = False if not self.left and not self.right: handle = True return handle def catch_leaf(self): \u0026#34;\u0026#34;\u0026#34;show the partition list of leafs below the node, and mainly used to debug Returns: list: a list contained all leaf below the node \u0026#34;\u0026#34;\u0026#34; LIST = [] # check if the node is leaf if self.isLeaf(): LIST.append(self.index) if self.left: LIST.extend(self.left.catch_leaf()) if self.right: LIST.extend(self.right.catch_leaf()) return LIST def postorder(self): \u0026#34;\u0026#34;\u0026#34;tree traversal with postorder, and mainly used to debug\u0026#34;\u0026#34;\u0026#34; if self.left: self.left.postorder() if self.right: self.right.postorder() print(self.key) def get_parser() -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;function used to parse parameters from command line and set log Returns: dict: return a dict stored parameters \u0026#34;\u0026#34;\u0026#34; # set logging logging.basicConfig( level=logging.DEBUG, format=\u0026#34;%(levelname)s:%(asctime)s:%(message)s\u0026#34; ) # set parser parser = argparse.ArgumentParser( prog=\u0026#34;PROG\u0026#34;, description=\u0026#34;Program designed to condcut NJ \u0026#34;, formatter_class=argparse.MetavarTypeHelpFormatter, ) # add parameter for input of fa file parser.add_argument( \u0026#34;-i\u0026#34;, \u0026#34;--input\u0026#34;, help=\u0026#34;The input file (fasta format)\u0026#34;, required=True, type=str ) # add parameter for output of edge file parser.add_argument( \u0026#34;-e\u0026#34;, \u0026#34;--edge\u0026#34;, help=\u0026#34;The output file of edges matrix. Default: edges.txt\u0026#34;, default=\u0026#34;edges.txt\u0026#34;, type=str, ) # add parameter for output of tree file parser.add_argument( \u0026#34;-t\u0026#34;, \u0026#34;--tree\u0026#34;, help=\u0026#34;The output file of newick tree. Default: tree.txt\u0026#34;, default=\u0026#34;tree.txt\u0026#34;, type=str, ) # add parameter for output of distance matrix parser.add_argument( \u0026#34;-d\u0026#34;, \u0026#34;--distance\u0026#34;, help=\u0026#34;The output file of distance matrix. Default: genetic_distance.txt\u0026#34;, default=\u0026#34;genetic_distance.txt\u0026#34;, type=str, ) # add parameter for number of bootstrap; Default: not conduct bootstrap parser.add_argument( \u0026#34;-b\u0026#34;, \u0026#34;--bootstrap\u0026#34;, help=\u0026#34;The number of bootstrap. Result will be stored in \u0026#39;bootstrap.txt\u0026#39;\u0026#34;, default=None, type=int, ) # get parameters args = parser.parse_args() return args ############################################## # CELL FOR GETTING GENETIC DISTANCE MATRIX ############################################## def read_input(input: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;read input of fa file and return a dict whose value is nametuple stored information Args: input (str): fasta file Returns: dict: seq_label: (seq,index ) \u0026#34;\u0026#34;\u0026#34; # create namedtuple to store information seqs, seq = {}, namedtuple(\u0026#34;info\u0026#34;, [\u0026#34;seq\u0026#34;, \u0026#34;index\u0026#34;]) # open file as a list content = open(input).readlines() # init index of sequence read_index = 1 # read seq into the dict for index, line in enumerate(content): # check header if line.startswith(\u0026#34;\u0026gt;\u0026#34;): seqs[line.strip(\u0026#34;\u0026gt;\\n\u0026#34;)] = seq(content[index + 1].strip(), read_index) read_index += 1 return seqs def genetic_distance(seq1: str, seq2: str) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;calculate genetic distance of seq1 and seqw Args: seq1 (str): sequences of seq1 seq2 (str): sequences of seq2 Returns: float: genetic distance \u0026#34;\u0026#34;\u0026#34; # get length of sequences length = len(seq1) # init mismatch mismatch = 0 # begin to calculate for s1, s2 in zip(seq1, seq2): if s1 != s2: mismatch += 1 return mismatch / length def fetch_distance(seq1_index: str, seq2_index: str, matrix: dict) -\u0026gt; float or None: \u0026#34;\u0026#34;\u0026#34;fetch distance from distance matrix in case comupute pair distance repeatedly Args: seq1_index (str): seq1 label seq2_index (str): seq2 label matrix (dict): store distances that is calculated Returns: float or None: if distance has calculated return value otherwise None \u0026#34;\u0026#34;\u0026#34; # init value distance = None # check if the distance is already calculated if (seq1_index, seq2_index) in matrix: distance = matrix[(seq1_index, seq2_index)] elif (seq2_index, seq1_index) in matrix: distance = matrix[(seq2_index, seq1_index)] return distance def get_distance_matrix(seqs: dict, out: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;write distance matrix to file Args: seqs (dict): store information of seqs like {seq_label: (seq:, index:)} out (str): the name of genetic distance file Returns: dict: {(seq1_label,seq2_label):genetic_distance} \u0026#34;\u0026#34;\u0026#34; # open new file out_put = open(out, \u0026#34;w\u0026#34;) # init dict distance_map = {} # write header header = \u0026#34;\\t\u0026#34;.join(seqs.keys()) out_put.write(f\u0026#34;\\t{header}\\n\u0026#34;) # begin to calculate distance and write to file for seq1_label, seq1_info in seqs.items(): # init list to store distance of every line line = [] for seq2_label, seq2_info in seqs.items(): # check if the distance is already computed distance = fetch_distance(seq1_label, seq2_label, distance_map) if not distance: # if not exist then calculate it and store distance = genetic_distance(seq1_info.seq, seq2_info.seq) distance_map[(seq1_label, seq2_label)] = distance line.append(distance) # write every line line = \u0026#34;\\t\u0026#34;.join([str(element) for element in line]) out_put.write(f\u0026#34;{seq1_label}\\t{line}\\n\u0026#34;) out_put.close() return distance_map ############################################## # NJ ALGORITHM ############################################## def cacl_sum_distance(taxa: str, leaf_set: set, distance_matrix: dict) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;calculate sum distance of node away from other nodes Args: taxa (str): label for one node leaf_set (set): a set store current nodes needed to be merged distance_matrix (dict): {(seq1_label,seq2_label):genetic_distance} Returns: float: sum distance of node away from others \u0026#34;\u0026#34;\u0026#34; # init value result = 0 # calculate sum distance for other_taxa in leaf_set - set([taxa]): # fectch genetic distance result += fetch_distance(taxa, other_taxa, distance_matrix) return result def find_qmin(leaf_set: set, distance_matrix: dict) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;calculate min q_value and get information Args: leaf_set (set): current nodes needed to be merged distance_matrix (dict): dict stored genetic distance of current nodes Returns: dict: value, combination,taxa1_sum, taxa2_sum \u0026#34;\u0026#34;\u0026#34; # init dict qmin = { \u0026#34;combination\u0026#34;: None, # combination of node for merging \u0026#34;q_value\u0026#34;: None, # min Q value \u0026#34;taxa1_sum\u0026#34;: None, # sum distance of node away from others \u0026#34;taxa2_sum\u0026#34;: None, } # calculate Q value every combination of nodes for combination, distance in distance_matrix.items(): taxa1, taxa2 = combination # calculate sum distance of node away from others taxa1_sum, taxa2_sum = cacl_sum_distance( taxa1, leaf_set, distance_matrix ), cacl_sum_distance(taxa2, leaf_set, distance_matrix) # get Q value value = (len(leaf_set) - 2) * distance - taxa1_sum - taxa2_sum # compare Q value if not qmin[\u0026#34;q_value\u0026#34;]: # if Q value does not exist then store related information ( qmin[\u0026#34;q_value\u0026#34;], qmin[\u0026#34;combination\u0026#34;], qmin[\u0026#34;taxa1_sum\u0026#34;], qmin[\u0026#34;taxa2_sum\u0026#34;], ) = (value, combination, taxa1_sum, taxa2_sum) # if Q value exist then compare to choose minimum value else: if value \u0026lt; qmin[\u0026#34;q_value\u0026#34;]: ( qmin[\u0026#34;q_value\u0026#34;], qmin[\u0026#34;combination\u0026#34;], qmin[\u0026#34;taxa1_sum\u0026#34;], qmin[\u0026#34;taxa2_sum\u0026#34;], ) = (value, combination, taxa1_sum, taxa2_sum) return qmin def help_nj(leaf_set: set, distance_matrix: dict): \u0026#34;\u0026#34;\u0026#34;helper function to conduct neighbor joining Args: leaf_set (set): current nodes needed to be merged distance_matrix (dict): store pair distance of current nodes Returns: TREEDICT: a dict stored node that has been merged in binary tree format final_leaf_set: final twos node after finishing NJ final_matrix: genetic distances of final two nodes \u0026#34;\u0026#34;\u0026#34; # init dict to store nodes that has been merged in format of Binary Tree TREEDICT = {} def neighbor_joining(leaf_set: set, distance_matrix: dict): \u0026#34;\u0026#34;\u0026#34;implementation of neighbors joining Args: leaf_set (set): current nodes needed to be merged distance_matrix (dict): store genetic distance of current nodes \u0026#34;\u0026#34;\u0026#34; # get nonlocal value nonlocal TREEDICT # the rule to end the recursion if len(leaf_set) \u0026lt; 3: return (leaf_set, distance_matrix) # enter recursion else: # find min Q value qmin_info = find_qmin(leaf_set, distance_matrix) # get nodes needed to be merged taxa1, taxa2 = qmin_info[\u0026#34;combination\u0026#34;] # edge from taxa1 to new node edge_taxa1_node = 0.5 * distance_matrix[qmin_info[\u0026#34;combination\u0026#34;]] + ( qmin_info[\u0026#34;taxa1_sum\u0026#34;] - qmin_info[\u0026#34;taxa2_sum\u0026#34;] ) / (2 * (len(leaf_set) - 2)) # edge from taxa1 to new node edge_taxa2_node = ( distance_matrix[qmin_info[\u0026#34;combination\u0026#34;]] - edge_taxa1_node ) # init dict to update genetic matrix matrix = {} # create label for new node new_node = f\u0026#34;{taxa1}_{taxa2}\u0026#34; # calculate distance from new node to other nodes for node in leaf_set - set(qmin_info[\u0026#34;combination\u0026#34;]): # store in new genetic matrix with information of old genetic matrix matrix[(new_node, node)] = ( fetch_distance(taxa1, node, distance_matrix) + fetch_distance(taxa2, node, distance_matrix) - distance_matrix[(taxa1, taxa2)] ) * 0.5 # store genetic distances of other nodes in new genetic matrix for combination, distance in distance_matrix.items(): if (taxa1 not in combination) and (taxa2 not in combination): matrix[combination] = distance # store phylogenetic nodes that has been merged in Binary Tree format # create a tree node that linked two merged nodes in Binary Tree # if one of merged nodes has been in Binary Tree then add edges # taxa1 is always regarded as left children if taxa1 in TREEDICT: left = TREEDICT.pop(taxa1) left.key[1] = edge_taxa1_node # if one of merged nodes is not stored in Binary Tree else: # create a Tree node and store its label and edge left = TreeNode([taxa1, edge_taxa1_node]) # taxa2 is same with taxa1 # taxa2 is always regarded as right children if taxa2 in TREEDICT: right = TREEDICT.pop(taxa2) right.key[1] = edge_taxa2_node else: right = TreeNode([taxa2, edge_taxa2_node]) # create a new_node linked two merged nodes in Binary Tree # the new_node has no edge internal_node = TreeNode([new_node, None]) # add paraent information for two merged nodes left.parent, right.parent = internal_node, internal_node # link two merged nodes as children in Binary Tree internal_node.left, internal_node.right = left, right # add new node to dict TREEDICT[new_node] = internal_node # update genetic distance matrix distance_matrix = matrix # update current phylogenetic nodes needed to be merged leaf_set -= set(qmin_info[\u0026#34;combination\u0026#34;]) leaf_set.add(new_node) return neighbor_joining(leaf_set, distance_matrix) # begin to compute until two phylogenetic nodes left final_leaf_set, final_matrix = neighbor_joining(leaf_set, distance_matrix) return TREEDICT, final_leaf_set, final_matrix ############################################## # PREORDER TREE AND ADD INDEX ############################################## def my_tree(TREEDICT: dict, final_distance_matrix: dict, seqs: dict): \u0026#34;\u0026#34;\u0026#34;function to create Binary tree but it has fake root because phylogenetic node has three children hence create a fake node as root to get a Binary Tree all following behaviors related Binary Tree usually consider this situation for binary_tree.key: key[0] -\u0026gt; label ; key[1] -\u0026gt; edge Args: TREEDICT (dict): store information of binary tree final_distance_matrix (dict): distance of final two nodes seqs (dict): store information of seqs Returns: binary tree: binary tree with fake root \u0026#34;\u0026#34;\u0026#34; # \u0026#39;merged node\u0026#39; means nodes have been changed as binary tree # init value to find if node is \u0026#39;merged node\u0026#39; in final two nodes after NJ last_node_label = None last_node_edges = None for index, value in final_distance_matrix.items(): # get edge last_node_edges = value # check if node is \u0026#39;merged node\u0026#39; for label in index: # if not if label in seqs: # get label last_node_label = label # create a fake root with label \u0026#39;root\u0026#39; fake_root = TreeNode((\u0026#34;root\u0026#34;, \u0026#34; \u0026#34;)) # get node label of final two nodes after NJ current_node_index = list(TREEDICT.keys()) # if final two nodes are merged node if not last_node_label: # add last edge to right children of fake root # because the left children of fake root is \u0026#34;TRUE root\u0026#34;, which has three childerns # the right children of fake root is linked to left children of it in real # so the edge indicates the distance between left children and right children # even though right children is linked to fake root in binary_tree TREEDICT[current_node_index[1]].key[1] = last_node_edges # create tree structure fake_root.left = TREEDICT[current_node_index[0]] fake_root.right = TREEDICT[current_node_index[1]] # if one of final two nodes is merged node else: # set node that is not merged node as tree structure right_node = TreeNode([last_node_label, last_node_edges]) # set one of final two nodes, a merged node, as left children of fake root fake_root.left = TREEDICT[current_node_index[0]] # add right children fake_root.right = right_node return fake_root def help_edge_matrix(tree, seqs: dict, edge_file: str): \u0026#34;\u0026#34;\u0026#34;help function to do preorder traversal in order to get edge file Args: tree (Binary Tree) seqs (dict): store information about label, seq, and index of seqs edge_file (str): output of edge Returns: Binary Tree: Binary Tree whose internal node has index \u0026#34;\u0026#34;\u0026#34; # init index for internal node N = len(seqs) + 1 # open new file EDGE_FILE = open(edge_file, \u0026#34;w\u0026#34;) # changes def edge_matrix(tree): # get nonlocal value nonlocal N # preorder tree if tree: # index of fake root is None if tree.isRoot(): tree.index = None # tree.key[0] is label of node # if it not in seqs then it is internal node elif tree.key[0] not in seqs: # set index for internal node tree.index = N N += 1 else: # get index of leaf node tree.index = seqs[tree.key[0]].index # write information of parent and children # left and right children of fake root have no parent : None if tree.parent: EDGE_FILE.write(f\u0026#34;{tree.parent.index}\\t{tree.index}\\t{tree.key[1]}\\n\u0026#34;) edge_matrix(tree.left) edge_matrix(tree.right) edge_matrix(tree) # add the TRUE ROOT and its children EDGE_FILE.write(f\u0026#34;{tree.left.index}\\t{tree.right.index}\\t{tree.right.key[1]}\\n\u0026#34;) EDGE_FILE.close() return tree ############################################## # POSTORDER TREE # WARNING: MUST CONDUCT AFTER PREODER ############################################## def help_newick_parse(tree, tree_file: str): \u0026#34;\u0026#34;\u0026#34;help function to do postorder traversal in order to get tree file Args: tree (Binary Tree) tree_file (str): output of tree \u0026#34;\u0026#34;\u0026#34; # open tree file TREE_FILE = open(tree_file, \u0026#34;w\u0026#34;) # get true root label (left children of fake root) true_root_label = tree.left.key[0] def newick_parse(tree): # get nonlocal value nonlocal true_root_label # postorder tree if tree: left = newick_parse(tree.left) right = newick_parse(tree.right) # if node is internal node if left and right: # check if node is true node if tree.key[0] == true_root_label: # not add \u0026#39;( )\u0026#39; so as to have three children finally return f\u0026#34;{left},{right}\u0026#34; # check if node is root elif tree.isRoot(): # add \u0026#39;()\u0026#39; finally return f\u0026#34;({left},{right})\u0026#34; # if node is internal node else: return f\u0026#34;({left},{right}):{tree.key[1]}\u0026#34; # if node is leaf node else: return f\u0026#34;{tree.key[0]}:{tree.key[1]}\u0026#34; newick = newick_parse(tree) TREE_FILE.write(f\u0026#34;{newick};\u0026#34;) TREE_FILE.close() ############################################## # BOOTSTRAP ############################################## def bootstrap(original_seqs: dict): \u0026#34;\u0026#34;\u0026#34;bootstrap original seqs and return bootstrap sample and bootstrap seqs Args: original_seqs (dict): original seqs Returns: tree_root_with_node_index: bootstrap sample whose node has index seqs_bootstrap : bootstrap seqs contained information like index, label, and sequences \u0026#34;\u0026#34;\u0026#34; # change data structure in order to use pandas: # 0 1 2 (column_number) # label 1 # label 2 # ... seqs_pd = {index: list(value.seq) for index, value in original_seqs.items()} df = pd.DataFrame(seqs_pd).T # using random to resample df_bootstrap = df.loc[:, random.choices(range(df.shape[1]), k=df.shape[1])] # get bootstrap information seqs_bootstrap = original_seqs.copy() # change sequences to bootstrap sequences for index, value in df_bootstrap.iterrows(): seqs_bootstrap[index] = seqs_bootstrap[index]._replace( seq=\u0026#34;\u0026#34;.join(value.values) ) # get nodes needed to be coudcted NJ leaf_set = set(seqs_bootstrap.keys()) # calculate genetic_distance for bootstrap samples distance_map = {} for seq1_label, seq1_info in seqs_bootstrap.items(): for seq2_label, seq2_info in seqs_bootstrap.items(): if seq1_label != seq2_label: distance = fetch_distance(seq1_label, seq2_label, distance_map) if not distance: distance = genetic_distance(seq1_info.seq, seq2_info.seq) distance_map[(seq1_label, seq2_label)] = distance # conduct NJ TREEDICT, _, final_distance_matrix = help_nj(leaf_set, distance_map) # construct binary_tree # get edge file for last bootstrap (NOT USED) tree_root = my_tree(TREEDICT, final_distance_matrix, seqs_bootstrap) tree_root_with_node_index = help_edge_matrix( tree_root, seqs_bootstrap, \u0026#34;last_bootstrap.edge\u0026#34; ) return tree_root_with_node_index, seqs_bootstrap def help_find_partion(tree) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;function to partition list for internal nodes in a binary tree Args: tree (binary_tree): binary_tree with fake root Returns: dict: node_index: partition list \u0026#34;\u0026#34;\u0026#34; # init dict to store index and its partition list internal_node_partion = defaultdict(list) # postorder traversal to find leafs for every internal node def find_partion(tree): LIST = [] # begin if tree: left = find_partion(tree.left) right = find_partion(tree.right) if tree.isLeaf(): LIST.append(tree.index) else: # flatten list LIST.extend([*left, *right]) internal_node_partion[tree.index].extend(sorted([*left, *right])) return LIST # change partition list of fake root to true root find_partion(tree) internal_node_partion[tree.left.index] = internal_node_partion.pop(tree.index) return internal_node_partion def get_bootstrap_value( bootstrap_number: int, original_seqs: dict, original_tree, bootstrap_file: str = \u0026#34;bootstrap.txt\u0026#34;, ): \u0026#34;\u0026#34;\u0026#34;function to integrate relation methods to get bootstrap values for original tree Args: bootstrap_number (int): number for bootstrap. default = None original_seqs (dict): information of original sequences original_tree (type): binary tree of original sequences bootstrap_file (str): output of bootstrap \u0026#34;\u0026#34;\u0026#34; # init list to store bootstrap value for every internal node internal_node_bootstrap_value = [0] * (len(original_seqs) - 2) # get partition list of every internal node in original tree original_partion_dict = help_find_partion(original_tree) # init list to store dict contained bootstrap_partion and its node_index bootstrap_partion_list = [] # begin to bootstrap and construct binary tree for number in range(bootstrap_number): logging.info(f\u0026#34;BOOTSTRAP NUMBER : {number+1}\u0026#34;) bootstrap_tree, _ = bootstrap(original_seqs) bootstrap_partion_list.append(help_find_partion(bootstrap_tree)) # compare partition list to update bootstrap value for internal_node_index, partion_conent in original_partion_dict.items(): for bootstrap_partion in bootstrap_partion_list: if partion_conent in bootstrap_partion.values(): internal_node_bootstrap_value[ internal_node_index - len(original_seqs) - 1 ] += 1 # change format for the bootstrap values result = [f\u0026#34;{value/bootstrap_number}\\n\u0026#34; for value in internal_node_bootstrap_value] # write to file with open(bootstrap_file, \u0026#34;w\u0026#34;) as file: file.writelines(result) ############################################## # MAIN ############################################## def worker(args: dict): \u0026#34;\u0026#34;\u0026#34;function to integrate all above method to conduct NJ and output tree and edge file Args: args (dict): parameters getting from command line Returns: tree_root_with_index : binary tree whose node has index seq : information of sequences \u0026#34;\u0026#34;\u0026#34; # read fa file and parse it seqs = read_input(args.input) # get labels of sequences leaf_set = set(seqs.keys()) # get pair genetic distance distance_matrix = get_distance_matrix(seqs, args.distance) # delete distances between same sequences distance_matrix = { item: value for item, value in distance_matrix.items() if value != 0 } # get TREEDICT and distance of final two nodes TREEDICT, _, final_distance_matrix = help_nj(leaf_set, distance_matrix) # construct binary tree REMEMBER it has fake root # left children of fake root is regarded as true root tree_root = my_tree(TREEDICT, final_distance_matrix, seqs) # get edge file and add index for every node in binary tree tree_root_with_index = help_edge_matrix(tree_root, seqs, args.edge) # get tree file REMEMBER it must conduct after getting edge files # because it need index of node help_newick_parse(tree_root_with_index, args.tree) return tree_root_with_index, seqs def main(): args = get_parser() with Timer() as t: logging.info(f\u0026#34;Neighbor Joining begin.......\u0026#34;) tree_root_with_index, seqs = worker(args) logging.info(f\u0026#34;Neighbor Joining DONE : TIME {t.elapsed:.2f}s\\n\u0026#34;) if args.bootstrap: with Timer() as t: get_bootstrap_value(args.bootstrap, seqs, tree_root_with_index) logging.info(f\u0026#34;BOOTSTRAP IS DONE : TIME {t.elapsed:.2f}s\\n\u0026#34;) else: logging.info(f\u0026#34;BOOTSTRAP IS NOT DONE\u0026#34;) logging.info(f\u0026#34;EDGE_FILE : {args.edge}\u0026#34;) logging.info(f\u0026#34;TREE_FILE : {args.tree}\u0026#34;) logging.info(f\u0026#34;DISTANCE_FILE: {args.distance}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Thanks for your reading! Hopefully helpful!\n","date":"3 April 2019","permalink":"/posts/008-nei-saitou-neighbor-joining/","section":"Posts","summary":"1.","title":"Nei Saitou Neighbor Joining"},{"content":"Hello and welcome! I am Yangyang Li, a Ph.D. candidate in the DGP Program at Northwestern University Feinberg School of Medicine. I specialize in developing algorithms and deep learning methods to tackle challenging biological problems.\nFurthermore, I have a fervent passion for harnessing the power of programming languages, such as C++, Rust, and Python, to tackle intricate challenges in Computational Biology. My journey as a Ph.D. student has afforded me the privilege to engage in pioneering research projects, each of which stretches the limits of our current understanding and holds the promise to effect genuine global impact.\nBeyond my academic pursuits, I have a keen interest in keeping a finger on the pulse of the latest tech industry advancements. I\u0026rsquo;m always on the lookout to learn novel programming languages and refine my techniques. When the rigors of academia don\u0026rsquo;t have me occupied, I\u0026rsquo;m either immersing myself in fresh side projects or embracing the serenity of the outdoors.\nI conceived this website as a platform to relay my experiences, insights, and invaluable resources to fellow enthusiasts in the programming and deep learning realms. It\u0026rsquo;s my earnest wish that you derive inspiration, knowledge, or even a hint of curiosity from my shared content!\nA cup of coffee will support me to insist sharing 😄\n","date":"1 January 0001","permalink":"/about/","section":"Welcome to my blog!","summary":"Hello and welcome!","title":"About"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":" The pulse of the present, echoing in the halls of Bytes of Life. Run Mag Miles AWESOME Release PxBLAT 🚀 2023-06-15 We are happy to release the Python package PxBLAT. There is no intermediate file, no system call, and no format conversion. Instead, there are straightforward and user-friendly APIs, performance boosts, and adequate documentation. Issues and contributions are welcome! https://t.co/fAtgAd7V2b\n\u0026mdash; yangli (@yangyangliz5) August 5, 2023 ylab-hi/pxblat PxBLAT: An Efficient and Ergonomics Python Binding Library for BLAT C 5 0 Become Ph.D. Candidate 🎉 2023-06-15 Become candidate. Next step: the Ph.D. Apply $2500 in OpenAI API successfully. Let\u0026#39;s make something cool🦀! pic.twitter.com/UmIIKCydxw\n\u0026mdash; yangli (@yangyangliz5) June 16, 2023 Join Biological Data Science Conference 🛫 2022-11-09 Continuing Reading Coauthor Paper is Published 🎉 2022-10-31 Continuing Reading Transfer to Northwestern University 🎉 2022-06-01 Northwestern University Become Ph.D. Student 🎉 2020-09-01 University of Minnesota Master Graduation 🎉 2020-06-01 China Agricultural University ","date":"1 January 0001","permalink":"/news/","section":"Welcome to my blog!","summary":"The pulse of the present, echoing in the halls of Bytes of Life.","title":"Recent News 🚀"}]