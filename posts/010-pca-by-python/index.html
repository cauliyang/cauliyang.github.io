<!doctype html><html lang=en><head><script>location.host!=new URL("https://cauliyang.github.io/").host&&(location.href="https://cauliyang.github.io/")</script><meta name=viewport content="width=device-width"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=7"><link rel=icon href=/favicon.png><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon href=/apple-touch-icon.png><link rel=icon href=/logo.svg type=image/svg+xml><title>PCA by Python &ndash;
Yangyang Li's Geek Logs</title><link href=/symbols-nerd-font/symbols-nerd-font.css rel=stylesheet integrity="sha512-lydow8GLOLlYNOtHlksNCmGWWCBsbIEtikXpHzfWqx78HLlyQZHOzyLwPpKol4Th6aCwLUXOfODVYgwrd3nwKQ=="><link href=/jetbrains-mono/jetbrains-mono.css rel=stylesheet integrity="sha512-tJxlgL6v1Y7kFf+qB8SloaAMKnOAw6WouknxXtIjkBux9Y/9aX81EUWOJO8c/3l98DmjG8brr4to7zaez606Fg=="><link rel=stylesheet href=https://latest.cactus.chat/style.css type=text/css><link type=text/css rel=stylesheet href=https://cauliyang.github.io/css/styles.f13b26608bee58be5a71a28a962268940cf92222048d1ee76e364329803107c0986170f96b89f0c24d03be85521b7514af6163a713f29d2e7b450c8933e9edfb.css integrity="sha512-8TsmYIvuWL5acaKKliJolAz5IiIEjR7nbjZDKYAxB8CYYXD5a4nwwk0DvoVSG3UUr2FjpxPynS57RQyJM+nt+w=="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta name=author content="Yangyang Li"><meta name=keywords content="Machine Learning,Python"><meta name=description content="This article records two methods of PCA analysis using Python, and visualizes 2-dimensional results."><meta property="og:site_name" content="Yangyang Li's Geek Logs"><meta property="og:title" content="PCA by Python"><meta property="og:type" content="article"><meta property="article:author" content="Yangyang Li"><meta property="article:published_time" content="2021-04-04T23:58:37Z+0800"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Python"><meta property="og:url" content="https://cauliyang.github.io/posts/010-pca-by-python/"><meta property="og:image" content="https://cauliyang.github.io/icon512.png"><meta property="og:description" content="This article records two methods of PCA analysis using Python, and visualizes 2-dimensional results."><meta name=twitter:card content="summary_large_image"><meta property="twitter:domain" content="cauliyang.github.io"><meta property="twitter:url" content="https://cauliyang.github.io/posts/010-pca-by-python/"><meta name=twitter:title content="PCA by Python"><meta name=twitter:image content="https://cauliyang.github.io/icon512.png"><meta name=twitter:description content="This article records two methods of PCA analysis using Python, and visualizes 2-dimensional results."><link rel=manifest href=/manifest/index.json></head><body><div id=baseContainer><header><div class=titleAndSearchContainer><div id=titleContainer><a class=unstyledLink href=/><img src=/logo.svg alt=Logo></a><div class=rightOfLogo><div class=titleAndHamburger><h1><a class=unstyledLink href=/>Yangyang Li's Geek Logs</a></h1><label id=hamburger-menu for=main-nav-toggler>&#xf85b;</label></div><div id=wide_nav><nav><input type=checkbox id=main-nav-toggler><ul id=main-nav><li><a href=/>Home</a></li><li><a href=/posts>Posts</a></li><li><a href=https://cauliyang.github.io/pages/about/>About</a></li><li><a href=/resume.pdf>CV</a></li><li><a href=/tags>Tags</a></li></ul></nav></div></div></div><div class=search><input id=searchbar type=text placeholder=Search>
<a class=nerdlink onclick=newSearch()>&#xf002;</a></div><script>function newSearch(){let e=searchbar.value.trim();if(!e)return;location.href=`/search?q=${e}`}searchbar.onkeyup=e=>{e.keyCode==13&&newSearch()}</script></div><div id=links><a rel=noreferrer target=_blank class=nerdlink href=/index.xml>&#xf09e;
<span>RSS</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://github.com/cauliyang>&#xf09b;
<span>GitHub</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://twitter.com/yangyangliz5>&#xf099;
<span>Twitter</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=mailto:yangyang.li@norwestern.edu>&#xf6ed;
<span>Email</span></a>
<a rel=noreferrer target=_blank class=nerdlink href="https://scholar.google.com/citations?user=ByWsRVMAAAAJ&hl=en"><svg class="pseudofont" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 27 27"><defs><clipPath id="id1"><path d="M3.386719 3H27.339844V28H3.386719zm0 0" clip-rule="nonzero" fill="#fff"/></clipPath></defs><g clip-path="url(#id1)"><path fill="#fff" d="M14.660156 3.25 3.386719 10.308594H11.125C11.097656 10.417969 11.050781 10.515625 11.027344 10.625 10.960938 10.964844 10.910156 11.34375 10.910156 11.734375c0 5.039063 5.144532 4.472656 5.144532 4.472656v1.285157C16.054688 18.011719 16.734375 17.832031 16.816406 18.890625 16.476562 18.890625 9.691406 18.695312 9.691406 23.277344c0 4.605468 5.988282 4.378906 5.988282 4.378906S22.59375 27.964844 22.59375 22.273438C22.597656 18.871094 18.636719 17.765625 18.636719 16.398438c0-1.382813 2.984375-1.789063 2.984375-5.023438C21.621094 9.960938 21.523438 8.953125 20.890625 8.238281 20.84375 8.1875 20.808594 8.152344 20.761719 8.121094 20.75 8.109375 20.738281 8.101562 20.726562 8.09375H20.898438l2.917968-2.191406V8.898438C23.816406 8.953125 23.820312 9.007812 23.832031 9.0625 23.609375 9.1875 23.429688 9.363281 23.300781 9.585938 23.171875 9.808594 23.109375 10.050781 23.113281 10.308594v1.414062C23.109375 11.910156 23.144531 12.09375 23.214844 12.269531 23.285156 12.445312 23.386719 12.597656 23.515625 12.734375 23.648438 12.867188 23.804688 12.972656 23.976562 13.042969 24.152344 13.117188 24.332031 13.152344 24.519531 13.152344 24.710938 13.152344 24.890625 13.117188 25.066406 13.042969 25.238281 12.972656 25.390625 12.867188 25.523438 12.734375 25.65625 12.597656 25.757812 12.445312 25.828125 12.269531 25.898438 12.09375 25.933594 11.910156 25.929688 11.722656V10.308594C25.933594 10.050781 25.871094 9.808594 25.742188 9.585938 25.613281 9.363281 25.433594 9.1875 25.207031 9.0625 25.21875 9.007812 25.226562 8.953125 25.226562 8.898438V4.839844L27.339844 3.25zM15.632812 7.5625C16.039062 7.542969 16.445312 7.640625 16.835938 7.863281 17.125 8.007812 17.402344 8.21875 17.644531 8.480469 18.148438 8.984375 18.570312 9.714844 18.796875 10.578125 19.332031 12.625 18.636719 14.597656 17.191406 14.96875 15.765625 15.375 14.171875 14.039062 13.621094 12.007812 13.378906 11.015625 13.410156 10.054688 13.6875 9.292969 13.691406 9.28125 13.695312 9.273438 13.699219 9.265625 13.703125 9.261719 13.710938 9.257812 13.714844 9.253906 13.792969 8.953125 13.921875 8.679688 14.082031 8.457031 14.371094 8.035156 14.753906 7.746094 15.226562 7.617188 15.363281 7.585938 15.496094 7.566406 15.632812 7.5625zM16.183594 19.75C18.566406 19.570312 20.597656 20.886719 20.746094 22.675781 20.84375 24.449219 19.007812 26.027344 16.605469 26.1875 14.222656 26.351562 12.160156 25.050781 12.046875 23.277344 11.933594 21.492188 13.78125 19.929688 16.183594 19.75zm0 0" fill-opacity="1" fill-rule="nonzero"/></g></svg><span>Google Scholar</span></a></div></header><div id=contentContainer><div id=content><main><article class="card single"><h1>PCA by Python</h1><p class=date><span title=Date>ï—¬</span>
2021-04-04</p><div class=articleToc><nav id=TableOfContents><ul><li><ul><li><a href=#1-introduction>1. Introduction</a><ul><li><a href=#11-whats-pca>1.1 What&rsquo;s PCA?</a></li></ul></li><li><a href=#12-algorithm>1.2 Algorithm</a></li><li><a href=#2-pca-from-scratch>2. PCA from scratch</a></li><li><a href=#3-pca-by-scikit-learn>3. PCA by scikit-learn</a></li><li><a href=#3-summary>3. Summary</a></li></ul></li></ul></nav><hr></div><div><h2 id=1-introduction>1. Introduction</h2><h3 id=11-whats-pca>1.1 What&rsquo;s PCA?</h3><p>When it comes to methods of reducing dimension, PCA that is an unsupervised linear transformation technique, must be not ignored.
Moreover, if you want to know the subtle relationships among data set and reduce the computational complexity in downstream analysis, the PCA may be your best choice!
Meanwhile, if you would like to present your data in a 2-dimension or 3-dimension coordinate system, and PCA would sweep your problems!</p><p>What is reducing dimension? I will show you an example as follows:
First, suppose you have a five-dimensional data set :</p><table><thead><tr><th>Id</th><th>1-d</th><th>2-d</th><th>3-d</th><th>4-d</th><th>5-d</th></tr></thead><tbody><tr><td>data-1</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>data-2</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>..</td><td>..</td><td>..</td><td>..</td><td>..</td><td></td></tr></tbody></table><p>Then, you could pick up PC1 and PC2 after PCA to reduce dimension for plotting:</p><table><thead><tr><th>Id</th><th>PC1</th><th>PC2</th></tr></thead><tbody><tr><td>data-1</td><td>0.3</td><td>0.6</td></tr><tr><td>data-2</td><td>0.1</td><td>1.2</td></tr><tr><td>..</td><td>..</td><td>..</td></tr></tbody></table><p><strong>PC1</strong> and <strong>PC2</strong> are the result obtained through data is projection on the unit vectors, which enable result to have the biggest variance(means its distribution is wide) and to be irrelevant(covariance = 0).</p><h2 id=12-algorithm>1.2 Algorithm</h2><ol><li>Normalize $d$ dimension raw data</li><li>Create the covariance matrix</li><li>Calculate the eigenvalues of the covariance matrix and the corresponding eigenvectors</li><li>The eigenvectors are sorted in the matrix according to the corresponding feature value, and the first k rows are formed into a matrix $W$. ($k&lt;&lt;d$)</li><li>$Y = xW$ is the result after reducing dimension to k dimension</li></ol><p><strong>Note:</strong> There are two prerequisites for conducting PCA</p><ul><li>Raw data has no NA</li><li>The raw data should be normalized</li></ul><h2 id=2-pca-from-scratch>2. PCA from scratch</h2><ul><li>Importing necessary modules</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> StandardScaler
</span></span></code></pre></div><ul><li>Creating raw data</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># get  data set</span>
</span></span><span style=display:flex><span>df_wine <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#34;</span>,
</span></span><span style=display:flex><span>    header<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>,
</span></span><span style=display:flex><span>    engine<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># check data</span>
</span></span><span style=display:flex><span>df_wine<span style=color:#ff79c6>.</span>head()
</span></span></code></pre></div><p><img src=https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/1572760322010.png alt=Test></p><ul><li>Creating train and test data set</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># create train and test data set</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff79c6>=</span> df_wine<span style=color:#ff79c6>.</span>iloc[:, <span style=color:#bd93f9>1</span>:], df_wine<span style=color:#ff79c6>.</span>iloc[:, <span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(
</span></span><span style=display:flex><span>    X, y, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.3</span>, stratify<span style=color:#ff79c6>=</span>y, random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><ul><li>Standardizing the features</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># create standard instance</span>
</span></span><span style=display:flex><span>sc <span style=color:#ff79c6>=</span> StandardScaler()
</span></span><span style=display:flex><span><span style=color:#6272a4># standard data</span>
</span></span><span style=display:flex><span>x_train_std <span style=color:#ff79c6>=</span> sc<span style=color:#ff79c6>.</span>fit_transform(x_train)
</span></span><span style=display:flex><span>x_test_std <span style=color:#ff79c6>=</span> sc<span style=color:#ff79c6>.</span>fit_transform(x_test)
</span></span></code></pre></div><ul><li>Creating the covariance matrix and Getting eigenvectors and eigenvalues</li></ul><p>the calculation of the covariance matrix :</p><p>$$\sigma_{jk} = \frac{1}{n} \sum^{n}<em>{i=1}\bigg(x</em>{j}^{(i)} - \mu_j\bigg)\bigg(x_{k}^{(i)} - \mu_k\bigg)$$</p><p>Then, using <code>numpy.cov</code> and <code>numpy.linalg.eig</code> to get the covariance matrix and eigenvectors respectively</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># calculate the covariance matrix</span>
</span></span><span style=display:flex><span>cov_mat <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>cov(x_train_std<span style=color:#ff79c6>.</span>T)
</span></span><span style=display:flex><span><span style=color:#6272a4># Getting eigenvectors and eigenvalues</span>
</span></span><span style=display:flex><span>eigen_vals, eigen_vecs <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>linalg<span style=color:#ff79c6>.</span>eig(cov_mat)
</span></span></code></pre></div><p><strong>NOTE:</strong> there are 13 eigenvectors totally, the number of eigenvalues might be not as same as the number of features sometimes.</p><p>Firstly, plotting the Variance interpretation ratio, which is obtained through eigenvalue $\lambda_j$ divided by the sum of all the eigenvalues:
$$ \frac{\lambda<em>j}{\sum^d</em>{j=1}\lambda_j}$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># get sum of all the eigenvalues</span>
</span></span><span style=display:flex><span>tot <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>sum</span>(eigen_vals)
</span></span><span style=display:flex><span><span style=color:#6272a4># get variance interpretation ratio</span>
</span></span><span style=display:flex><span>var_exp <span style=color:#ff79c6>=</span> [(i <span style=color:#ff79c6>/</span> tot) <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>sorted</span>(eigen_vals, reverse<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)]
</span></span><span style=display:flex><span>cum_var_exp <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>cumsum(var_exp)
</span></span></code></pre></div><p>Besides, plotting the result to get in-depth understanding:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>figure()  <span style=color:#6272a4># create plot</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># create bar plot</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>bar(
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>14</span>),
</span></span><span style=display:flex><span>    var_exp,
</span></span><span style=display:flex><span>    alpha<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.5</span>,
</span></span><span style=display:flex><span>    label<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;individual explained variance&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># create step plot</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>step(<span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>14</span>), cum_var_exp, where<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;mid&#34;</span>, label<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;cumulative explained variance&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># add label</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#34;Explained variance ratio&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#34;Principal component index&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># add legend</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>legend(loc<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;best&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># save picture</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>savefig(<span style=color:#f1fa8c>&#34;pca_index.png&#34;</span>, <span style=color:#8be9fd;font-style:italic>format</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;png&#34;</span>, bbox_inches<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;tight&#34;</span>, dpi<span style=color:#ff79c6>=</span><span style=color:#bd93f9>300</span>)
</span></span></code></pre></div><p><img src=https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/1572764173795.png alt>
We can conclude that <strong>PC1</strong> only takes account for about 40%. Furthermore, the sum of <strong>PC1</strong> and <strong>PC2</strong> have 60% variance.</p><ul><li>Selecting the first <strong>k</strong> values to form matrix $W$</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># integrate eigenvalues  and eigenvectors</span>
</span></span><span style=display:flex><span>eigen_paris <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>    (np<span style=color:#ff79c6>.</span>abs(eigen_vals[i]), eigen_vecs[:, i]) <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(eigen_vals))
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span><span style=color:#6272a4># sort according to eigenvalues</span>
</span></span><span style=display:flex><span>eigen_paris<span style=color:#ff79c6>.</span>sort(key<span style=color:#ff79c6>=</span><span style=color:#ff79c6>lambda</span> x: x[<span style=color:#bd93f9>0</span>], reverse<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># pick up the first 2 eigenvalues</span>
</span></span><span style=display:flex><span>w <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>hstack([eigen_paris[<span style=color:#bd93f9>0</span>][<span style=color:#bd93f9>1</span>][:, np<span style=color:#ff79c6>.</span>newaxis], eigen_paris[<span style=color:#bd93f9>1</span>][<span style=color:#bd93f9>1</span>][:, np<span style=color:#ff79c6>.</span>newaxis]])
</span></span><span style=display:flex><span><span style=color:#6272a4># check matrix x</span>
</span></span><span style=display:flex><span>w
</span></span></code></pre></div><p><img src=https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/1572766213478.png alt></p><ul><li>Transforming raw data</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># reduce dimension</span>
</span></span><span style=display:flex><span>x_train_pca <span style=color:#ff79c6>=</span> x_train_std<span style=color:#ff79c6>.</span>dot(w)
</span></span><span style=display:flex><span><span style=color:#6272a4># check resulted data</span>
</span></span><span style=display:flex><span>x_train_pca<span style=color:#ff79c6>.</span>shape
</span></span></code></pre></div><p><code>(124, 2)</code></p><p>Then plotting the result and putting the label in terms of original info, but keeping in mind PCA is unsupervised learning skill without labels</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># init colors and markers</span>
</span></span><span style=display:flex><span>colors <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;r&#34;</span>, <span style=color:#f1fa8c>&#34;b&#34;</span>, <span style=color:#f1fa8c>&#34;g&#34;</span>]
</span></span><span style=display:flex><span>markers <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;s&#34;</span>, <span style=color:#f1fa8c>&#34;x&#34;</span>, <span style=color:#f1fa8c>&#34;o&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#6272a4># plot scatter</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> l, c, m <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(np<span style=color:#ff79c6>.</span>unique(y_train), colors, markers):
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>scatter(
</span></span><span style=display:flex><span>        x_train_pca[y_train <span style=color:#ff79c6>==</span> l, <span style=color:#bd93f9>0</span>],
</span></span><span style=display:flex><span>        x_train_pca[y_train <span style=color:#ff79c6>==</span> l, <span style=color:#bd93f9>1</span>],
</span></span><span style=display:flex><span>        c<span style=color:#ff79c6>=</span>c,
</span></span><span style=display:flex><span>        label<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>        marker<span style=color:#ff79c6>=</span>m,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span><span style=color:#6272a4># add label and legend</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#34;PC 1&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#34;PC 2&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>legend(loc<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;lower left&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>savefig(<span style=color:#f1fa8c>&#34;distribution.png&#34;</span>, <span style=color:#8be9fd;font-style:italic>format</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;png&#34;</span>, bbox_inches<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;tight&#34;</span>, dpi<span style=color:#ff79c6>=</span><span style=color:#bd93f9>300</span>)
</span></span></code></pre></div><p><img src=https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/1572766761786.png alt></p><h2 id=3-pca-by-scikit-learn>3. PCA by scikit-learn</h2><p>we can conduct PCA easily by <strong>sklearn</strong></p><ul><li>Importing modules</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.decomposition <span style=color:#ff79c6>import</span> PCA
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> matplotlib.colors <span style=color:#ff79c6>import</span> ListedColormap
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.linear_model <span style=color:#ff79c6>import</span> LogisticRegression
</span></span></code></pre></div><ul><li>Defining function of plot_decision_region</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>plot_dicision_regions</span>(X, y, classifier, resolution<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.02</span>):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># init markers and colors</span>
</span></span><span style=display:flex><span>    markers <span style=color:#ff79c6>=</span> (<span style=color:#f1fa8c>&#34;s&#34;</span>, <span style=color:#f1fa8c>&#34;x&#34;</span>, <span style=color:#f1fa8c>&#34;o&#34;</span>, <span style=color:#f1fa8c>&#34;^&#34;</span>, <span style=color:#f1fa8c>&#34;v&#34;</span>)
</span></span><span style=display:flex><span>    colors <span style=color:#ff79c6>=</span> (<span style=color:#f1fa8c>&#34;red&#34;</span>, <span style=color:#f1fa8c>&#34;blue&#34;</span>, <span style=color:#f1fa8c>&#34;lightgreen&#34;</span>, <span style=color:#f1fa8c>&#34;gray&#34;</span>, <span style=color:#f1fa8c>&#34;cyan&#34;</span>)
</span></span><span style=display:flex><span>    cmap <span style=color:#ff79c6>=</span> ListedColormap(colors[: <span style=color:#8be9fd;font-style:italic>len</span>(np<span style=color:#ff79c6>.</span>unique(y))])
</span></span><span style=display:flex><span>    <span style=color:#6272a4># create info for plot region</span>
</span></span><span style=display:flex><span>    x1_min, x1_max <span style=color:#ff79c6>=</span> X[:, <span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>min() <span style=color:#ff79c6>-</span> <span style=color:#bd93f9>1</span>, X[:, <span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>max() <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>    x2_min, x2_max <span style=color:#ff79c6>=</span> X[:, <span style=color:#bd93f9>1</span>]<span style=color:#ff79c6>.</span>min() <span style=color:#ff79c6>-</span> <span style=color:#bd93f9>1</span>, X[:, <span style=color:#bd93f9>1</span>]<span style=color:#ff79c6>.</span>max() <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>    xx1, xx2 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>meshgrid(
</span></span><span style=display:flex><span>        np<span style=color:#ff79c6>.</span>arange(x1_min, x1_max, resolution), np<span style=color:#ff79c6>.</span>arange(x2_min, x2_max, resolution)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#6272a4># test classifier&#39;s accurate</span>
</span></span><span style=display:flex><span>    z <span style=color:#ff79c6>=</span> classifier<span style=color:#ff79c6>.</span>predict(np<span style=color:#ff79c6>.</span>array([xx1<span style=color:#ff79c6>.</span>ravel(), xx2<span style=color:#ff79c6>.</span>ravel()])<span style=color:#ff79c6>.</span>T)
</span></span><span style=display:flex><span>    z <span style=color:#ff79c6>=</span> z<span style=color:#ff79c6>.</span>reshape(xx1<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># plot decision region</span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>contourf(xx1, xx2, z, alpha<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.4</span>, cmap<span style=color:#ff79c6>=</span>cmap)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># set x,y length</span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>xlim(xx1<span style=color:#ff79c6>.</span>min(), xx1<span style=color:#ff79c6>.</span>max())
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>ylim(xx2<span style=color:#ff79c6>.</span>min(), xx2<span style=color:#ff79c6>.</span>max())
</span></span><span style=display:flex><span>    <span style=color:#6272a4># plot result</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> idx, cl <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(np<span style=color:#ff79c6>.</span>unique(y)):
</span></span><span style=display:flex><span>        plt<span style=color:#ff79c6>.</span>scatter(
</span></span><span style=display:flex><span>            x<span style=color:#ff79c6>=</span>X[y <span style=color:#ff79c6>==</span> cl, <span style=color:#bd93f9>0</span>],
</span></span><span style=display:flex><span>            y<span style=color:#ff79c6>=</span>X[y <span style=color:#ff79c6>==</span> cl, <span style=color:#bd93f9>1</span>],
</span></span><span style=display:flex><span>            alpha<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.6</span>,
</span></span><span style=display:flex><span>            color<span style=color:#ff79c6>=</span>cmap(idx),
</span></span><span style=display:flex><span>            edgecolor<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;black&#34;</span>,
</span></span><span style=display:flex><span>            marker<span style=color:#ff79c6>=</span>markers[idx],
</span></span><span style=display:flex><span>            label<span style=color:#ff79c6>=</span>cl,
</span></span><span style=display:flex><span>        )
</span></span></code></pre></div><ul><li>PCA by sklearn</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># create pca instance</span>
</span></span><span style=display:flex><span>pca <span style=color:#ff79c6>=</span> PCA(n_components<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># create classifier instance</span>
</span></span><span style=display:flex><span>lr <span style=color:#ff79c6>=</span> LogisticRegression()
</span></span><span style=display:flex><span><span style=color:#6272a4># reduce dimension for  data set</span>
</span></span><span style=display:flex><span>x_train_pca <span style=color:#ff79c6>=</span> pca<span style=color:#ff79c6>.</span>fit_transform(x_train_std)
</span></span><span style=display:flex><span>x_test_pca <span style=color:#ff79c6>=</span> pca<span style=color:#ff79c6>.</span>transform(x_test_std)
</span></span><span style=display:flex><span><span style=color:#6272a4># classify x_train_pca</span>
</span></span><span style=display:flex><span>lr<span style=color:#ff79c6>.</span>fit(x_train_pca, y_train)
</span></span><span style=display:flex><span><span style=color:#6272a4># plot dicision region</span>
</span></span><span style=display:flex><span>plot_dicision_regions(x_train_pca, y_train, classifier<span style=color:#ff79c6>=</span>lr)
</span></span><span style=display:flex><span><span style=color:#6272a4># add info</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#34;PC 1&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#34;PC 2&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>legend(loc<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;lower left&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><p><img src=https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/1572767868782.png alt>
We can see that the classifier&rsquo;s accurate is excellent according to actual labels</p><p><strong>TIPS</strong>:</p><p>You can set <code>n_components = None</code>, and the result would retain all principle components. Moreover, you could call <code>explained_variance_ration_</code> to use variance explanation ratio.</p><h2 id=3-summary>3. Summary</h2><p>All the above are the main content, welcome everybody communicates with me! ðŸ¤ </p><p><strong>Reference book :</strong> <a href=https://m.media-amazon.com/images/I/41ehHjX-XhL.jpg>Python machine learning</a></p></div></article><hr><p class=articleTagsContainer><span>ï€«</span>
<strong>Tags:</strong>
<a href=/tags/machine-learning>#Machine Learning</a>
<a href=/tags/python>#Python</a></p><script type=text/javascript src=https://latest.cactus.chat/cactus.js></script><div id=ficurinia-cactus-comments></div><script>initComments({node:document.getElementById("ficurinia-cactus-comments"),defaultHomeserverUrl:"https://matrix.cactus.chat:8448",serverName:"cactus.chat",siteName:"blog",commentSectionId:"03fb34183525935008a6381c290bcc28"})</script><div class=relatedArticlesContainer><hr><h2>More posts like this</h2><div class=postlist><article class="card postlistitem"><div><h2><a href=https://cauliyang.github.io/posts/005-important-resources-for-python-development/>Comprehensive Resources for Python Development</a></h2><p class=date><span title=Date>ï—¬</span>
2022-06-16
|
<span title=Tags>ï€«</span>
<a href=/tags/python>#Python</a></p><div class=articlePreview><p>Python project for development 1. Git Using git-flow to automate your git branching workflow
Gitflow
Git-flow-completion
Understand how to write a good commit message
Awesome-Profile-README-templates
Mergigy for pull request
Jupyter notebook diff and merge
2. Document system 2.1 Sphinx and RST Sphinx and RST syntax guide (0.9.3) Python Docstring Sphinx and RST Syntax reStructuredText Markup Specification Sphinx Themes Gallery 2.2 pdoc pdoc 2.3 Mkdocs Getting Started - MkDocs 3. Module statistics for installation Pepy stats PyPI Stats 4.</p><p><a href=https://cauliyang.github.io/posts/005-important-resources-for-python-development/>Continue reading ï•“</a></p></div></div><hr></article></div></div></main><footer><hr><p><small>2022 &copy; Yangyang Li - <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</small></p><p><small><a href=https://gitlab.com/gabmus/hugo-ficurinia>Ficurinia theme</a> for <a href=https://gohugo.io>Hugo</a> by <a href=https://gabmus.org>Gabriele Musco</a>. Licensed under <a href=https://www.gnu.org/licenses/agpl-3.0.html>GNU AGPLv3</a>.</small></p></footer></div></div></div><script async defer data-domain=example.com src=https://something.com/...></script>
<script async defer data-website-id=example-tracking-code src=https://something.com/...></script></body></html>