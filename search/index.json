{"pages":[{"date":"2022-12-15","image":"https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/iShot_2022-12-17_01.41.16.png","imageAlt":"","link":"https://yangyangli.top/posts/012-make-a-powerful-ternimal/","summary":"Using Alacritty, Zellij, and Fish to empower the terminal","tags":["Tool"],"text":"i recently change my terminal setup from iterm2 + zsh + tmux to alacritty + fish + zellij. i found that my new combination is really powerful. these tools are more flexible and high-degree customized is an important reason i keep them as well. however, the configuration is a time-consuming and frustration processing. you can click the header to download the related tool. all the tools are open source :rocket:\n1. alacritty after installing it you can create a file ~/.alacritty.yml, which is configuration file of alacritty. i use github to host [my all configuration] including alacritty. it contains detailed comments to help people understand what the options do. one thing i want to mention is we need to change key mapping in order to map alt to option or meta key in macos. this issue has enough information about how to fix it. you are able to change theme color you prefer. i like using base 16 themes.\n2. zellij i used tmux before as it very popular terminal windows management tool. recently, i am learning rust so that i pay more attention on tool implemented by rust. zellij is one of excellent tool implemented by rust. also, it is user-friendly compared to tmux since it can provide key-mapping and some tips in your terminal. hence, we do not need to find and review a cheat-sheet again and again. ~/.config/zellij/config.kdl is the configuration file of zellij. to be honest, you may not need to add any configuration. same as alacritty you can change color theme.\n3. fish i really like fish which already save my life. compared with zsh, you do not need to install plenty of plugins to own powerful features. however, you can use out of box features including auto-suggestion, searching from history, fancy completion. meanwhile, it is more user-friendly. you can install plugins, but i think you will not need too much. fisher is plugins management tool of fish shell. take a look, and you will find recommended plugins as well. fish also is high-degree customized, you can change prompt and greeting message as you like. i use [tide] prompt, and it is a clean and beautiful prompt.\n4. summary the documentation is enough for you to start using these amazing tools. meanwhile, you can reference my configuration to modify yours. i use many fancy and powerful terminal applications as well, and i have shared that at the blog about my desktop setup.\n","title":"Make A Powerful Terminal Workspace"},{"date":"2022-11-26","image":"https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/iShot2021-11-27%2022.27.14.png","imageAlt":"","link":"https://yangyangli.top/posts/002-macos-configuration/","summary":"Concrete description about how to config my mac","tags":["Tool"],"text":"1. introduction configuration migration is time-consuming task so that i decide to write a blog to record the process. i will divide the configuration into two parts. in the first part, i will introduce the software i heavily use, and then share the configuration files in the second part. my newest device is macbook pro of m1 model, here is a list of tools i use:\ni use homebrew to install most software and how to install will be shown in every section.\n2. system environment tools the first part will show the applications and tools i used for production, and every app or tool has a simple description. if you have an interest in a certain app, you can click the link i leave to get more information.\nhomebrew homebrew is an important tool for package manager in mac, which help you download and uninstall applications or tools efficiently. for detailed information, please click the link above to check that. installation\n/bin/bash -c \u0026#34;$(curl -fssl https://raw.githubusercontent.com/homebrew/install/head/install.sh)\u0026#34; now i replace some tool with more interesting ones:\niterm2 → alacritty zsh → fish tmux → zellij however, i still leave information about previous tools. i also write [another blog] to talk about how to move to new terminal setting.\nalacritty brew install --cask alacritty brew install zellij alacritty is light-weight terminal emulator compared to iterm2. it is flexible, and it is configured by yaml file directly. i think i will keep using it so far. meanwhile, i use it with zellij to manage the terminal windows. compared to tmux, zellij provide more friendly interface. like tmux, you can use configuration file to change zellij. both are blazing fast, and they are implemented by rust.\nfish fish is a smart and user-friendly command line shell for linux, macos, and the rest of the family. it includes some valuable features by default in comparison with zsh. for example, autosuggestion. hence, you can use it out of box without any efforts for configuration. however, fish has plugins system as well, and i use fisher to manage plugins. importantly, the customization and configuration of fish is easy and friendly.\niterm2 brew install --cask iterm2 iterm2 is a replacement for terminal and the successor to iterm. it works on macs with macos 10.14 or newer. iterm2 brings the terminal into the modern age with features you never knew you always wanted. also, iterm2 has integrated tmux, you can try to use tmux, which is a terminal multiplexer. it lets you switch easily between several programs in one terminal, detach them (they keep running in the background), and reattach them to a different terminal.\nif you usually work on a remote server, it is a good idea to use tmux in the remote server. that will help you keep the connection in case you lose control of the current shell in which you are working. there is a tip, you can add ssh -t user@hostname \u0026quot;tmux attach -t coding01 || tmux new -s coding01\u0026quot;. then, you will connect the shell you have worked on last time when you log in to the remote server. if you never heard tmux, you can click the link above to learn how to use it. in addition, tmuxinator is great choice to empower tmux\nzsh zsh is an alternative to bash, and it has more advanced features. oh my zsh is a delightful, open-source, community-driven framework for managing your zsh configuration. it comes bundled with thousands of helpful functions, helpers, plugins, themes, and a few things that make you shout. the screenshot of my terminal is shown in the figure below.\nzsh\nyou can customize your own terminal, which will improve working efficiency. also, there are many plugins you can use to improve your working efficiency.\nvim brew install nvim vim is an editor used in the terminal, and i use it every day. vim is shipped with macos, and you also install other vim-extended editors such as neovim and spacevim. currently, i use lunarvim, an opinionated, extensible, and fast ide layer for neovim. i think it is so cool and powerful.\ngit brew install git git is a free and open-source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. also, git a t tool used in the terminal to download and upload data or code to the github. similarly, git is shipped with macos, so you may need to update that by brew upgrade git. what is more, oh my zsh provides some alias and plugins, which help us to use git easily.\nconda conda is a package, dependency, and environment management for any language such as python, r, ruby, c/c++, and more. conda is an open-source package management system and environment management system that runs on windows, macos, and linux. conda quickly installs, runs, and updates packages and their dependencies. conda easily creates, saves, loads, and switches between environments on your local computer. it was created for python programs, but it can package and distribute software for any language.\nin addition, i recommend that you use mamba to wrap conda to accelerate running speed.\nhowever, you should install miniforge that is a minimal installer for conda with some pre-configured features if you use m1 model. miniforge emphasis on supporting various cpu architectures including apple m1. you can also use mamba or micromamba to install packages in conda environment.\ntldr the tldr pages are a community effort to simplify the beloved man pages with practical examples. you can get usage clearly and quickly by using tldr compared to man. in mac, you can install that by brew install tldr.\ntldr\ntree brew install tree tree is a recursive directory listing program that produces a depth-indented listing of files. with no arguments, tree lists the files in the current directory. when directory arguments are given, tree lists all the files or directories found in the given directories each in turn. upon completion of listing all files and directories found, tree returns the total number of files. you can install that by brew install tree.\ntree\ncheat.sh cheat.sh is the unified access to the best community driven cheat sheets repositories of the world. i like it very much :heart:\ncheat.sh\ndust brew install dust dust = du + rust. it like du but more intuitive.\ndust\nhyperfine brew install hyperfine hyperfine is a command-line benchmarking tool.\nhyperfine\n3. editor i will list the editors i used in different situations. if you have any other recommendations, please reach out to me!\nvs code vs code is an open-source editor, and i mainly use vs code to write markdown, latex, and rst. some simple modifications of the project for python, c/c++, rust, will be written in vs code. there are lots of extensions to help you achieve your goals. i usually use it to write my blog.\npycharm pycharm is my main editor when i develop tools or applications in python. i use edu emails to apply for the professional version. i think it is my favorite ide when i develop python project in local environment. in one word, i love that very much! :smile:\nclion clion is my c/c++ and rust ide, which includes many features like code generation, code quality assurance and code refactor. i use edu emails to apply for the professional version as well.\nlunarvim lunarvim is a comprehensive configuration of neovim, and it gives you full-profile ide developed experience! i cannot live without it! :rocket:\nclosing thoughts: i really like neovim when i develop some project about rust.\n4. application this part list variety of applications used for different goals. every application has a one-word description. some of them can be installed by homebrew. my config files and installation commands are kept in github.\nalfred a tool can give your different control and efficiency in mac sensei the next generation mac performance tool default folder x a tool can empower default finder docker is a tool to create a safe environment for development or production chrome there is no reason not to use it. :heart: iina great tool that is used to play video imagine compress images before you upload somewhere. light and powerful! magnet a tool is used to manage windows for different applications rectangle a tool to move and resize windows in macos office :smile: monitorcontrol it manages brightness and sound for different monitors monodraw a tool is used to design fancy ascii strings pdf expert best pdf reader in mac picgo a tool is used to upload images to web service like github. it is beneficial for writing blogs. snippetslab my favorite tool stored code snippets, and it can be integrated with alfred xcode :smile: zoom meeting! xmind always makes your creative and keeps your minds clears transmit upload, download and manage files on servers with beautiful and powerful ui time sink is a good tool to record your using time to track your behavior soundsource can help you get truly powerful control over all the audio reeder 5 a rss reader and keep control of your reading [path finder] better file manager for macos notion a excellent notion tool one switch can help you finish some progress like keep awake and hide icons on one button ishot a great tool to take screenshots google drive is a cloud storage service that allows you to store and share files with anyone ferdi can integrate other tools like gmail, slack, or others to allow you manage information in one place dash is a good friend for developers cleanshot x i usually use it to create gif, although it can take screenshots as well bartender is a great application to manage icons of all your working tools alttab is a good tool to manage windows for different applications aldente is able to keep your battery healthy by controlling the power consumption zotero is my favorite tools to manage research papers ","title":"Configuration of macOS for development and research"},{"date":"2022-09-26","image":"","imageAlt":"","link":"https://yangyangli.top/posts/011-bioinformatics-algorithm-library/","summary":"A collection of algorithms and data structures that are designed for modern C++ bioinformatics applications.","tags":["Algorithm","Bioinformatics","C++","Data Structure"],"text":"the library is a collection of algorithms and data structures that are designed for modern c++ bioinformatics applications. you can use the library in your own projects or as a part of a larger project.\nthe library will include efficient data structure and algorithm implemented by modern c++.\nthe design philosophy of the library:\nembrace c++20 standard supports modern c++ features support concurrency and thread safety priority is given to safety and clean design hard to use wrongly testing extensively python bindings the current project is developing and evolving, and changes will be made to the library as time goes on.\n","title":"Bioinformatics Algorithm Library aka BINARY"},{"date":"2022-09-25","image":"","imageAlt":"","link":"https://yangyangli.top/posts/009-new-tool-bioinformatics-toolbox-aka-boss/","summary":"A toolbox for bioinformatics analysis in C++","tags":["Bioinformatics","C++","Develop"],"text":"boss is a bioinformatics toolbox, which will contain efficient tools. it is written in modern c++ and is tested exhaustively. it is designed to be easy to use and time-efficient. boss is a free software and is distributed under the terms of the gnu general public license v3.\ncurrently, boss contains the following tools:\nboss-fqsp : a fast tool for splitting fastq files into paired files. boss-squeue: summary status of jobs in hpc queues it is now evolving and will be updated frequently. please check the github repository for the latest version.\n","title":"Bioinformatics Toolbox Aka Boss"},{"date":"2022-06-15","image":"","imageAlt":"","link":"https://yangyangli.top/posts/003-guide-build-cpp-development-env-with-htslib/","summary":"1. Config Compile Environment I am preparing to develop a tool using C++ in the Linux and macOS environments. Usually, I do not have root rights to download dependencies for C++ using apt-get install -y denpendencies directly in Ubuntu. However, I usually feel frustrated when the dependency chain is complicated. That means I need to download and compile every library I use. This may cost you one night or even one week.","tags":["Develop"],"text":"1. config compile environment i am preparing to develop a tool using c++ in the linux and macos environments. usually, i do not have root rights to download dependencies for c++ using apt-get install -y denpendencies directly in ubuntu. however, i usually feel frustrated when the dependency chain is complicated. that means i need to download and compile every library i use. this may cost you one night or even one week. conda is a package manager mainly used in the data-science domain. so far, conda provides some other language dependencies, for instance c++, rust and r as well. for concrete package support, you can go through the conda cloud. the package name in conda cloud may change at any time. you may need to search for \u0026ldquo;real name\u0026rdquo; in conda cloud before installation. hence, conda can be regarded as a tool used to install dependencies for c++, especially in the bioinformatics domain. however, i must say that nowadays, there are several solutions for managing c++ dependencies like vcpkg, conan, cpm, etc. i use cpm as an alternative choice.\n1.1 install gcc or clang when it comes to the compilation environment, it is important to install a compiler, and gcc or clang may be your choices. in general, linux systems will ship with gcc, but the version may be low (4. 9). that will not allow you to use the latest features of c++. in the meantime, you do not have root access yet. but you can use conda to install any version of gcc or clang by running conda install -c conda-forge gcc or conda install -c conda-forge clang. keep in mind that you should search for gcc or clang in conda cloud first before installation in order to install the proper version.\nafter installation, conda may set three significant variables for you: cflags, cxxflags, and ldflags. you can check that by using echo $cflags. you need to set that in your ~/.bashrc or ~/.zshrc if you do not find that. here are examples:\nexport cxxflags=\u0026#34;-fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fpic -fstack-protector-strong -fno-plt -o2 -ffunction-sections -pipe -isystem /your_conda_absolute_path/miniconda3/include\u0026#34; export cflags=\u0026#34;-march=nocona -mtune=haswell -ftree-vectorize -fpic -fstack-protector-strong -fno-plt -o2 -ffunction-sections -pipe -isystem /your_conda_absolute_path/include\u0026#34; export ldflags=\u0026#34;-wl,-o2 -wl,--sort-common -wl,--as-needed -wl,-z,relro -wl,-z,now -wl,--disable-new-dtags -wl,--gc-sections -wl,--allow-shlib-undefined -wl,-rpath,/your_conda_absolute_path/miniconda3/lib -wl,-rpath-link,/your_conda_absolute_path/miniconda3/lib -l/your_conda_absolute_path/miniconda3/lib\u0026#34; you must change your conda absolute path to your path of conda. i installed conda in the base environment, and i also recommend installing gcc or clang in the base environment.\n2. add htslib dependencies htslib is a classic and well-known library used to manage bioinformatics format files, including bam, sam, vcf, and bcf, etc.\nhtslib is implemented in c so that it is able to meet high performance requirements. so far, many popular tools, for example samtools and bcftools, are all based on htslib. there are wrappers for other languages like pysam based on python, rhtslib based on r, and [rust_htslib] based on rust. that will enable people using different languages to apply htslib in their applications.\nzlib is only one library that htslib must depend on. in the meantime, htslib has other dependencies, which enables htslib to have more rich features. here, i will not explain what feature dependencies will provide respectably. the detailed information can be found on the htslib website. however, *conda* can install htslib easily by conda install htslib. keep in mind that we can also define the library version with conda install htslib=1.15.1. as mentioned above, htslib may have been installed in your environment if samtools or bcftools are already installed.\n2.1 cmake scripts i use cmake as a build system, and here are several useful and valuable cmake scripts. you can use that in your project. i think it will help you fix most dependency problems with htslib.\nhtslib.cmake\nfindhtslib.cmake\nfinddeflate.cmake\nzlib.cmake\nfirstly, i assume your directory structure looks like this:\n. ├── cmakelists.txt ├── build ├── cmake ├── include └── source these cmake scripts should be found in the cmake directory. we can use in cmakelists.txt\nlist(append cmake_module_path \u0026#34;${cmake_current_source_dir}/cmake\u0026#34;) include(htslib) if htslib is found in your current environment, cmake will define the variables htslib_found and htslib_include_dirs``andhtslib_libraries. otherwise, cmakewill build static htslib from sources, andcmakewill check if every dependency of htslib exists respectively. if it exists,cmakewill use the compiler flags of htslib to build a static library. otherwise,cmakewill disable related compiler flags. however, zlib is the only one that must exist. so, if zlib does not exist,cmake will help you build zlib from source. all these cmake scripts can be found at the above link. please feel free to explore that.\nindeed, we use findhtslib.cmake to search for htslib. htslib.cmake is shown below, and i have added some comments to explain how it works.\ninclude(externalproject) # find htslib by using findhtslib.cmake find_package(htslib) if(htslib_found) message(status \u0026#34;htslib_use_static_libs: ${htslib_use_static_libs}\u0026#34;) # not found, try to build it to static libs from source else() set(htslib_prefix ${cmake_binary_dir}/cmake-ext/htslib-prefix) set(htslib_install ${cmake_binary_dir}/cmake-ext/htslib-install) if(cmake_generator strequal \u0026#34;unix makefiles\u0026#34;) set(make_command \u0026#34;$(make)\u0026#34;) else() find_program(make_command names make gmake) endif() message(status \u0026#34;building static htslib from source\u0026#34;) message(notice \u0026#34;set env cflags and cxxflags if you use conda environment!\u0026#34;) set(disable_flags --disable-gcs --disable-s3 --disable-plugins) # find lzma if not founed the disable compiler flags find_package(liblzma) if(liblzma_found) include_directories(system ${liblzma_include_dirs}) list(append deps_lib ${liblzma_libraries}) else() list(append disable_flags --disable-lzma) endif() # find curl if not founed the disable compiler flags find_package(curl) if(curl_found) include_directories(system ${curl_include_dirs}) list(append deps_lib ${curl_libraries}) else() list(append disable_flags --disable-libcurl) endif() #find bzip2 if not founed the disable compiler flags find_package(bzip2) if(bzip2_found) include_directories(system ${bzip2_include_dirs}) list(append deps_lib ${bzip2_libraries}) else() list(append disable_flags --disable-bz2) endif() # find defalte if not founed the disable compiler flags find_package(deflate) if(deflate_found) include_directories(system ${deflate_include_dirs}) list(append deps_lib ${deflate_libraries}) endif() message(status \u0026#34; dependencies: ${deps_lib}\u0026#34;) # compiler and install htslib from source externalproject_add( htslib prefix ${htslib_prefix} url https://github.com/samtools/htslib/releases/download/1.15.1/htslib-1.15.1.tar.bz2 build_in_source 1 update_command \u0026#34;\u0026#34; configure_command autoreconf -i \u0026amp;\u0026amp; ./configure --prefix=${htslib_prefix} ${disable_flags} build_command ${make_command} lib-static install_command ${make_command} install prefix=${htslib_install} ) # user pre-defined variable (-dzlib_build=on) to control is build zlib from sources message(status \u0026#34;zlib_build: ${zlib_build}\u0026#34;) if(zlib_build) include(cmake/zlib.cmake) add_dependencies(htslib zlib) else() find_package(zlib) if(zlib_found) include_directories(system ${zlib_include_dirs}) list(append deps_lib ${zlib_libraries}) else() # build zlib from source message(status \u0026#34;building zlib from source\u0026#34;) include(cmake/zlib.cmake) add_dependencies(htslib zlib) list(append deps_lib ${zlib_libraries}) endif() endif() list(append deps_lib ${zlib_libraries}) # define two variables for usage set(htslib_include_dirs ${htslib_install}/include) set(htslib_libraries ${htslib_install}/lib/libhts.a ${deps_lib}) message(status \u0026#34;htslib_include_dirs: ${htslib_include_dirs}\u0026#34;) message(status \u0026#34;htslib_libraries: ${htslib_libraries}\u0026#34;) endif() now you can use the htslib like this in cmake:\nadd_library(test test.h test.cpp) # if htslib is not in your environment if(not htslib_found) # if htslib build from source you need add this add_dependencies(${project_name} htslib) endif() target_link_libraries(test private ${htslib_libraries}) target_include_directories(test private ${htslib_include_dirs} ${htslib_include_dirs}/htslib) you can change private to public if you want to export htslib. it depends on your goal.\n3. recommend practices in the beginning, i recommend you use a suitable directory structure. here are a few of the best examples:\nhttps://github.com/thelartians/moderncppstarter https://github.com/cpp-best-practices/gui_starter_template https://github.com/filipdutescu/modern-cpp-template i prefer the first one. using a good template can help you learn other tools in order to make the project better. after you dive into one of the templates, i think you will learn more than you imagined.\n4. summary in this blog, i will introduce how to install c++ dependencies and how to configure htslib in your development environment. various cmake scripts are provided and can help the project fix htslib dependency problems smoothly. the source of these scripts at the top. if you have any questions, please feel free to reach me. thanks for your time and reading!\n","title":"Guide for building Cpp Development in Bioinformatics"},{"date":"2021-12-03","image":"https://cdn.jsdelivr.net/gh/cauliyang/blog-image@main//img/logo.png","imageAlt":"","link":"https://yangyangli.top/posts/001-a-wonderful-library-you-must-not-know/","summary":"A toolbox including a batch of useful commands for annoying tasks.","tags":["Develop","Python"],"text":"1. introduction many people have been asking for a toolbox of useful commands for their everyday tasks. i also want to build a toolbox for my own use, and i want to make it as easy as possible to use. hence, i decided to build this project, which may be a valuable tool for other people as well. i will add more commands to this toolbox once i find them useful in the future. now, there is a command about google driver, which helps you to download files located on google driver. it can download one single file or files under folder. however, downloading files under one folder need authentication of google api. moreover, this project includes other features shown below. forget to say, i call it pybox as it is a toolbox based on python.\n2. installation the library is available on pypi so that you can install it with pip or pip3 by typing pip install pyboxes. i also have plan to publish this library on conda so that you can install it with conda install pyboxes.\n3. usage $ pybox -h # show help usage: pybox [options] \u0026lt;command\u0026gt; this tool include a bunch of useful commands: 1. download single file or all files in a folder for google driver 2. send message to slack 3. more to come... options: -h, --help show this message and exit. commands: asyncdown download files in terms of links asynchronously. gfile download file in google driver. gfolder download files in folders in google drive. slack send message to slack. yangyang-li https://yangyangli.top/ 2022 4. features a simple and easy to download files by sharing link a simple and easy to send message to slack channel download multiple files asynchronously download books from zlib in terms of title will come! i will add more commands to this toolbox as i find them useful in the future. what is more, i would like to make sure the quality of the toolbox is high. i write some examples of commands. if you need something specific, please let me know. we can discuss it.\n5. contributing if you would like to contribute to this toolbox, please feel free to fork it on github. please make sure you read the contributing guide before you start. also, please make sure you pass all the tests before you pull request.\n","title":"A Python Toolbox - Pybox"},{"date":"2021-04-04","image":"","imageAlt":"","link":"https://yangyangli.top/posts/010-pca-by-python/","summary":"This article records two methods of PCA analysis using Python, and visualizes 2-dimensional results.","tags":["Machine Learning","Python"],"text":"1. introduction 1.1 what\u0026rsquo;s pca? when it comes to methods of reducing dimension, pca that is an unsupervised linear transformation technique, must be not ignored. moreover, if you want to know the subtle relationships among data set and reduce the computational complexity in downstream analysis, the pca may be your best choice! meanwhile, if you would like to present your data in a 2-dimension or 3-dimension coordinate system, and pca would sweep your problems!\nwhat is reducing dimension? i will show you an example as follows: first, suppose you have a five-dimensional data set :\nid 1-d 2-d 3-d 4-d 5-d data-1 1 2 3 4 5 data-2 6 7 8 9 10 .. .. .. .. .. then, you could pick up pc1 and pc2 after pca to reduce dimension for plotting:\nid pc1 pc2 data-1 0.3 0.6 data-2 0.1 1.2 .. .. .. pc1 and pc2 are the result obtained through data is projection on the unit vectors, which enable result to have the biggest variance(means its distribution is wide) and to be irrelevant(covariance = 0).\n1.2 algorithm normalize $d$ dimension raw data create the covariance matrix calculate the eigenvalues of the covariance matrix and the corresponding eigenvectors the eigenvectors are sorted in the matrix according to the corresponding feature value, and the first k rows are formed into a matrix $w$. ($k\u0026lt;\u0026lt;d$) $y = xw$ is the result after reducing dimension to k dimension note: there are two prerequisites for conducting pca\nraw data has no na the raw data should be normalized 2. pca from scratch importing necessary modules import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import standardscaler creating raw data # get data set df_wine = pd.read_csv( \u0026#34;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\u0026#34;, header=none, engine=\u0026#34;python\u0026#34;, ) # check data df_wine.head() creating train and test data set # create train and test data set x, y = df_wine.iloc[:, 1:], df_wine.iloc[:, 0] x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.3, stratify=y, random_state=0 ) standardizing the features # create standard instance sc = standardscaler() # standard data x_train_std = sc.fit_transform(x_train) x_test_std = sc.fit_transform(x_test) creating the covariance matrix and getting eigenvectors and eigenvalues the calculation of the covariance matrix :\n$$\\sigma_{jk} = \\frac{1}{n} \\sum^{n}{i=1}\\bigg(x{j}^{(i)} - \\mu_j\\bigg)\\bigg(x_{k}^{(i)} - \\mu_k\\bigg)$$\nthen, using numpy.cov and numpy.linalg.eig to get the covariance matrix and eigenvectors respectively\n# calculate the covariance matrix cov_mat = np.cov(x_train_std.t) # getting eigenvectors and eigenvalues eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) note: there are 13 eigenvectors totally, the number of eigenvalues might be not as same as the number of features sometimes.\nfirstly, plotting the variance interpretation ratio, which is obtained through eigenvalue $\\lambda_j$ divided by the sum of all the eigenvalues: $$ \\frac{\\lambdaj}{\\sum^d{j=1}\\lambda_j}$$\n# get sum of all the eigenvalues tot = sum(eigen_vals) # get variance interpretation ratio var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=true)] cum_var_exp = np.cumsum(var_exp) besides, plotting the result to get in-depth understanding:\nplt.figure() # create plot # create bar plot plt.bar( range(1, 14), var_exp, alpha=0.5, label=\u0026#34;individual explained variance\u0026#34;, ) # create step plot plt.step(range(1, 14), cum_var_exp, where=\u0026#34;mid\u0026#34;, label=\u0026#34;cumulative explained variance\u0026#34;) # add label plt.ylabel(\u0026#34;explained variance ratio\u0026#34;) plt.xlabel(\u0026#34;principal component index\u0026#34;) # add legend plt.legend(loc=\u0026#34;best\u0026#34;) # save picture plt.savefig(\u0026#34;pca_index.png\u0026#34;, format=\u0026#34;png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;, dpi=300) we can conclude that pc1 only takes account for about 40%. furthermore, the sum of pc1 and pc2 have 60% variance.\nselecting the first k values to form matrix $w$ # integrate eigenvalues and eigenvectors eigen_paris = [ (np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals)) ] # sort according to eigenvalues eigen_paris.sort(key=lambda x: x[0], reverse=true) # pick up the first 2 eigenvalues w = np.hstack([eigen_paris[0][1][:, np.newaxis], eigen_paris[1][1][:, np.newaxis]]) # check matrix x w transforming raw data # reduce dimension x_train_pca = x_train_std.dot(w) # check resulted data x_train_pca.shape (124, 2)\nthen plotting the result and putting the label in terms of original info, but keeping in mind pca is unsupervised learning skill without labels\n# init colors and markers colors = [\u0026#34;r\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;g\u0026#34;] markers = [\u0026#34;s\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;o\u0026#34;] # plot scatter for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter( x_train_pca[y_train == l, 0], x_train_pca[y_train == l, 1], c=c, label=1, marker=m, ) # add label and legend plt.xlabel(\u0026#34;pc 1\u0026#34;) plt.ylabel(\u0026#34;pc 2\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(\u0026#34;distribution.png\u0026#34;, format=\u0026#34;png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;, dpi=300) 3. pca by scikit-learn we can conduct pca easily by sklearn\nimporting modules from sklearn.decomposition import pca from matplotlib.colors import listedcolormap from sklearn.linear_model import logisticregression defining function of plot_decision_region def plot_dicision_regions(x, y, classifier, resolution=0.02): # init markers and colors markers = (\u0026#34;s\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;o\u0026#34;, \u0026#34;^\u0026#34;, \u0026#34;v\u0026#34;) colors = (\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;lightgreen\u0026#34;, \u0026#34;gray\u0026#34;, \u0026#34;cyan\u0026#34;) cmap = listedcolormap(colors[: len(np.unique(y))]) # create info for plot region x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1 x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1 xx1, xx2 = np.meshgrid( np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution) ) # test classifier\u0026#39;s accurate z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).t) z = z.reshape(xx1.shape) # plot decision region plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap) # set x,y length plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot result for idx, cl in enumerate(np.unique(y)): plt.scatter( x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), edgecolor=\u0026#34;black\u0026#34;, marker=markers[idx], label=cl, ) pca by sklearn # create pca instance pca = pca(n_components=2) # create classifier instance lr = logisticregression() # reduce dimension for data set x_train_pca = pca.fit_transform(x_train_std) x_test_pca = pca.transform(x_test_std) # classify x_train_pca lr.fit(x_train_pca, y_train) # plot dicision region plot_dicision_regions(x_train_pca, y_train, classifier=lr) # add info plt.xlabel(\u0026#34;pc 1\u0026#34;) plt.ylabel(\u0026#34;pc 2\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.show() we can see that the classifier\u0026rsquo;s accurate is excellent according to actual labels\ntips:\nyou can set n_components = none, and the result would retain all principle components. moreover, you could call explained_variance_ration_ to use variance explanation ratio.\n3. summary all the above are the main content, welcome everybody communicates with me! 🤠\nreference book : python machine learning\n","title":"PCA by Python"},{"date":"2020-12-15","image":"","imageAlt":"","link":"https://yangyangli.top/pages/about/","summary":"Hi there! My name is Yangyang Li, and I\u0026rsquo;m a Ph.D. student in the DGP Program at Northwestern University Feinberg School of Medicine.\nI\u0026rsquo;m passionate about developing algorithm and using programming languages like C++, Rust, and Python to solve complex problems in the field of Computational Biology. As a Ph.D. student, I have the opportunity to work on exciting research projects that push the boundaries of what we know and have the potential to make a real impact in the world.","tags":[],"text":" hi there! my name is yangyang li, and i\u0026rsquo;m a ph.d. student in the dgp program at northwestern university feinberg school of medicine.\ni\u0026rsquo;m passionate about developing algorithm and using programming languages like c++, rust, and python to solve complex problems in the field of computational biology. as a ph.d. student, i have the opportunity to work on exciting research projects that push the boundaries of what we know and have the potential to make a real impact in the world.\nin addition to my studies and research, i enjoy staying up-to-date on the latest developments in the tech industry and learning new programming languages and techniques. when i\u0026rsquo;m not busy with school or work, you can find me tinkering with new projects or exploring the outdoors. i started this website as a way to share my experiences, insights, and resources with others who are interested in programming and deep learning. i hope you\u0026rsquo;ll find something helpful or interesting here!\na cup of coffee will support me to insist sharing :smile:\n","title":"About"},{"date":"2019-04-03","image":"","imageAlt":"","link":"https://yangyangli.top/posts/008-nei-saitou-neighbor-joining/","summary":"one of my homework that requires me to implement Nei-Saitou Neighbor Joining algorithm to construct phylogenetic tree, as well as evaluating the bootstrap cconfidence","tags":["Algorithm","Bioinformatics"],"text":"1. background before diving into code, the description of nj algorithm can be found in , where first column indicates parent node, and second column is its children node, the last column is the value of edge.\n2. neighbor joining algorithm the neighbor-joining algorithm given a distance matrix d compute an uprooted tree topology complete with edge lengths that tries to preserve the additive property: $d_{i,m} + d_{j,m} − d_{i,j} = 2d_{k,m}$ where $k$ is the k-th node on both routes from $i$ and $j$ to $m$.\nlet the set of clusters be called $l$ and initially $i → c_i; ∀i$ that is $| c_i | = 1$ and $l = c_1 , c_2 , . . . c_n$. $d_{i,j}$ is the distance from the initial distance matrix. compute “normalized distance matrix” $d_{i,j}$ for all $i, j$ such that $$d_{i,j} = d_{i,j} − (r_i + r_j ) \\ where\\ r_i = \\frac{1}{|l|-2} \\sum_{z \\in l} d_{i,z}$$ this subtracts the average distance to all other nodes than the pair involved. note: this is not where we use the distance identity. use normalized distance to $(i, j) = argmin d_{i,j}; c_i,c_j \\in l$ merge $c_i ∪ c_j → c_k$ where $k$ is a new cluster number. mark old clusters as used so that effectively: $l ← l − c_i − c_j$ compute a new normalized distance matrix including the new cluster $k$ and excluding $i, j$. $$d_{k,z} = d_{z,k} = (d_{i,z} + d_{j,,z} − d_{i,j} ) for all z ∈ l 2$$ this uses the additivity of the distances to compute the distance to the new cluster from each other node. compute the length of the edges from $k$ to $i$ and $j$. even though $c_k$has assumed the role of both $c_i$ and $c_j$ you still need the edge length to $i$ and $j$ from $k$ in order to “draw” the tree. $$edge_{i,k} = (d_{i,j} + r_i − r_j)$$ $$edge_{j,k} = (d_{i,j} + r_j − r_i)$$ define height $h_k = d_{i,j} /2$ where $h_k$ is the height of node that is the ancestor to all in $c_k$. when drawing the tree $h_k$ is the height above the baseline (where all the leaves are). $l ← l ∪ c_k$ while there is more than two clusters left go to step 3 finally, join the remaining two clusters with: $$edge_{j,k} = d_{i,j} $$ implementation notes consider this part of the computation: $$d_{i,j} = d_{i,j} − (r_i + r_j ) \\ where \\ r_i = \\frac{1}{|l|-2} \\sum_{z \\in l}d_{i,z}$$ the values of rz can be computed once each time we want to compute matrix $d$. this saves a vast amount of time. furthermore, since $d{i,j}$ is only used to find the argmin of $d_{i,j}$ we actually don’t have to save array $d$; we only need to find the argmin of it. so computing all the r and then combine the argmin step with the computation of $d_{i,j}$\n3. implementation i write code contained comments, and it is about 1000 lines that consumes me two whole days. now let me show my code with rich comments. if you have any questions or recommendation, i am very glad to communicate with you! please feel free to reach me.\n# -*- coding: utf-8 -*- # time 20201109 \u0026#34;\u0026#34;\u0026#34; this code integrate bootstrap, getting edge_file and tree_file usage: python -i fa_file -d out_distance_matrix -e out_edge -t out_tree -b number_bootstrap \u0026#34;\u0026#34;\u0026#34; # import needed library import argparse import logging import random import time from collections import defaultdict, namedtuple import pandas as pd class timer: \u0026#34;\u0026#34;\u0026#34;[construct timer to show working time of tasks]\u0026#34;\u0026#34;\u0026#34; def __init__(self, func=time.perf_counter): \u0026#34;\u0026#34;\u0026#34;[init values] args: func : defaults to time.perf_counter. \u0026#34;\u0026#34;\u0026#34; self.elapsed = 0.0 self._func = func self._start = none def start(self): \u0026#34;\u0026#34;\u0026#34;[start a task] raises: runtimeerror:[if task has started then raise error] \u0026#34;\u0026#34;\u0026#34; if self._start is not none: raise runtimeerror(\u0026#34;already started\u0026#34;) self._start = self._func() def stop(self): \u0026#34;\u0026#34;\u0026#34;[end a task] raises: runtimeerror[if task has not started then raise error] \u0026#34;\u0026#34;\u0026#34; if self._start is none: raise runtimeerror(\u0026#34;not started\u0026#34;) end = self._func() self.elapsed += end - self._start self._start = none def reset(self): \u0026#34;\u0026#34;\u0026#34;[reset the working time]\u0026#34;\u0026#34;\u0026#34; self.elapsed = 0 def running(self): \u0026#34;\u0026#34;\u0026#34;[check if task is running] returns: bool \u0026#34;\u0026#34;\u0026#34; return self._start is not none def __enter__(self): \u0026#34;\u0026#34;\u0026#34;[function used to address \u0026#39;with text\u0026#39;]\u0026#34;\u0026#34;\u0026#34; self.start() return self def __exit__(self, *args): \u0026#34;\u0026#34;\u0026#34;[function used to address \u0026#39;with text\u0026#39;]\u0026#34;\u0026#34;\u0026#34; self.stop() class treenode: \u0026#34;\u0026#34;\u0026#34;[class of tree node]\u0026#34;\u0026#34;\u0026#34; def __init__(self, key=none): \u0026#34;\u0026#34;\u0026#34;[init values of tree node] args: key ([str, float], optional): [node_label, edge]. defaults to none. self.index: node_index \u0026#34;\u0026#34;\u0026#34; self.left, self.right = none, none self.key = key self.parent, self.index = none, none self.list = [] def insert_left_children(self, left_object): \u0026#34;\u0026#34;\u0026#34;[insert left children] args: left_object ([str, float]): [node_label, edge] \u0026#34;\u0026#34;\u0026#34; # create a new node temp = treenode(left_object) # if left children exist if self.left: temp.left = self.left self.left = temp # if left children does not exist else: self.left = temp def insert_right_children(self, right_object): \u0026#34;\u0026#34;\u0026#34;[insert right children] args: right_object ([str, float]): [node_label, edge] \u0026#34;\u0026#34;\u0026#34; # create a new node temp = treenode(right_object) # if right children exist if self.right: temp.right = self.right self.right = temp # if right children does not exist else: self.right = temp def isroot(self): \u0026#34;\u0026#34;\u0026#34;[check if the node is root node] returns: [bool]: [if node is root return true] \u0026#34;\u0026#34;\u0026#34; handle = false if self.key[0] == \u0026#34;root\u0026#34;: handle = true return handle def isleaf(self): \u0026#34;\u0026#34;\u0026#34;check if the node is leaf node returns: bool: if the node is leaf return true \u0026#34;\u0026#34;\u0026#34; handle = false if not self.left and not self.right: handle = true return handle def catch_leaf(self): \u0026#34;\u0026#34;\u0026#34;show the partition list of leafs below the node, and mainly used to debug returns: list: a list contained all leaf below the node \u0026#34;\u0026#34;\u0026#34; list = [] # check if the node is leaf if self.isleaf(): list.append(self.index) if self.left: list.extend(self.left.catch_leaf()) if self.right: list.extend(self.right.catch_leaf()) return list def postorder(self): \u0026#34;\u0026#34;\u0026#34;[tree traversal with postorder, and mainly used to debug]\u0026#34;\u0026#34;\u0026#34; if self.left: self.left.postorder() if self.right: self.right.postorder() print(self.key) def get_parser() -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;[function used to parse parameters from command line and set log] returns: [dict]: [return a dict stored parameters] \u0026#34;\u0026#34;\u0026#34; # set logging logging.basicconfig( level=logging.debug, format=\u0026#34;%(levelname)s:%(asctime)s:%(message)s\u0026#34; ) # set parser parser = argparse.argumentparser( prog=\u0026#34;prog\u0026#34;, description=\u0026#34;program designed to condcut nj \u0026#34;, formatter_class=argparse.metavartypehelpformatter, ) # add parameter for input of fa file parser.add_argument( \u0026#34;-i\u0026#34;, \u0026#34;--input\u0026#34;, help=\u0026#34;the input file (fasta format)\u0026#34;, required=true, type=str ) # add parameter for output of edge file parser.add_argument( \u0026#34;-e\u0026#34;, \u0026#34;--edge\u0026#34;, help=\u0026#34;the output file of edges matrix. default: edges.txt\u0026#34;, default=\u0026#34;edges.txt\u0026#34;, type=str, ) # add parameter for output of tree file parser.add_argument( \u0026#34;-t\u0026#34;, \u0026#34;--tree\u0026#34;, help=\u0026#34;the output file of newick tree. default: tree.txt\u0026#34;, default=\u0026#34;tree.txt\u0026#34;, type=str, ) # add parameter for output of distance matrix parser.add_argument( \u0026#34;-d\u0026#34;, \u0026#34;--distance\u0026#34;, help=\u0026#34;the output file of distance matrix. default: genetic_distance.txt\u0026#34;, default=\u0026#34;genetic_distance.txt\u0026#34;, type=str, ) # add parameter for number of bootstrap; default: not conduct bootstrap parser.add_argument( \u0026#34;-b\u0026#34;, \u0026#34;--bootstrap\u0026#34;, help=\u0026#34;the number of bootstrap. result will be stored in \u0026#39;bootstrap.txt\u0026#39;\u0026#34;, default=none, type=int, ) # get parameters args = parser.parse_args() return args ############################################## # cell for getting genetic distance matrix ############################################## def read_input(input: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;[read input of fa file and return a dict whose value is nametuple stored information] args: input (str): [fasta file] returns: dict: [seq_label: (seq,index )] \u0026#34;\u0026#34;\u0026#34; # create namedtuple to store information seqs, seq = {}, namedtuple(\u0026#34;info\u0026#34;, [\u0026#34;seq\u0026#34;, \u0026#34;index\u0026#34;]) # open file as a list content = open(input).readlines() # init index of sequence read_index = 1 # read seq into the dict for index, line in enumerate(content): # check header if line.startswith(\u0026#34;\u0026gt;\u0026#34;): seqs[line.strip(\u0026#34;\u0026gt;\\n\u0026#34;)] = seq(content[index + 1].strip(), read_index) read_index += 1 return seqs def genetic_distance(seq1: str, seq2: str) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;[calculate genetic distance of seq1 and seqw] args: seq1 (str): [sequences of seq1] seq2 (str): [sequences of seq2] returns: float: [genetic distance] \u0026#34;\u0026#34;\u0026#34; # get length of sequences length = len(seq1) # init mismatch mismatch = 0 # begin to calculate for s1, s2 in zip(seq1, seq2): if s1 != s2: mismatch += 1 return mismatch / length def fetch_distance(seq1_index: str, seq2_index: str, matrix: dict) -\u0026gt; float or none: \u0026#34;\u0026#34;\u0026#34;[fetch distance from distance matrix in case comupute pair distance repeatedly] args: seq1_index (str): [seq1 label] seq2_index (str): [seq2 label] matrix (dict): [store distances that is calculated] returns: float or none: [if distance has calculated return value otherwise none] \u0026#34;\u0026#34;\u0026#34; # init value distance = none # check if the distance is already calculated if (seq1_index, seq2_index) in matrix: distance = matrix[(seq1_index, seq2_index)] elif (seq2_index, seq1_index) in matrix: distance = matrix[(seq2_index, seq1_index)] return distance def get_distance_matrix(seqs: dict, out: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;[write distance matrix to file] args: seqs (dict): [store information of seqs like {seq_label: (seq:, index:)}] out (str): [the name of genetic distance file] returns: dict: [{(seq1_label,seq2_label):genetic_distance}] \u0026#34;\u0026#34;\u0026#34; # open new file out_put = open(out, \u0026#34;w\u0026#34;) # init dict distance_map = {} # write header header = \u0026#34;\\t\u0026#34;.join(seqs.keys()) out_put.write(f\u0026#34;\\t{header}\\n\u0026#34;) # begin to calculate distance and write to file for seq1_label, seq1_info in seqs.items(): # init list to store distance of every line line = [] for seq2_label, seq2_info in seqs.items(): # check if the distance is already computed distance = fetch_distance(seq1_label, seq2_label, distance_map) if not distance: # if not exist then calculate it and store distance = genetic_distance(seq1_info.seq, seq2_info.seq) distance_map[(seq1_label, seq2_label)] = distance line.append(distance) # write every line line = \u0026#34;\\t\u0026#34;.join([str(element) for element in line]) out_put.write(f\u0026#34;{seq1_label}\\t{line}\\n\u0026#34;) out_put.close() return distance_map ############################################## # nj algorithm ############################################## def cacl_sum_distance(taxa: str, leaf_set: set, distance_matrix: dict) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;[calculate sum distance of node away from other nodes] args: taxa ([str]): [label for one node] leaf_set ([set]): [a set store current nodes needed to be merged] distance_matrix ([dict]): [{(seq1_label,seq2_label):genetic_distance}] returns: [float]: [sum distance of node away from others] \u0026#34;\u0026#34;\u0026#34; # init value result = 0 # calculate sum distance for other_taxa in leaf_set - set([taxa]): # fectch genetic distance result += fetch_distance(taxa, other_taxa, distance_matrix) return result def find_qmin(leaf_set: set, distance_matrix: dict) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;[ calculate min q_value and get information] args: leaf_set ([set]): [current nodes needed to be merged] distance_matrix ([dict]): [dict stored genetic distance of current nodes] returns: [dict]: [value, combination,taxa1_sum, taxa2_sum] \u0026#34;\u0026#34;\u0026#34; # init dict qmin = { \u0026#34;combination\u0026#34;: none, # combination of node for merging \u0026#34;q_value\u0026#34;: none, # min q value \u0026#34;taxa1_sum\u0026#34;: none, # sum distance of node away from others \u0026#34;taxa2_sum\u0026#34;: none, } # calculate q value every combinatin of nodes for combination, distance in distance_matrix.items(): taxa1, taxa2 = combination # calculate sum distance of node away from others taxa1_sum, taxa2_sum = cacl_sum_distance( taxa1, leaf_set, distance_matrix ), cacl_sum_distance(taxa2, leaf_set, distance_matrix) # get q value value = (len(leaf_set) - 2) * distance - taxa1_sum - taxa2_sum # compare q value if not qmin[\u0026#34;q_value\u0026#34;]: # if q value does not exist then store related information ( qmin[\u0026#34;q_value\u0026#34;], qmin[\u0026#34;combination\u0026#34;], qmin[\u0026#34;taxa1_sum\u0026#34;], qmin[\u0026#34;taxa2_sum\u0026#34;], ) = (value, combination, taxa1_sum, taxa2_sum) # if q value exist then compare to choose minimum value else: if value \u0026lt; qmin[\u0026#34;q_value\u0026#34;]: ( qmin[\u0026#34;q_value\u0026#34;], qmin[\u0026#34;combination\u0026#34;], qmin[\u0026#34;taxa1_sum\u0026#34;], qmin[\u0026#34;taxa2_sum\u0026#34;], ) = (value, combination, taxa1_sum, taxa2_sum) return qmin def help_nj(leaf_set: set, distance_matrix: dict): \u0026#34;\u0026#34;\u0026#34;[helper function to conduct neighbor joining] args: leaf_set ([set]): [current nodes needed to be merged] distance_matrix ([dict]): [store pair distance of current nodes] returns: treedict: a dict stored node that has been merged in binary tree format final_leaf_set: final twos node after finishing nj final_matrix: genetic distances of final two nodes \u0026#34;\u0026#34;\u0026#34; # init dict to store nodes that has been merged in format of binary tree treedict = {} def neighbor_joining(leaf_set: set, distance_matrix: dict): \u0026#34;\u0026#34;\u0026#34;[implementation of neighbors joining] args: leaf_set ([set]): [current nodes needed to be merged] distance_matrix ([dict]): [store genetic distance of current nodes] \u0026#34;\u0026#34;\u0026#34; # get nonlocal value nonlocal treedict # the rule to end the recursion if len(leaf_set) \u0026lt; 3: return (leaf_set, distance_matrix) # enter recursion else: # find min q value qmin_info = find_qmin(leaf_set, distance_matrix) # get nodes needed to be merged taxa1, taxa2 = qmin_info[\u0026#34;combination\u0026#34;] # edge from taxa1 to new node edge_taxa1_node = 0.5 * distance_matrix[qmin_info[\u0026#34;combination\u0026#34;]] + ( qmin_info[\u0026#34;taxa1_sum\u0026#34;] - qmin_info[\u0026#34;taxa2_sum\u0026#34;] ) / (2 * (len(leaf_set) - 2)) # edge from taxa1 to new node edge_taxa2_node = ( distance_matrix[qmin_info[\u0026#34;combination\u0026#34;]] - edge_taxa1_node ) # init dict to update genetic matrix matrix = {} # create label for new node new_node = f\u0026#34;{taxa1}_{taxa2}\u0026#34; # calculate distance from new node to other nodes for node in leaf_set - set(qmin_info[\u0026#34;combination\u0026#34;]): # store in new genetic matrix with information of old genetic matrix matrix[(new_node, node)] = ( fetch_distance(taxa1, node, distance_matrix) + fetch_distance(taxa2, node, distance_matrix) - distance_matrix[(taxa1, taxa2)] ) * 0.5 # store genetic distances of other nodes in new genetic matrix for combination, distance in distance_matrix.items(): if (taxa1 not in combination) and (taxa2 not in combination): matrix[combination] = distance # store phylogenetic nodes that has been merged in binary tree format # create a tree node that linked two merged nodes in binary tree # if one of merged nodes has been in binary tree then add edges # taxa1 is always regarded as left children if taxa1 in treedict: left = treedict.pop(taxa1) left.key[1] = edge_taxa1_node # if one of merged nodes is not stored in binary tree else: # create a tree node and store its label and edge left = treenode([taxa1, edge_taxa1_node]) # taxa2 is same with taxa1 # taxa2 is always regarded as right children if taxa2 in treedict: right = treedict.pop(taxa2) right.key[1] = edge_taxa2_node else: right = treenode([taxa2, edge_taxa2_node]) # create a new_node linked two merged nodes in binary tree # the new_node has no edge internal_node = treenode([new_node, none]) # add paraent information for two merged nodes left.parent, right.parent = internal_node, internal_node # link two merged nodes as children in binary tree internal_node.left, internal_node.right = left, right # add new node to dict treedict[new_node] = internal_node # update genetic distance matrix distance_matrix = matrix # update current phylogenetic nodes needed to be merged leaf_set -= set(qmin_info[\u0026#34;combination\u0026#34;]) leaf_set.add(new_node) return neighbor_joining(leaf_set, distance_matrix) # begin to compute until two phylogenetic nodes left final_leaf_set, final_matrix = neighbor_joining(leaf_set, distance_matrix) return treedict, final_leaf_set, final_matrix ############################################## # preorder tree and add index ############################################## def my_tree(treedict: dict, final_distance_matrix: dict, seqs: dict): \u0026#34;\u0026#34;\u0026#34;function to create binary tree but it has fake root because phylogenetic node has three children hence create a fake node as root to get a binary tree all following behaviors related binary tree usually consider this situation for binary_tree.key: key[0] -\u0026gt; label ; key[1] -\u0026gt; edge args: treedict ([dict]): [store information of binary tree ] final_distance_matrix ([dict]): [distance of final two nodes] seqs ([dict]): [store information of seqs] returns: [binary tree]: [binary tree with fake root] \u0026#34;\u0026#34;\u0026#34; # \u0026#39;merged node\u0026#39; means nodes have been changed as binary tree # init value to find if node is \u0026#39;merged node\u0026#39; in final two nodes after nj last_node_label = none last_node_edges = none for index, value in final_distance_matrix.items(): # get edge last_node_edges = value # check if node is \u0026#39;merged node\u0026#39; for label in index: # if not if label in seqs: # get label last_node_label = label # create a fake root with label \u0026#39;root\u0026#39; fake_root = treenode((\u0026#34;root\u0026#34;, \u0026#34; \u0026#34;)) # get node label of final two nodes after nj current_node_index = list(treedict.keys()) # if final two nodes are merged node if not last_node_label: # add last edge to right children of fake root # because the left children of fake root is \u0026#34;true root\u0026#34;, which has three childerns # the right children of fake root is linked to left children of it in real # so the edge indicates the distance between left children and right children # even though right children is linked to fake root in binary_tree treedict[current_node_index[1]].key[1] = last_node_edges # create tree structure fake_root.left = treedict[current_node_index[0]] fake_root.right = treedict[current_node_index[1]] # if one of final two nodes is merged node else: # set node that is not merged node as tree structure right_node = treenode([last_node_label, last_node_edges]) # set one of final two nodes, a merged node, as left children of fake root fake_root.left = treedict[current_node_index[0]] # add right children fake_root.right = right_node return fake_root def help_edge_matrix(tree, seqs: dict, edge_file: str): \u0026#34;\u0026#34;\u0026#34;[help function to do preorder traversal in order to get edge file] args: tree ([binary tree]) seqs (dict): [store information about label, seq, and index of seqs] edge_file (str): [output of edge] returns: [binary tree]: [binary tree whose internal node has index] \u0026#34;\u0026#34;\u0026#34; # init index for internal node n = len(seqs) + 1 # open new file edge_file = open(edge_file, \u0026#34;w\u0026#34;) # changes def edge_matrix(tree): # get nonlocal value nonlocal n # preorder tree if tree: # index of fake root is none if tree.isroot(): tree.index = none # tree.key[0] is label of node # if it not in seqs then it is internal node elif tree.key[0] not in seqs: # set index for internal node tree.index = n n += 1 else: # get index of leaf node tree.index = seqs[tree.key[0]].index # write information of parent and children # left and right children of fake root have no parent : none if tree.parent: edge_file.write(f\u0026#34;{tree.parent.index}\\t{tree.index}\\t{tree.key[1]}\\n\u0026#34;) edge_matrix(tree.left) edge_matrix(tree.right) edge_matrix(tree) # add the true root and its children edge_file.write(f\u0026#34;{tree.left.index}\\t{tree.right.index}\\t{tree.right.key[1]}\\n\u0026#34;) edge_file.close() return tree ############################################## # postorder tree # warning: must conduct after preoder ############################################## def help_newick_parse(tree, tree_file: str): \u0026#34;\u0026#34;\u0026#34;[help function to do postorder traversal in order to get tree file] args: tree ([binary tree]) tree_file (str): [output of tree] \u0026#34;\u0026#34;\u0026#34; # open tree file tree_file = open(tree_file, \u0026#34;w\u0026#34;) # get true root label (left children of fake root) true_root_label = tree.left.key[0] def newick_parse(tree): # get nonlocal value nonlocal true_root_label # postorder tree if tree: left = newick_parse(tree.left) right = newick_parse(tree.right) # if node is internal node if left and right: # check if node is true node if tree.key[0] == true_root_label: # not add \u0026#39;( )\u0026#39; so as to have three children finally return f\u0026#34;{left},{right}\u0026#34; # check if node is root elif tree.isroot(): # add \u0026#39;()\u0026#39; finally return f\u0026#34;({left},{right})\u0026#34; # if node is internal node else: return f\u0026#34;({left},{right}):{tree.key[1]}\u0026#34; # if node is leaf node else: return f\u0026#34;{tree.key[0]}:{tree.key[1]}\u0026#34; newick = newick_parse(tree) tree_file.write(f\u0026#34;{newick};\u0026#34;) tree_file.close() ############################################## # bootstrap ############################################## def bootstrap(original_seqs: dict): \u0026#34;\u0026#34;\u0026#34;bootstrap original seqs and return bootstrap sample and bootstrap seqs args: original_seqs (dict): original seqs returns: tree_root_with_node_index: bootstrap sample whose node has index seqs_bootstrap : bootstrap seqs contained information like index, label, and sequences \u0026#34;\u0026#34;\u0026#34; # change data structure in order to use pandas: # 0 1 2 (column_number) # label 1 # label 2 # ... seqs_pd = {index: list(value.seq) for index, value in original_seqs.items()} df = pd.dataframe(seqs_pd).t # using random to resample df_bootstrap = df.loc[:, random.choices(range(df.shape[1]), k=df.shape[1])] # get bootstrap information seqs_bootstrap = original_seqs.copy() # change sequences to bootstrap sequences for index, value in df_bootstrap.iterrows(): seqs_bootstrap[index] = seqs_bootstrap[index]._replace( seq=\u0026#34;\u0026#34;.join(value.values) ) # get nodes needed to be coudcted nj leaf_set = set(seqs_bootstrap.keys()) # calculate genetic_distance for bootstrap samples distance_map = {} for seq1_label, seq1_info in seqs_bootstrap.items(): for seq2_label, seq2_info in seqs_bootstrap.items(): if seq1_label != seq2_label: distance = fetch_distance(seq1_label, seq2_label, distance_map) if not distance: distance = genetic_distance(seq1_info.seq, seq2_info.seq) distance_map[(seq1_label, seq2_label)] = distance # conduct nj treedict, _, final_distance_matrix = help_nj(leaf_set, distance_map) # construct binary_tree # get edge file for last bootstrap (not used) tree_root = my_tree(treedict, final_distance_matrix, seqs_bootstrap) tree_root_with_node_index = help_edge_matrix( tree_root, seqs_bootstrap, \u0026#34;last_bootstrap.edge\u0026#34; ) return tree_root_with_node_index, seqs_bootstrap def help_find_partion(tree) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;function to partition list for internal nodes in a binary tree args: tree (binary_tree): [binary_tree with fake root returns: dict: node_index: partition list \u0026#34;\u0026#34;\u0026#34; # init dict to store index and its partition list internal_node_partion = defaultdict(list) # postorder traversal to find leafs for every internal node def find_partion(tree): list = [] # begin if tree: left = find_partion(tree.left) right = find_partion(tree.right) if tree.isleaf(): list.append(tree.index) else: # flatten list list.extend([*left, *right]) internal_node_partion[tree.index].extend(sorted([*left, *right])) return list # change partition list of fake root to true root find_partion(tree) internal_node_partion[tree.left.index] = internal_node_partion.pop(tree.index) return internal_node_partion def get_bootstrap_value( bootstrap_number: int, original_seqs: dict, original_tree, bootstrap_file: str = \u0026#34;bootstrap.txt\u0026#34;, ): \u0026#34;\u0026#34;\u0026#34;[function to integrate relation methods to get bootstrap values for original tree] args: bootstrap_number (int): [number for bootstrap. default = none] original_seqs (dict): [information of original sequences] original_tree ([type]): [binary tree of original sequences] bootstrap_file (str): [output of bootstrap] \u0026#34;\u0026#34;\u0026#34; # init list to store bootstrap value for every internal node internal_node_bootstrap_value = [0] * (len(original_seqs) - 2) # get partition list of every internal node in original tree original_partion_dict = help_find_partion(original_tree) # init list to store dict contained bootstrap_partion and its node_index bootstrap_partion_list = [] # begin to bootstrap and consruct binary tree for number in range(bootstrap_number): logging.info(f\u0026#34;bootstrap number : {number+1}\u0026#34;) bootstrap_tree, _ = bootstrap(original_seqs) bootstrap_partion_list.append(help_find_partion(bootstrap_tree)) # compare partition list to update bootstrap value for internal_node_index, partion_conent in original_partion_dict.items(): for bootstrap_partion in bootstrap_partion_list: if partion_conent in bootstrap_partion.values(): internal_node_bootstrap_value[ internal_node_index - len(original_seqs) - 1 ] += 1 # change format for the bootstrap values result = [f\u0026#34;{value/bootstrap_number}\\n\u0026#34; for value in internal_node_bootstrap_value] # write to file with open(bootstrap_file, \u0026#34;w\u0026#34;) as file: file.writelines(result) ############################################## # main ############################################## def worker(args: dict): \u0026#34;\u0026#34;\u0026#34;[function to integrate all above method to conduct nj and output tree and edge file] args: args ([dict]): [parameters getting from command line] returns: tree_root_with_index : binary tree whose node has index seq : information of sequences \u0026#34;\u0026#34;\u0026#34; # read fa file and parse it seqs = read_input(args.input) # get labels of sequences leaf_set = set(seqs.keys()) # get pair genetic distance distance_matrix = get_distance_matrix(seqs, args.distance) # delete distances between same sequences distance_matrix = { item: value for item, value in distance_matrix.items() if value != 0 } # get treedict and distance of final two nodes treedict, _, final_distance_matrix = help_nj(leaf_set, distance_matrix) # construct binary tree remember it has fake root # left children of fake root is regarded as true root tree_root = my_tree(treedict, final_distance_matrix, seqs) # get edge file and add index for every node in binary tree tree_root_with_index = help_edge_matrix(tree_root, seqs, args.edge) # get tree file remember it must conduct after getting edge files # because it need index of node help_newick_parse(tree_root_with_index, args.tree) return tree_root_with_index, seqs def main(): args = get_parser() with timer() as t: logging.info(f\u0026#34;neighbor joining begin.......\u0026#34;) tree_root_with_index, seqs = worker(args) logging.info(f\u0026#34;neighbor joining done : time {t.elapsed:.2f}s\\n\u0026#34;) if args.bootstrap: with timer() as t: get_bootstrap_value(args.bootstrap, seqs, tree_root_with_index) logging.info(f\u0026#34;bootstrap is done : time {t.elapsed:.2f}s\\n\u0026#34;) else: logging.info(f\u0026#34;bootstrap is not done\u0026#34;) logging.info(f\u0026#34;edge_file : {args.edge}\u0026#34;) logging.info(f\u0026#34;tree_file : {args.tree}\u0026#34;) logging.info(f\u0026#34;distance_file: {args.distance}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() thanks for your reading! hopefully helpful!\n","title":"Nei Saitou Neighbor Joining"}]}